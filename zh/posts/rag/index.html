<!DOCTYPE html>
<html lang="zh" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>虚假信息检索增强检测：理论、技术与实战教程 | Lyuxi Liu</title>
<meta name="keywords" content="Digital Humanities, Computational Social Science, NLP, Fake News Detection, RAG, Python">
<meta name="description" content="本文面向传播学研究者，系统介绍信息失序（Information Disorder）的理论机制，详述从端到端分类到“检索-验证”范式的演化。涵盖 BM25S 稀疏检索、语义向量检索、NLI 推理及 G-Eval 解释生成等核心技术，并提供完整的 Python 实战代码与案例分析。">
<meta name="author" content="Lyuxi Liu">
<link rel="canonical" href="https://llspublic.github.io/zh/posts/rag/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://llspublic.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://llspublic.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://llspublic.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://llspublic.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://llspublic.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://llspublic.github.io/posts/rag/">
<link rel="alternate" hreflang="zh" href="https://llspublic.github.io/zh/posts/rag/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://llspublic.github.io/zh/posts/rag/">
  <meta property="og:site_name" content="Lyuxi Liu">
  <meta property="og:title" content="虚假信息检索增强检测：理论、技术与实战教程">
  <meta property="og:description" content="本文面向传播学研究者，系统介绍信息失序（Information Disorder）的理论机制，详述从端到端分类到“检索-验证”范式的演化。涵盖 BM25S 稀疏检索、语义向量检索、NLI 推理及 G-Eval 解释生成等核心技术，并提供完整的 Python 实战代码与案例分析。">
  <meta property="og:locale" content="zh-cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-01-04T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-04T00:00:00+00:00">
    <meta property="article:tag" content="Digital Humanities">
    <meta property="article:tag" content="Computational Social Science">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Fake News Detection">
    <meta property="article:tag" content="RAG">
    <meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="虚假信息检索增强检测：理论、技术与实战教程">
<meta name="twitter:description" content="本文面向传播学研究者，系统介绍信息失序（Information Disorder）的理论机制，详述从端到端分类到“检索-验证”范式的演化。涵盖 BM25S 稀疏检索、语义向量检索、NLI 推理及 G-Eval 解释生成等核心技术，并提供完整的 Python 实战代码与案例分析。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "虚假信息检索增强检测：理论、技术与实战教程",
      "item": "https://llspublic.github.io/zh/posts/rag/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "虚假信息检索增强检测：理论、技术与实战教程",
  "name": "虚假信息检索增强检测：理论、技术与实战教程",
  "description": "本文面向传播学研究者，系统介绍信息失序（Information Disorder）的理论机制，详述从端到端分类到“检索-验证”范式的演化。涵盖 BM25S 稀疏检索、语义向量检索、NLI 推理及 G-Eval 解释生成等核心技术，并提供完整的 Python 实战代码与案例分析。",
  "keywords": [
    "Digital Humanities", "Computational Social Science", "NLP", "Fake News Detection", "RAG", "Python"
  ],
  "articleBody": "虚假信息检索增强检测：理论、技术与实战教程 导言 在生成式人工智能重塑信息生态的当下，“后真相”时代的信息传播正变得空前复杂。从真假难辨的 Deepfake 视频到公共卫生领域层出不穷的阴谋论，信息失序（Information Disorder）已演变为一种复杂的社会技术现象，而不仅是简单的“真”或“假”二元问题。传统的“假新闻”一词因过度政治化已失去精确含义，难以概括当前多样化的失实信息类型。为了有效研究和治理信息失序，我们需要建立严谨的概念框架和技术体系，既涵盖理论上的分类与认知机制，又包含实用的自动化检测流程。\n本文面向传播学领域的研究者，系统介绍信息失序的理论背景与心理机制，阐述检测范式的演化，详解关键技术模块（包括稀疏检索、语义检索、自然语言推理推断及结果聚合），并通过实操代码演示自动化事实核查系统的构建方法。随后，我们将结合案例研究（如疫苗谣言与气候变化叙事）说明模型如何应对披着“科学”外衣的虚假叙事。最后，讨论当前方法的前沿探索与挑战，包括检索增强生成（RAG）架构的整合、使用 LLM 评估解释的机制（G-Eval）、零样本与多模态检测难题，以及模型的优势与局限。整篇文章力求结构严谨、简明学术，帮助传播学研究者构建从理论到实战的完整认知。\n理论背景：信息失序的概念与心理机制 1. 信息失序的概念与分类 要研究虚假信息，首先需要厘清概念边界。信息失序（Information Disorder）框架由 Wardle 和 Derakhshan 在 2017 年提出，用真实性和意图两个维度严格定义了失实信息的类型。该框架将网络谣言现象分为三类：\n误导信息（Misinformation）： 内容客观上是虚假的，但分享者并无主观恶意。例如由于知识欠缺或统计疏忽导致传播了错误信息。 虚假信息（Disinformation）： 内容虚假且有意制造或传播以误导或伤害公众，带有明确的恶意动机，包括编造的政治阴谋论或伪造宣传。 恶意信息（Malinformation）： 内容本身可能真实，但被断章取义或恶意泄露，用于造成伤害。例如泄露隐私真相来抹黑他人。 上述分类为研究者提供了严谨的本体基础，也为数据标注提供了标准。在此基础上，Wardle 进一步细分出七种常见的信息失序形态，构成从轻度误导到完全捏造的连续谱：\n讽刺与恶搞（Satire/Parody）： 本意并非欺骗，但可能被受众当真。 错误连接（False Connection）： 标题或配图与内容不符，以标题党方式误导。 误导性内容（Misleading Content）： 通过选取部分事实或断章取义营造误导性的印象。 错误语境（False Context）： 将真实内容放在错误的时间或背景下呈现，以产生误导。 冒充内容（Imposter Content）： 伪装成权威机构或来源发布虚假信息。 篡改内容（Manipulated Content）： 对真实内容进行篡改、编辑或加工，使其传递错误信息。 捏造内容（Fabricated Content）： 完全虚构的新内容，毫无真实依据，旨在欺骗受众。 上述分类强调，不同类型虚假信息的特征迥异。例如讽刺内容往往带有幽默元素，而捏造内容则可能精心伪装成新闻报道。若不加区分地将它们混为一谈（统称为“假新闻”），模型训练只会学到噪音。因此，研究者应针对具体类别定制检测策略和特征工程，以提高模型的精度。\n2. 信息失序的心理机制 传播学不仅关心信息如何传播，更关注个体为何相信失实信息。研究表明，人们并非因为无知才信谣言，高知识人群也可能受骗，这是由于人类认知中存在一系列偏误和捷径。如果模型能融合这些心理学机制，将有助于预测谁更容易受虚假信息影响。\n几种核心的心理因素包括：\n身份认同偏差： 也称动机性推理或群体认同偏见。人们倾向于根据自身的党派或群体身份来选择性地接受信息，自动筛选出符合自身立场的内容。这种动机性的认知处理使得即便有相反证据，人们仍坚持原有信念，以维护群体认同。简言之，政治立场会“过滤”事实，高度党派化的人会对符合其意识形态的错误信息更容易信以为真。 认知懒惰： 即缺乏认知反思或深入思考的倾向。心理学将人类思维分为依赖直觉的“系统1”和理性分析的“系统2”。有些人更依赖直觉而不愿费脑思考，从而对谣言缺乏质疑。研究发现，分析性思维能力低的人更容易相信假新闻，其易感性与“懒于思考”密切相关。这意味着假消息传播往往不是因为过度党派偏见，而是由于缺乏理性思考的习惯。 虚幻真相效应： 仅因为重复多次听到某个说法，人们就更倾向认为它是真的，即使客观上它是假的。认知心理学称此为 illusory truth effect（真相幻觉效应）。重复让信息变得熟悉，熟悉度又被大脑误用为真实性的线索，从而提高错误信息的可信度。哪怕受众原本知道真实情况，反复的曝光仍可能削弱其判断。 情感唤起： 带有强烈情绪的信息（如恐惧、愤怒、道德义愤）更容易吸引注意并广泛传播。高唤起的信息可绕过理性分析直接影响判断。当人们情绪高涨时，对信息真假的辨别力会下降，更可能草率地转发或相信与情绪吻合的说法。 除了上述心理机制，传播数据还揭示了一些反直觉的现象。例如，我们容易高估虚假信息对全民的影响，仿佛所有人都被“洗脑”，却低估了人的主动性。实证研究显示：大部分普通用户接触到的谣言极少，虚假信息的大规模传播往往集中在极少数活跃的边缘群体中。虚假信息的泛滥更像是“一小群人的狂欢”，而非“全民皆中招”。因此，治理上应将重点从全网扫描转向对高危易感人群的精准干预。这一点提醒我们，不应陷入“算法决定论”的陷阱——社交媒体算法并非全能造就谣言的元凶，人的需求和选择同样重要。\n最后，需要特别关注那些披着科学外衣的误导性叙事。科学议题上的虚假信息通常更隐蔽，不直接谎报事实，而是通过框架重构来扭曲认知。例如，在气候变化议题上，不同措辞（“全球变暖”vs“气候变化”）会影响不同群体的接受度。研究发现，相比“全球变暖”，美国保守派对“气候变化”这一表述的抵触更小。这种语义差异本质上反映了立场的不同。再如疫苗谣言，常引用听起来“科学”的话术（例如引用已被撤稿的 Wakefield 伪研究）来博取信任。这些内容并非编造新事实，而是利用现有科学名词或数据进行误导，使得传统真伪检测更加困难。这对自动检测系统提出了更高要求：不仅要判断真假，还要捕捉立场和框架是否有偏差。\n综上，信息失序已不是简单的谣言问题，而是包含多维分类和深层心理机制的复杂现象。理解这些理论背景，有助于设计更有针对性的检测范式与模型。\n检测范式的演化：从分类器到“检索-验证” 面对层出不穷的虚假信息，早期研究往往尝试训练端到端的文本分类模型（如判断一则消息是否是假新闻）。这类传统二分类方法虽然直观，但存在明显不足：它们仅根据输入文本本身做出真伪判断，缺乏对判断依据的解释。在实践中，这容易遭遇信任危机——模型给出“真”或“假”的标签，却无法说明“为什么”，让人无从信服模型的结论。\n为提高检测的可信度和效果，近年来出现了**“检索-验证”（Retrieve-then-Verify）的范式**。该范式模拟人类事实核查员的工作流程：先针对待验证的声明去检索外部证据，再根据证据对声明真伪进行判定。这一流程被证明比单纯的文本分类更具可解释性和鲁棒性：\n证据检索： 针对每个需要核查的主张（claim），从大型可信知识库中检索相关文档或句子作为证据来源。例如针对某政治言论，检索维基百科或权威新闻报道中提及相同事件或数据的片段。 自然语言推理： 将检索到的证据与原主张进行对比分析，由 NLI 模型判断证据对主张的关系——是支持（entails）、反驳（contradicts），还是无关（neutral）。这一过程相当于逻辑推理，确定主张在证据面前是否站得住脚。 结果聚合： 综合多条证据的推理结果，给出最终的判定。例如，如果多数证据支持主张且无矛盾，则判定为真；若有可靠证据反驳主张，则判定为假；若证据不足则标记为信息不足。这一聚合策略确保模型决策基于充分的证据集合，而非孤立单一来源。 “检索-验证”范式的优点在于过程可解释。模型不仅输出结论，还可以同时提供它参考的证据（如引用哪篇维基文章的句子），使用户清楚判定依据。例如，模型可以说：“这条健康谣言为假，因为权威医学研究 X 表明与其说法相反。” 这样的检测系统更容易获得用户和事实核查员的信任。此外，该范式还能更好地应对开放域的未知主张：即使主张本身不在训练数据中，模型也能通过检索新信息来做出判断，具备一定的零样本能力。\n需要强调的是，在“检索-验证”框架下，我们不再鼓励训练一个不透明的端到端假新闻分类器。相反，我们构建的是一个模块化管道：每个步骤（检索、推理、聚合）都可独立优化，并能产出中间可解释结果（如检索列表、证据标注），从而整体上提高系统的透明度和可解释性。这对于传播学研究尤为重要——当我们用 AI 模型分析舆论中的谣言传播时，需要清楚模型基于哪些事实依据得出了结论，以便与新闻专业主义和社会科学理论接轨。\n总之，检测范式正从“黑箱式”的文本分类，演进到仿人的“检索证据→逻辑验证”流程。下一节我们将深入解读这一流程中的关键技术模块。\n关键技术模块详解 要实现检索增强的检测系统，需要将多个 NLP 技术模块有机结合。下面我们详细介绍其中的核心组件：稀疏检索、语义检索、自然语言推理（NLI），以及结果聚合与标签映射策略。\n1. 稀疏检索：BM25S 算法 稀疏检索（Sparse Retrieval）指传统基于关键词匹配的检索方法，其代表之一是 BM25。BM25 是一种对 TF-IDF 加以改进的概率检索模型，对文档和查询词频进行了饱和度处理和长度归一化。简单来说，BM25 会根据查询和文档之间共享的词汇，以及词在文档中的频率和在整个语料库中的逆文档频率，来计算相关评分。它擅长精确匹配查询词，对于人名、地名等实体词汇往往非常有效。\n在本文介绍的系统中，我们使用了 BM25 的一个高效实现变体，称为 BM25S（BM25 Sparse）算法。BM25S 在 Python 中有现成库可用（如 bm25s），具有以下特点：\n精确匹配： 对专有名词、数字等精确匹配需求的查询表现出色。如果声明中出现具体的人名地名，BM25 能迅速锁定包含这些实体的文档。相比之下，一些语义向量模型可能因为过度泛化而错过这些精确匹配。 高效索引： BM25S 使用倒排索引结构，针对大型语料库检索速度极快，可在上百万文档规模下实现毫秒级查询响应。 长度归一： BM25 公式中引入了文档长度惩罚，避免长文档因为涵盖更多词而获得不公平的高分。这提高了相关性评分的公平性。 实现简便： 使用 BM25S 需要对语料进行分词和建立索引。一旦建好索引对象，就可以高效地对任意查询进行检索。作为 RAG 系统的“第一道防线”，稀疏检索模块实现相对简单且稳定。 代码示例： 假设我们有一个证据句子库 corpus（每个元素为一条证据句文本），可以使用 BM25S 进行索引和检索：\nimport bm25s # 构建BM25索引 retriever = bm25s.BM25(corpus=corpus) retriever.index(bm25s.tokenize(corpus)) # 对查询主张执行检索 claim = \"The number of new cases of shingles per year extends from 1.2-3.4 per 1,000.\" results, scores = retriever.retrieve(bm25s.tokenize(claim), k=10) print(results[0:3]) # 打印前3条最相关证据句 上述代码将返回与 claim 内容高度重合的证据文本列表。BM25 会优先返回包含了诸如“shingles”（带状疱疹）、“1.2-3.4 per 1,000”等关键词的句子，因为这些稀疏特征直接匹配，提高了相关性得分。\n2. 语义检索：双塔模型与向量表示 稀疏检索虽然高效，但它依赖表层词汇匹配，对同义表达和隐含语义无能为力。例如声明使用了“导致”，而证据用词是“引发”，BM25 可能因为字面不匹配而错失相关证据。为解决这个问题，我们需要借助稠密检索（Dense Retrieval）方法，即通过语义向量表示来捕捉文本的深层含义。\n双塔模型（Dual Encoder）是常用的稠密检索架构。它包含两个塔（神经网络），分别将查询和文档编码为向量，并在向量空间中通过相似度（通常是余弦相似度）来衡量相关性。典型实现如 Facebook 提出的 DPR（Dense Passage Retriever）模型，或者更通用的 Sentence Transformers 库提供的双塔预训练模型。\n在我们的系统中，我们采用预训练好的句向量模型（如 sentence-transformers 提供的 all-MiniLM-L6-v2）将文本映射为向量。其要点包括：\n高维语义映射： 模型将每条文本（声明或证据句）映射到一个高维向量，使得语义相似的文本在向量空间中距离更近。这意味着哪怕两个句子没有任何相同的字词，只要意思相关，向量也会彼此靠拢。 同义词与概念对齐： 向量表示可以自动学习到同义表达的接近性。如“引发”和“导致”两个词语，它们出现在相似语境中时，模型会将它们嵌入到相邻的位置。 双塔编码： 查询和文档分别编码，计算相似度时无需交叉输入。这种结构便于离线预计算文档向量：我们可以提前算好整个证据库中每个句子的向量并存储。当有新查询时，只需算一次查询向量，然后与所有证据向量计算相似度即可，大幅提升检索效率（常用工具如 Faiss、Milvus 能支持快速向量近邻搜索）。 混合检索： 实践中，我们常将 BM25 结果与向量检索结果融合，以兼顾精确匹配和语义召回。这样可以最大化召回率——既不会错过同义表述的相关证据，也确保精确实体匹配不被语义模型漏掉。 代码示例： 使用 SentenceTransformer 加载模型并检索相似语义证据：\nfrom sentence_transformers import SentenceTransformer, util import numpy as np # 加载预训练的轻量级句向量模型 model = SentenceTransformer('all-MiniLM-L6-v2') # 计算语料库证据句的向量表示（建议离线预计算并存储） evidence_embeddings = model.encode(corpus, convert_to_tensor=True) # 计算查询主张的向量 claim_embedding = model.encode(claim, convert_to_tensor=True) # 计算余弦相似度并获取前10个最相近证据索引 cosine_scores = util.cos_sim(claim_embedding, evidence_embeddings)[0] # 余弦相似度向量 top_indices = np.argpartition(-cosine_scores, range(10))[0:10] # 前10高分索引（无序） top_indices = top_indices[cosine_scores[top_indices].argsort(descending=True)] # 按相似度排序 # 查看最相关的一条证据 top_evidence = corpus[top_indices[0]] print(top_evidence) 这段代码利用 sentence-transformers 库的 util.cos_sim 快捷计算了相似度，并选出前 10 条证据。相比 BM25，语义检索能够找出与主张意思相近但未必有明显词重叠的句子。例如，对于“疫苗接种会导致自闭症”这样的谣言主张，语义检索可以找到科学论文中的相关句子“研究已未发现疫苗与自闭症存在关联”之类的证据句，尽管表面词汇不同，但意义上直接反驳了谣言。\n性能评估： 为衡量检索模块的效果，可使用 MRR（Mean Reciprocal Rank） 指标。MRR 关注第一个相关结果出现在检索列表中的位置，位置越靠前得分越高。公式是对每个查询计算其相关结果的倒数排名，然后取平均值。以伪代码为例：\ndef mean_reciprocal_rank(results_list, ground_truth_list): rs = [] for results, true_doc in zip(results_list, ground_truth_list): ranks = [idx for idx, doc in enumerate(results) if doc == true_doc] rs.append(1.0 / (ranks[0] + 1) if ranks else 0.0) return np.mean(rs) 我们希望 MRR 越高越好，意味着大部分查询的正确证据在检索结果中排名很靠前。一般而言，将 BM25 和向量检索结合可以明显提升 MRR，因为两者互补弥补了各自盲区。\n3. 自然语言推理：蕴含式的真假判断 完成证据检索后，下一步是判断证据与声明的关系。这被形式化为一个自然语言推理（NLI）任务：给定一句声明和一段证据文本，机器需要判断证据是否支持声明、反驳声明，或与声明无关。这一转换非常关键——它把事实核查问题提升为一种逻辑推理，从而赋予模型跨领域的一般推理能力。即使模型没见过特定谣言，也可以通过 NLI 的推理技能来判断证据和主张的矛盾与否。\n我们通常采用已经在大规模 NLI 数据集（如 MultiNLI，SNLI 等）上训练微调过的 Transformer 模型来执行这一任务。零样本设置下，一个在 NLI 任务上训练良好的模型可以直接泛化到事实核查标签，无需额外针对谣言数据微调。例如，著名的 FEVER 论文就将事实核查标签映射到 NLI 三分类：Entailment 对应“Supports”，Contradiction 对应“Refutes”，Neutral 对应“Not Enough Info”。\nNLI 模块的实现要点包括：\n输入构造： 将声明和证据拼接成模型输入对。如标准做法是在 Transformer 输入中构造 [CLS] 声明 [SEP] 证据 [SEP] 的格式，让模型同时读入二者并利用注意力机制分析它们的关系。这样模型可以对照证据内容理解声明是否被支持或否定。 交叉编码（Cross-Encoder）： 用于 NLI 判断的模型通常是单塔结构（不同于前述双塔）——即将声明和证据拼接后送入同一个 Transformer 编码。这种 Cross-Encoder 可以利用跨句的注意力捕捉微妙的逻辑关系，比独立编码再比对更加精细。这也使得 Cross-Encoder 常用于对检索结果重排序，因为它能更准确地区分细微相关性。 标签映射： NLI 模型输出通常为三类之一：Entailment（蕴含）、Neutral（中立）、Contradiction（矛盾）。我们需要将其映射到事实核查的标签空间。例如，可对应映射为：Supports（支持）、Not Enough Info（信息不足）、Refutes（反驳）。这个映射策略确保模型判断与人类判定标准一致。 代码示例： 利用 🤗 Transformers 的 pipeline 快速进行 NLI 判断：\nfrom transformers import pipeline # 加载在NLI数据上微调的RoBERTa大型模型（支持零样本推理） nli_pipeline = pipeline( \"text-classification\", model=\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\" ) # 示例：声明和证据 claim_text = \"The number of new cases of shingles per year extends from 1.2-3.4 per 1,000.\" evidence_text = \"The number of new cases per year ranges from 1.2 -- 3.4 per 1,000...\" # 构造输入对并预测 result = nli_pipeline([{'text': claim_text, 'text_pair': evidence_text}])[0] print(result['label']) # 可能输出 'ENTAILMENT' 或 'CONTRADICTION' 等 print(result['score']) # 概率分数 在这个例子中，如果 evidence_text 内容与 claim_text 逻辑一致，模型应输出 Entailment（蕴含）；如果证据明显反驳了声明则输出 Contradiction。随后我们将 Entailment 映射为“支持”标签，Contradiction 映射为“反驳”，Neutral 映射为“信息不足”。\n需要注意的是，证据可能有多条，模型应对每条证据分别判断，然后聚合结果。这属于下一节要讨论的策略。不过简单而言，一种常用规则是：若有任何一条证据被判定为 Contradiction（反驳），则综合判定声明为 False；否则如果至少一条证据是 Entailment 且无 Contradiction，则判定为 True；若既无 Entailment 也无 Contradiction，则判为未知。这类似于事实核查领域的 FEVER 评测标准。\n除了使用专门微调的判别式模型，近年来还出现了利用 大语言模型（LLM） 做零样本 NLI 的思路。即通过提示（prompt）让 GPT-4 等模型直接阅读证据并给出支持/反驳判断。举例来说，可以构造如下 Prompt：\nYou are a fact-checking assistant. Decide if the claim is supported, refuted, or not enough info based on the statements. Claim: \u003c填入待判定声明\u003e Statements: 1. \u003c证据句1\u003e 2. \u003c证据句2\u003e ... Answer with one of: SUPPORTS / REFUTES / NOT ENOUGH INFO. 将此提示提交给强大的 LLM（如 GPT-4），理论上可以得到正确的标签。我们的教程实验中，使用一个小型 GPT-4 同架构模型（称为 GPT-4o mini）实现了零样本分类，结果达到约 79.6% 的准确率，接近专门微调的 RoBERTa 模型（83.2%）。这说明在特定任务上，经过微调的小型模型往往在性价比和效果上优于通用的大型模型。不过 LLM 的方法无需训练，易于扩展到新任务，仍具有很大实用价值。\n4. 结果聚合与标签映射 当经过以上步骤后，我们对每个“声明-证据”对都有一个 NLI 模型输出结果。现实中，一个声明往往需要多条证据一起才能定论，因而最后需要聚合这些结果并输出最终标签。\n标签映射我们已在上一节提到，即将 NLI 的三元分类映射回我们感兴趣的输出标签。以事实核查为例，一般输出三类：真实（True/Supported）、虚假（False/Refuted）、不确定（Not Enough Info/Neutral）。这个映射通常是一一对应的。但在其他任务中可能不同，比如有的谣言检测任务只分“谣言/非谣言”二类，则可能需要将 NLI 的 Entailment 视为“非谣言”，Contradiction 视为“谣言”，Neutral 也视作“谣言”（因为缺乏证据佐证，也不能确认其真）。具体取决于任务定义。\n结果聚合则需要考虑多条证据的综合情况。常见策略包括：\n证明式聚合： 如果至少有一条证据明确支持主张，且没有矛盾证据，则判定为真；若至少一条证据反驳主张，则判定为假；否则在无支持且无反驳的情况下为信息不足。这种策略类似于法律中的“存在一条有效证据即可定罪/免责”的思路，适用于多数事实核查任务。 多数投票： 当证据质量可能参差不齐时，可让多条证据各自投票，以投票结果决定标签。例如 3 条证据中 2 条支持 1 条反驳，可决定支持。但这种简单投票未必可靠，因为证据并非独立同质，通常会优先考虑最权威或相关度最高的证据。 加权融合： 根据证据的检索得分或来源可信度，对不同证据赋予权重，再累积它们属于各个类别的置信度。例如，如果一条高相关度证据显示反驳，那么即便其他几条模糊支持，也可能以高权重的反驳为准。 学习融合模型： 在高级应用中，可以训练一个判决融合模型，输入是多条证据的 NLI logits 或嵌入，输出最终标签。这样模型可以学习如何在证据冲突时做决定。但这需要一定的训练数据支撑，不属零样本范畴。 在我们的教程框架中，我们采用了简化的证明式规则：有反驳则 False，有支持无反驳则 True，都无则 NEI。这一规则虽然简单，但符合直觉且在 FEVER 任务中表现良好。事实上，FEVER 的评测也要求模型给出正确证据集合才能算判定正确，即所谓 FEVER 分数：只有标签正确且提交的证据包含完整支持/反驳链条，才计为成功。这鼓励系统在聚合时严谨考虑证据充分性，否则宁可输出“信息不足”。FEVER 分数衡量模型证明过程的准确率，因此比单纯标签准确率更苛刻，但也更实用。\n小结一下技术模块：我们介绍了 BM25 稀疏检索保障了实体匹配，语义向量检索捕获了同义语义，NLI 推理进行逻辑判别，最后通过标签映射和结果聚合得到最终决策。接下来，我们将把这些模块串联起来，演示一个实际的自动化事实核查流程，并提供代码示例。\n实操流程与代码演示 下面我们按照实际建模的顺序，将前述模块依次串联，构建一个简化的谣言检测/事实核查系统，并提供关键代码片段。流程包括：主张检测（Check-Worthiness Detection）、证据检索（结合稀疏和稠密方法）、事实核查判断（NLI推理输出标签）、以及解释生成（由生成模型给出判定理由）。\n1. 主张检测（Check-worthiness Classification） 在社交媒体帖子或政治演讲中，并非每一句话都值得去核查。大部分句子只是闲聊寒暄或表达观点，只有少数句子包含可验证的具体事实主张，需要我们进一步检索证据。这就提出了主张检测任务：从一段文本中识别出有核查价值的语句。这是一个二分类问题（可核查 vs 不需核查），但具有极端类别不平衡的特点——可核查的句子往往不到全部句子的 5%。\n我们可以利用监督学习来训练一个主张检测模型。例如，使用一个预训练的 BERT 模型做二分类微调。为了应对类别不平衡，常用做法是对正例（可核查句）给予更大损失权重，或过采样正例等。在 HuggingFace 的 Trainer 框架下，可以自定义 loss 来实现。例如：\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments import torch model_name = \"bert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # 构造Dataset的数据映射 def encode_examples(examples): outputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128) outputs[\"labels\"] = [1 if lbl == \"Check-worthy\" else 0 for lbl in examples[\"label\"]] return outputs encoded_dataset = raw_dataset.map(encode_examples, batched=True) # 定义带加权损失的Trainer class WeightedLossTrainer(Trainer): def __init__(self, class_weights, *args, **kwargs): super().__init__(*args, **kwargs) self.class_weights = torch.tensor(class_weights).to(self.model.device) def compute_loss(self, model, inputs, return_outputs=False): labels = inputs.get(\"labels\") outputs = model(**inputs) logits = outputs.logits loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights) loss = loss_fct(logits, labels) return (loss, outputs) if return_outputs else loss # 计算类别权重，例如不核查:核查 ≈ 1:20 weights = [1.0, 20.0] trainer = WeightedLossTrainer(class_weights=weights, model=model, args=TrainingArguments(...), train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[\"val\"]) trainer.train() 上述代码中，我们手动设置了类别权重为 1:20，意味着每个核查类样本的损失被放大 20 倍，从而引导模型更关注少数类。这种加权交叉熵是缓解类别失衡的有效手段。\n训练完成的模型即可用于从输入文本中过滤出需要核查的句子列表。假定我们已经定位到若干值得核查的主张，接下来对每个主张进行事实核查。\n2. 证据检索 拿到一个具体主张后，我们需要在海量非结构化文本库（知识库）中找到支持或反驳它的证据句子。我们将 BM25 稀疏检索和语义稠密检索结合使用。\nBM25 检索： 首先用主张中的关键词去索引中搜索。代码示例如前文 BM25S 部分所示，这里不再重复。BM25 会返回一组候选证据句（如 Top 50）。由于 BM25 偏向精确匹配，这一步可以确保一些关键实体相关的文档不被遗漏。 向量检索： 并行地，我们用语义向量对主张进行检索。代码示例在前文语义检索部分也给出。我们可以取向量相似度最高的一组候选句（如 Top 50）。向量检索会补充 BM25 遗漏的、语义相关但表述不同的证据。 候选融合： 将两种方法得到的候选集合并，通常也就数十条句子，然后可能需要去重（因为有时两种方法会命中同一句）并简单排序。排序可以根据 BM25 分数或向量分数，或者训练一个学习排序模型。但出于简单，可按比如 BM25 得分标准排序。 证据筛选： 为了减轻后续 NLI 模型负担，我们可以增加一层筛选。例如利用一个轻量的 Cross-Encoder 对候选证据再打分重排，选取 Top 5 送入下一步推理。这一步提升准确率但复杂度增加。若为了演示概念，可省略，直接使用所有候选。 3. 事实核查判断（NLI推理） 对于每个主张，我们现在有若干候选证据句。我们让 NLI 模型对每对 主张-证据进行判断。如前文所述，使用经过训练的 NLI 模型可以直接给出“支持/反驳/无关”三分类结果。\n伪代码如下：\nlabels_map = {\"ENTAILMENT\": \"SUPPORTS\", \"CONTRADICTION\": \"REFUTES\", \"NEUTRAL\": \"NOT ENOUGH INFO\"} final_decisions = [] for claim in claims_to_check: decisions = [] for evidence in candidate_evidences[claim]: result = nli_pipeline({'text': claim, 'text_pair': evidence}) decisions.append(labels_map[result[0]['label']]) # 结果聚合 if \"REFUTES\" in decisions: final = \"False\" # 有证据反驳 elif \"SUPPORTS\" in decisions: final = \"True\" # 有支持且无反驳 else: final = \"Not Enough Info\" final_decisions.append((claim, final)) 上述逻辑实现了简单的证据聚合策略。在得到最终标签后，我们就完成了对主张真伪的判断。不过，仅给出标签往往不够。在谣言治理中，我们还希望解释模型为何这么判定。为此，引入下一步：解释生成。\n4. 解释生成 为了提高检测结果的可接受度和有效性，我们希望模型能生成一段自然语言解释，说明它基于哪些证据、如何推理得出了结论。例如，对于输出“False”的谣言，解释可能是：“因为证据 X 表明情况恰好相反，因此该主张不成立”。这样的解释不仅帮助受众理解真相，也对抗了谣言的说服力。\n生成解释可被视为一个 NLG（自然语言生成）任务，我们可以使用大型生成模型（如 GPT）来完成。在实践中，我们可以设计一个 Prompt，让模型根据主张、标签和证据来生成解释。例如：\nYou are tasked with creating an explanation of a fact check. Claim: \u003c主张\u003e Label: \u003c模型判定的标签（True/False/Not Enough Info）\u003e Statements: - \u003c证据句1\u003e - \u003c证据句2\u003e ... Please provide a brief explanation as to why the label is correct based on the provided evidence. 将上述模板填入具体内容后提交给如 GPT-4o mini 模型，它会生成一段解释说明。例如，对于一个谣言主张“Maze Runner 是一项体育比赛”而证据显示 Maze Runner 是电影制片人的例子，它可能生成解释：\n“‘反驳’标签是正确的，因为证据明确指出《移动迷宫》（The Maze Runner）是一部由 Ellen Goldsmith-Vein 制作的电影，而该主张声称它是一项体育竞赛。”\n这个解释清晰地表述了证据与主张如何矛盾，从而支撑了“反驳”结论。\n解释评估： 生成的解释质量如何评估？传统的 NLG 评测指标如 BLEU、ROUGE 主要看字面重合，无法衡量解释与证据的逻辑一致性（即“忠实度”）。为此，我们引入 G-Eval 评估机制：让一个更强大的 LLM 充当评审，按照预先定义的标准打分。例如定义评分 1-5 分，其中 1 表示解释完全与证据不符，5 表示解释准确完备地反映了证据和主张的关系。\n我们可以构造如下评价提示模板：\nEvaluation Criteria: Faithfulness (1-5) - the factual alignment between the fact-checking explanation and the evidence. Evidence Provided: \u003c证据文本\u003e Fact-Checking Explanation: \u003c模型生成的解释\u003e Evaluation: 请仅给出一个1到5的分数。 让 GPT-4 来多次（如 n=20 次）评估同一解释。由于 LLM 输出存在随机性，我们可以取多次评分的平均值作为最终得分，借此减少单次偏差。这样的评估方式利用 LLM 的理解和推理能力，被称为 LLM-as-a-judge 评估，也即 G-Eval 的基本思想。研究表明，G-Eval 能够在一定程度上以接近人类的一致性评价生成文本。\n需要说明的是，解释生成是锦上添花的部分。在实际应用中，如果只是为了最终的自动判断，前三步已经可以完成谣言检测。但提供解释无疑会增强模型输出的可解释性和说服力，非常适合辅助人工审核、公众科普等场景。\n至此，我们的自动化检测流程已经涵盖了从输入原始文本一直到输出带解释的判定结果的各个环节。下面通过实际案例来看看这一流程的作用。\n案例研究：疫苗谣言与气候变化叙事 为了更直观地理解上述模型如何应对“披着科学外衣”的虚假叙事，我们以两个社会热点为例演示。\n案例 1：疫苗谣言 情境： 某网络帖子声称：“研究证明接种 MMR（三联）疫苗会导致儿童患自闭症。” 这是一个经典的反疫苗叙事，它引用了“研究证明”以增加科学性。然而，该主张来源于 1998 年发表在《柳叶刀》上的一篇论文，后来已被证实是造假并正式撤稿。\n主张检测： 模型会将这句话识别为可核查的主张，因为它涉及医学因果关系，影响重大且可查证。 证据检索： BM25 可能使用关键词如“MMR”“autism”“study”检索到相关新闻或维基百科页面，向量检索则会捕捉“没有关联”“研究发现无证据”等语义相关的句子。实际证据很容易在医学权威来源找到，例如美国儿科学会（AAP）的新闻稿指出：“大量跨年代跨国家的研究均未发现疫苗与自闭症有可信关联。最初声称 MMR 致自闭症的报告因造假已被撤回，其作者也被吊销行医执照。” NLI判断： 给定上述证据句，模型会判定为“Contradiction”（矛盾），因为证据明确反驳了主张（证据说没有关联而主张说有因果）。标签映射后即“False”（虚假）。 结果聚合： 如有多条证据都显示未发现关联，则一致指向 False，无冲突。模型最终输出该谣言为虚假信息。 解释生成： 模型可以进一步生成解释，例如：“权威研究已多次证实 MMR 疫苗与自闭症之间没有因果关系，最初声称相关的论文因数据造假被撤回。因此该主张不成立。” 这段解释点出了证据的关键信息（“没有因果关系”“论文造假撤回”），有力地驳斥了谣言的科学伪装。 案例 2：气候变化叙事 情境： 一篇博客写道：“过去几年北极海冰面积呈增长趋势，全球变暖是场骗局。” 这是一种通过选择性时间段数据来质疑气候变暖的论调，看似引用科学观测，其实是误导性内容的一例。它并未彻底编造事实（某些年份北极海冰确有反弹），但通过断章取义忽略了长期趋势。\n主张检测： 该句也属于可核查主张，涉及科学数据与结论，需要验证。 证据检索： 检索可能返回 NASA 或 IPCC 关于北极海冰长期趋势的报告摘要。例如找到：“卫星数据显示，北极海冰覆盖的长期趋势依然在下降，尽管某些年份有短暂增长，但总体趋势支持全球变暖持续。” 模型还可能检索到对“全球变暖骗局”这种说法的科学辟谣文章。 NLI判断： 证据与主张的关系有些微妙。证据并不直接说主张“错误”，而是提供了完整背景（短期波动不改长期变暖）。模型可能倾向于判定为“Contradiction”（因为主张称变暖是骗局，而证据强调变暖真实存在）。也可能部分证据被模型视为“Neutral”（信息不足），如果证据只是提供数据而未直接评价“骗局”这一论断。 结果聚合： 综合权威证据，应该有充分理由反驳“全球变暖是骗局”。哪怕某条证据模型没判为 Contradiction，多条科学报告一起也能形成强力反证。因此最终应输出虚假。 解释生成： 解释需要兼顾科学性和通俗性，例如：“虽然近年部分时间段北极海冰有所反弹，但总体趋势依然下降。权威气候报告明确指出全球变暖正在发生，并非骗局。因此断言‘全球变暖是骗局’属于断章取义的误导。” 这样的解释揭示了主张的技巧（选取短期增长混淆长期趋势），给出完整科学结论，帮助读者理解为什么主张是错的。 通过以上案例可以看到，我们的模型流程不仅能识别完全虚假的声明，也能处理基于事实但结论错误的叙事。对于后者，模型依赖多个证据片段，从不同角度揭示主张的问题。这也是为啥多证据检索和 NLI 推理是必要的——它让 AI 有能力像专家一样对复杂的带框架偏见的说法做出判断，而非只能处理简单的是非题。\n当然，这些案例也提醒我们模型的局限：如果主张以隐晦方式表达（如暗示性的错误因果），模型的 NLI 判定可能不自信；如果证据不足或知识库没有相关信息（长尾冷门谣言），模型只能输出“不确定”。下一节我们将深入讨论这些挑战和未来方向。\n前沿探索与挑战 当前的检索增强检测框架在实验中表现出较高的准确率和可解释性，但仍有诸多开放问题和改进空间。有些是技术前沿，有些是应用落地时必须克服的挑战。在本节，我们列举几个值得关注的方向：\n检索增强生成（RAG）架构整合： RAG 是指将检索模块无缝集成到生成式模型中，使模型能一边调用外部知识库一边生成回答。将来我们可以将事实核查系统与大型语言模型更紧密结合，让模型在生成结论或解释时动态检索资料，确保输出内容的事实性和完整性。这一思路实际上已经在开放域问答等任务中验证有效，对于谣言检测的解释生成也大有裨益。RAG 架构还可以处理更长文档的多跳推理，通过逐步检索相关信息来应对复杂声明。 解释质量评估（G-Eval）机制： 随着生成模型的应用，如何评估它们输出的解释是否可靠成为新挑战。G-Eval 框架提供了利用 LLM 进行评估的思路。未来可进一步研究多维度的评估标准，比如不仅考察解释的忠实度（是否严格基于证据），还考察可读性、说服力等。评估过程也可以更加自动化和稳健，比如结合多个不同 LLM 评审以抵消单一模型偏见。对于追求学术严谨性的传播学研究，用 AI 评估 AI 生成的解释需要特别谨慎，但这一方向有望大幅提升评估效率。 零样本与域外泛化： 信息失序的一个难点在于其瞬息万变。新的阴谋论、俚语表达不断出现，不可能穷尽列入训练集。因此，零样本学习和跨领域泛化能力非常重要。我们已经通过 NLI 引入了一定零样本推理能力，但未来还可探索大模型 Prompting 更复杂的推理，或利用自监督预训练让模型掌握常识推理，从而在面对从未见过的谣言时也能有所判断。当然，完全的零样本是不现实的，因此持续学习（Continual Learning）也是方向，让模型随着新数据更新而进化，但要避免灾难性遗忘和谣言数据污染模型。 多模态检测： 目前讨论的方法主要针对文本。而现实中的虚假信息常常是多模态的——假视频、伪造图片、音频片段等。Deepfake 视频的崛起就是一例，需要结合图像取证、声音识别等技术。多模态融合的检测系统亟待研究，比如当一条谣言通过图文并茂的方式传播时，如何将 NLP 模型与计算机视觉模型结合，共享判断。已有一些多模态事实核查数据集，但这一领域仍在起步，具有很大挑战性。 对抗性攻击与鲁棒性： 生成式 AI 不仅是防守方工具，也被不法分子利用来产生更具迷惑性的谣言文本。这些对抗性文本可能专门针对检测器弱点，规避关键词或使用花哨句式，让模型难以判断。提高模型对这类攻击的鲁棒性是当务之急。可行方案包括：引入对抗训练，让模型见过被扰动的假文本；以及发展可验证的模型，如利用逻辑规则约束输出。与此同时，还要防范模型自身产生错误信息——在 RAG 架构中如检索到假信息模型可能被误导，因此知识来源可信度评估也很重要。 知识缺口与长尾问题： 当谣言涉及非常新颖或冷门的话题，知识库中可能查不到直接证据。这时检索式方法会陷入“知识真空”。模型可能将这类主张一概标记为“信息不足”，但实际上在传播学研究中，我们有时需要预测性地判断风险。为此，可以拓展知识来源，例如接入科学文献、专家数据库，或者引入推演机制：基于常识和相关知识推理出结论。然而，让 AI 基于不完整知识推理真相存在风险，仍需深入探索。 评估指标与误判代价： 对于检测系统来说，单纯追求总体准确率可能掩盖问题。例如，如果谣言仅占样本的 1%，模型全预测“非谣言”就有 99% 准确率，但毫无意义。因此更合适的评估是关注正类的召回率和 F1 分数。在实践中，漏掉一个真实谣言（假阴性）和错判一个真信息为谣言（假阳性）的代价是不同的，应根据应用场景调整模型阈值和优化目标。传播学者特别关心的是减少假阴性（别漏掉重要谣言）同时提供证据以便人工复核。这需要结合定量指标和定性分析全面评估模型性能。 优势与局限： 当前检索增强的检测模型相比纯分类器有明显优势：可解释、可扩展且领域无关（通过证据检索跨领域应用）。同时，它还能方便地融入人工流程——事实核查员可以直接查看模型检索的证据和判定辅助决策。然而局限也不可忽视：系统复杂度高、需大量外部知识支持，对知识库的新鲜度依赖强（知识库没收录的新假消息就无能为力），并且难以捕捉含蓄的误导（模型擅长显性真伪判断，但对技巧性误导如讽刺、夸张有时无从定性）。另外，模型输出的解释质量目前还不稳定，可能存在 AI 虚构（hallucination）现象，需要用人工校验或更可靠的生成约束。\n综上，虚假信息检测是一个持续演进的攻防领域，需要跨学科的视角和多方面的创新。从传播学理论到人工智能技术，每一步发展都将有助于我们更好地识别和应对信息失序。希望本教程提供的理论-实操框架，能为相关研究提供有益参考，也期待未来在更复杂多变的环境中催生出更智能可靠的检测系统，共同守护健康的信息生态。\nReferences (Selected)\nAuthor/Source Reference: BM25S（BM25 Sparse）/ @汤圆键盘坏了不能写论文\n",
  "wordCount" : "1037",
  "inLanguage": "zh",
  "datePublished": "2026-01-04T00:00:00Z",
  "dateModified": "2026-01-04T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lyuxi Liu"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://llspublic.github.io/zh/posts/rag/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lyuxi Liu",
    "logo": {
      "@type": "ImageObject",
      "url": "https://llspublic.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://llspublic.github.io/zh/" accesskey="h" title="Lyuxi Liu (Alt + H)">Lyuxi Liu</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                    <ul class="lang-switch"><li>|</li>
                        <li>
                            <a href="https://llspublic.github.io/" title="English"
                                aria-label="English">En</a>
                        </li>
                    </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://llspublic.github.io/zh/about/" title="關於">
                    <span>關於</span>
                </a>
            </li>
            <li>
                <a href="https://llspublic.github.io/zh/archive/" title="歸檔">
                    <span>歸檔</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      虚假信息检索增强检测：理论、技术与实战教程
    </h1>
    <div class="post-meta"><span title='2026-01-04 00:00:00 +0000 UTC'>2026年1月4日</span>&nbsp;·&nbsp;<span>Lyuxi Liu</span>&nbsp;|&nbsp;<span>语言:</span>
<ul class="i18n_list">
    <li>
        <a href="https://llspublic.github.io/posts/rag/">En</a>
    </li>
</ul>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e8%99%9a%e5%81%87%e4%bf%a1%e6%81%af%e6%a3%80%e7%b4%a2%e5%a2%9e%e5%bc%ba%e6%a3%80%e6%b5%8b%e7%90%86%e8%ae%ba%e6%8a%80%e6%9c%af%e4%b8%8e%e5%ae%9e%e6%88%98%e6%95%99%e7%a8%8b" aria-label="虚假信息检索增强检测：理论、技术与实战教程">虚假信息检索增强检测：理论、技术与实战教程</a><ul>
                        
                <li>
                    <a href="#%e5%af%bc%e8%a8%80" aria-label="导言">导言</a></li>
                <li>
                    <a href="#%e7%90%86%e8%ae%ba%e8%83%8c%e6%99%af%e4%bf%a1%e6%81%af%e5%a4%b1%e5%ba%8f%e7%9a%84%e6%a6%82%e5%bf%b5%e4%b8%8e%e5%bf%83%e7%90%86%e6%9c%ba%e5%88%b6" aria-label="理论背景：信息失序的概念与心理机制">理论背景：信息失序的概念与心理机制</a><ul>
                        
                <li>
                    <a href="#1-%e4%bf%a1%e6%81%af%e5%a4%b1%e5%ba%8f%e7%9a%84%e6%a6%82%e5%bf%b5%e4%b8%8e%e5%88%86%e7%b1%bb" aria-label="1. 信息失序的概念与分类">1. 信息失序的概念与分类</a></li>
                <li>
                    <a href="#2-%e4%bf%a1%e6%81%af%e5%a4%b1%e5%ba%8f%e7%9a%84%e5%bf%83%e7%90%86%e6%9c%ba%e5%88%b6" aria-label="2. 信息失序的心理机制">2. 信息失序的心理机制</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%a3%80%e6%b5%8b%e8%8c%83%e5%bc%8f%e7%9a%84%e6%bc%94%e5%8c%96%e4%bb%8e%e5%88%86%e7%b1%bb%e5%99%a8%e5%88%b0%e6%a3%80%e7%b4%a2-%e9%aa%8c%e8%af%81" aria-label="检测范式的演化：从分类器到“检索-验证”">检测范式的演化：从分类器到“检索-验证”</a></li>
                <li>
                    <a href="#%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e6%a8%a1%e5%9d%97%e8%af%a6%e8%a7%a3" aria-label="关键技术模块详解">关键技术模块详解</a><ul>
                        
                <li>
                    <a href="#1-%e7%a8%80%e7%96%8f%e6%a3%80%e7%b4%a2bm25s-%e7%ae%97%e6%b3%95" aria-label="1. 稀疏检索：BM25S 算法">1. 稀疏检索：BM25S 算法</a></li>
                <li>
                    <a href="#2-%e8%af%ad%e4%b9%89%e6%a3%80%e7%b4%a2%e5%8f%8c%e5%a1%94%e6%a8%a1%e5%9e%8b%e4%b8%8e%e5%90%91%e9%87%8f%e8%a1%a8%e7%a4%ba" aria-label="2. 语义检索：双塔模型与向量表示">2. 语义检索：双塔模型与向量表示</a></li>
                <li>
                    <a href="#3-%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%e6%8e%a8%e7%90%86%e8%95%b4%e5%90%ab%e5%bc%8f%e7%9a%84%e7%9c%9f%e5%81%87%e5%88%a4%e6%96%ad" aria-label="3. 自然语言推理：蕴含式的真假判断">3. 自然语言推理：蕴含式的真假判断</a></li>
                <li>
                    <a href="#4-%e7%bb%93%e6%9e%9c%e8%81%9a%e5%90%88%e4%b8%8e%e6%a0%87%e7%ad%be%e6%98%a0%e5%b0%84" aria-label="4. 结果聚合与标签映射">4. 结果聚合与标签映射</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%ae%9e%e6%93%8d%e6%b5%81%e7%a8%8b%e4%b8%8e%e4%bb%a3%e7%a0%81%e6%bc%94%e7%a4%ba" aria-label="实操流程与代码演示">实操流程与代码演示</a><ul>
                        
                <li>
                    <a href="#1-%e4%b8%bb%e5%bc%a0%e6%a3%80%e6%b5%8bcheck-worthiness-classification" aria-label="1. 主张检测（Check-worthiness Classification）">1. 主张检测（Check-worthiness Classification）</a></li>
                <li>
                    <a href="#2-%e8%af%81%e6%8d%ae%e6%a3%80%e7%b4%a2" aria-label="2. 证据检索">2. 证据检索</a></li>
                <li>
                    <a href="#3-%e4%ba%8b%e5%ae%9e%e6%a0%b8%e6%9f%a5%e5%88%a4%e6%96%adnli%e6%8e%a8%e7%90%86" aria-label="3. 事实核查判断（NLI推理）">3. 事实核查判断（NLI推理）</a></li>
                <li>
                    <a href="#4-%e8%a7%a3%e9%87%8a%e7%94%9f%e6%88%90" aria-label="4. 解释生成">4. 解释生成</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%a1%88%e4%be%8b%e7%a0%94%e7%a9%b6%e7%96%ab%e8%8b%97%e8%b0%a3%e8%a8%80%e4%b8%8e%e6%b0%94%e5%80%99%e5%8f%98%e5%8c%96%e5%8f%99%e4%ba%8b" aria-label="案例研究：疫苗谣言与气候变化叙事">案例研究：疫苗谣言与气候变化叙事</a><ul>
                        
                <li>
                    <a href="#%e6%a1%88%e4%be%8b-1%e7%96%ab%e8%8b%97%e8%b0%a3%e8%a8%80" aria-label="案例 1：疫苗谣言">案例 1：疫苗谣言</a></li>
                <li>
                    <a href="#%e6%a1%88%e4%be%8b-2%e6%b0%94%e5%80%99%e5%8f%98%e5%8c%96%e5%8f%99%e4%ba%8b" aria-label="案例 2：气候变化叙事">案例 2：气候变化叙事</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%89%8d%e6%b2%bf%e6%8e%a2%e7%b4%a2%e4%b8%8e%e6%8c%91%e6%88%98" aria-label="前沿探索与挑战">前沿探索与挑战</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="虚假信息检索增强检测理论技术与实战教程">虚假信息检索增强检测：理论、技术与实战教程<a hidden class="anchor" aria-hidden="true" href="#虚假信息检索增强检测理论技术与实战教程">#</a></h1>
<h2 id="导言">导言<a hidden class="anchor" aria-hidden="true" href="#导言">#</a></h2>
<p>在生成式人工智能重塑信息生态的当下，“后真相”时代的信息传播正变得空前复杂。从真假难辨的 Deepfake 视频到公共卫生领域层出不穷的阴谋论，信息失序（Information Disorder）已演变为一种复杂的社会技术现象，而不仅是简单的“真”或“假”二元问题。传统的“假新闻”一词因过度政治化已失去精确含义，难以概括当前多样化的失实信息类型。为了有效研究和治理信息失序，我们需要建立严谨的概念框架和技术体系，既涵盖理论上的分类与认知机制，又包含实用的自动化检测流程。</p>
<p>本文面向传播学领域的研究者，系统介绍信息失序的理论背景与心理机制，阐述检测范式的演化，详解关键技术模块（包括稀疏检索、语义检索、自然语言推理推断及结果聚合），并通过实操代码演示自动化事实核查系统的构建方法。随后，我们将结合案例研究（如疫苗谣言与气候变化叙事）说明模型如何应对披着“科学”外衣的虚假叙事。最后，讨论当前方法的前沿探索与挑战，包括检索增强生成（RAG）架构的整合、使用 LLM 评估解释的机制（G-Eval）、零样本与多模态检测难题，以及模型的优势与局限。整篇文章力求结构严谨、简明学术，帮助传播学研究者构建从理论到实战的完整认知。</p>
<hr>
<h2 id="理论背景信息失序的概念与心理机制">理论背景：信息失序的概念与心理机制<a hidden class="anchor" aria-hidden="true" href="#理论背景信息失序的概念与心理机制">#</a></h2>
<h3 id="1-信息失序的概念与分类">1. 信息失序的概念与分类<a hidden class="anchor" aria-hidden="true" href="#1-信息失序的概念与分类">#</a></h3>
<p>要研究虚假信息，首先需要厘清概念边界。信息失序（Information Disorder）框架由 Wardle 和 Derakhshan 在 2017 年提出，用真实性和意图两个维度严格定义了失实信息的类型。该框架将网络谣言现象分为三类：</p>
<ul>
<li><strong>误导信息（Misinformation）：</strong> 内容客观上是虚假的，但分享者并无主观恶意。例如由于知识欠缺或统计疏忽导致传播了错误信息。</li>
<li><strong>虚假信息（Disinformation）：</strong> 内容虚假且有意制造或传播以误导或伤害公众，带有明确的恶意动机，包括编造的政治阴谋论或伪造宣传。</li>
<li><strong>恶意信息（Malinformation）：</strong> 内容本身可能真实，但被断章取义或恶意泄露，用于造成伤害。例如泄露隐私真相来抹黑他人。</li>
</ul>
<p>上述分类为研究者提供了严谨的本体基础，也为数据标注提供了标准。在此基础上，Wardle 进一步细分出七种常见的信息失序形态，构成从轻度误导到完全捏造的连续谱：</p>
<ol>
<li><strong>讽刺与恶搞（Satire/Parody）：</strong> 本意并非欺骗，但可能被受众当真。</li>
<li><strong>错误连接（False Connection）：</strong> 标题或配图与内容不符，以标题党方式误导。</li>
<li><strong>误导性内容（Misleading Content）：</strong> 通过选取部分事实或断章取义营造误导性的印象。</li>
<li><strong>错误语境（False Context）：</strong> 将真实内容放在错误的时间或背景下呈现，以产生误导。</li>
<li><strong>冒充内容（Imposter Content）：</strong> 伪装成权威机构或来源发布虚假信息。</li>
<li><strong>篡改内容（Manipulated Content）：</strong> 对真实内容进行篡改、编辑或加工，使其传递错误信息。</li>
<li><strong>捏造内容（Fabricated Content）：</strong> 完全虚构的新内容，毫无真实依据，旨在欺骗受众。</li>
</ol>
<p>上述分类强调，不同类型虚假信息的特征迥异。例如讽刺内容往往带有幽默元素，而捏造内容则可能精心伪装成新闻报道。若不加区分地将它们混为一谈（统称为“假新闻”），模型训练只会学到噪音。因此，研究者应针对具体类别定制检测策略和特征工程，以提高模型的精度。</p>
<h3 id="2-信息失序的心理机制">2. 信息失序的心理机制<a hidden class="anchor" aria-hidden="true" href="#2-信息失序的心理机制">#</a></h3>
<p>传播学不仅关心信息如何传播，更关注个体为何相信失实信息。研究表明，人们并非因为无知才信谣言，高知识人群也可能受骗，这是由于人类认知中存在一系列偏误和捷径。如果模型能融合这些心理学机制，将有助于预测谁更容易受虚假信息影响。</p>
<p>几种核心的心理因素包括：</p>
<ul>
<li><strong>身份认同偏差：</strong> 也称动机性推理或群体认同偏见。人们倾向于根据自身的党派或群体身份来选择性地接受信息，自动筛选出符合自身立场的内容。这种动机性的认知处理使得即便有相反证据，人们仍坚持原有信念，以维护群体认同。简言之，政治立场会“过滤”事实，高度党派化的人会对符合其意识形态的错误信息更容易信以为真。</li>
<li><strong>认知懒惰：</strong> 即缺乏认知反思或深入思考的倾向。心理学将人类思维分为依赖直觉的“系统1”和理性分析的“系统2”。有些人更依赖直觉而不愿费脑思考，从而对谣言缺乏质疑。研究发现，分析性思维能力低的人更容易相信假新闻，其易感性与“懒于思考”密切相关。这意味着假消息传播往往不是因为过度党派偏见，而是由于缺乏理性思考的习惯。</li>
<li><strong>虚幻真相效应：</strong> 仅因为重复多次听到某个说法，人们就更倾向认为它是真的，即使客观上它是假的。认知心理学称此为 illusory truth effect（真相幻觉效应）。重复让信息变得熟悉，熟悉度又被大脑误用为真实性的线索，从而提高错误信息的可信度。哪怕受众原本知道真实情况，反复的曝光仍可能削弱其判断。</li>
<li><strong>情感唤起：</strong> 带有强烈情绪的信息（如恐惧、愤怒、道德义愤）更容易吸引注意并广泛传播。高唤起的信息可绕过理性分析直接影响判断。当人们情绪高涨时，对信息真假的辨别力会下降，更可能草率地转发或相信与情绪吻合的说法。</li>
</ul>
<p>除了上述心理机制，传播数据还揭示了一些反直觉的现象。例如，我们容易高估虚假信息对全民的影响，仿佛所有人都被“洗脑”，却低估了人的主动性。实证研究显示：大部分普通用户接触到的谣言极少，虚假信息的大规模传播往往集中在极少数活跃的边缘群体中。虚假信息的泛滥更像是“一小群人的狂欢”，而非“全民皆中招”。因此，治理上应将重点从全网扫描转向对高危易感人群的精准干预。这一点提醒我们，不应陷入“算法决定论”的陷阱——社交媒体算法并非全能造就谣言的元凶，人的需求和选择同样重要。</p>
<p>最后，需要特别关注那些披着科学外衣的误导性叙事。科学议题上的虚假信息通常更隐蔽，不直接谎报事实，而是通过框架重构来扭曲认知。例如，在气候变化议题上，不同措辞（“全球变暖”vs“气候变化”）会影响不同群体的接受度。研究发现，相比“全球变暖”，美国保守派对“气候变化”这一表述的抵触更小。这种语义差异本质上反映了立场的不同。再如疫苗谣言，常引用听起来“科学”的话术（例如引用已被撤稿的 Wakefield 伪研究）来博取信任。这些内容并非编造新事实，而是利用现有科学名词或数据进行误导，使得传统真伪检测更加困难。这对自动检测系统提出了更高要求：不仅要判断真假，还要捕捉立场和框架是否有偏差。</p>
<p>综上，信息失序已不是简单的谣言问题，而是包含多维分类和深层心理机制的复杂现象。理解这些理论背景，有助于设计更有针对性的检测范式与模型。</p>
<hr>
<h2 id="检测范式的演化从分类器到检索-验证">检测范式的演化：从分类器到“检索-验证”<a hidden class="anchor" aria-hidden="true" href="#检测范式的演化从分类器到检索-验证">#</a></h2>
<p>面对层出不穷的虚假信息，早期研究往往尝试训练端到端的文本分类模型（如判断一则消息是否是假新闻）。这类传统二分类方法虽然直观，但存在明显不足：它们仅根据输入文本本身做出真伪判断，缺乏对判断依据的解释。在实践中，这容易遭遇信任危机——模型给出“真”或“假”的标签，却无法说明“为什么”，让人无从信服模型的结论。</p>
<p>为提高检测的可信度和效果，近年来出现了**“检索-验证”（Retrieve-then-Verify）的范式**。该范式模拟人类事实核查员的工作流程：先针对待验证的声明去检索外部证据，再根据证据对声明真伪进行判定。这一流程被证明比单纯的文本分类更具可解释性和鲁棒性：</p>
<ol>
<li><strong>证据检索：</strong> 针对每个需要核查的主张（claim），从大型可信知识库中检索相关文档或句子作为证据来源。例如针对某政治言论，检索维基百科或权威新闻报道中提及相同事件或数据的片段。</li>
<li><strong>自然语言推理：</strong> 将检索到的证据与原主张进行对比分析，由 NLI 模型判断证据对主张的关系——是支持（entails）、反驳（contradicts），还是无关（neutral）。这一过程相当于逻辑推理，确定主张在证据面前是否站得住脚。</li>
<li><strong>结果聚合：</strong> 综合多条证据的推理结果，给出最终的判定。例如，如果多数证据支持主张且无矛盾，则判定为真；若有可靠证据反驳主张，则判定为假；若证据不足则标记为信息不足。这一聚合策略确保模型决策基于充分的证据集合，而非孤立单一来源。</li>
</ol>
<p>“检索-验证”范式的优点在于过程可解释。模型不仅输出结论，还可以同时提供它参考的证据（如引用哪篇维基文章的句子），使用户清楚判定依据。例如，模型可以说：“这条健康谣言为假，因为权威医学研究 X 表明与其说法相反。” 这样的检测系统更容易获得用户和事实核查员的信任。此外，该范式还能更好地应对开放域的未知主张：即使主张本身不在训练数据中，模型也能通过检索新信息来做出判断，具备一定的零样本能力。</p>
<p>需要强调的是，在“检索-验证”框架下，我们不再鼓励训练一个不透明的端到端假新闻分类器。相反，我们构建的是一个模块化管道：每个步骤（检索、推理、聚合）都可独立优化，并能产出中间可解释结果（如检索列表、证据标注），从而整体上提高系统的透明度和可解释性。这对于传播学研究尤为重要——当我们用 AI 模型分析舆论中的谣言传播时，需要清楚模型基于哪些事实依据得出了结论，以便与新闻专业主义和社会科学理论接轨。</p>
<p>总之，检测范式正从“黑箱式”的文本分类，演进到仿人的“检索证据→逻辑验证”流程。下一节我们将深入解读这一流程中的关键技术模块。</p>
<hr>
<h2 id="关键技术模块详解">关键技术模块详解<a hidden class="anchor" aria-hidden="true" href="#关键技术模块详解">#</a></h2>
<p>要实现检索增强的检测系统，需要将多个 NLP 技术模块有机结合。下面我们详细介绍其中的核心组件：稀疏检索、语义检索、自然语言推理（NLI），以及结果聚合与标签映射策略。</p>
<h3 id="1-稀疏检索bm25s-算法">1. 稀疏检索：BM25S 算法<a hidden class="anchor" aria-hidden="true" href="#1-稀疏检索bm25s-算法">#</a></h3>
<p>稀疏检索（Sparse Retrieval）指传统基于关键词匹配的检索方法，其代表之一是 BM25。BM25 是一种对 TF-IDF 加以改进的概率检索模型，对文档和查询词频进行了饱和度处理和长度归一化。简单来说，BM25 会根据查询和文档之间共享的词汇，以及词在文档中的频率和在整个语料库中的逆文档频率，来计算相关评分。它擅长精确匹配查询词，对于人名、地名等实体词汇往往非常有效。</p>
<p>在本文介绍的系统中，我们使用了 BM25 的一个高效实现变体，称为 BM25S（BM25 Sparse）算法。BM25S 在 Python 中有现成库可用（如 <code>bm25s</code>），具有以下特点：</p>
<ul>
<li><strong>精确匹配：</strong> 对专有名词、数字等精确匹配需求的查询表现出色。如果声明中出现具体的人名地名，BM25 能迅速锁定包含这些实体的文档。相比之下，一些语义向量模型可能因为过度泛化而错过这些精确匹配。</li>
<li><strong>高效索引：</strong> BM25S 使用倒排索引结构，针对大型语料库检索速度极快，可在上百万文档规模下实现毫秒级查询响应。</li>
<li><strong>长度归一：</strong> BM25 公式中引入了文档长度惩罚，避免长文档因为涵盖更多词而获得不公平的高分。这提高了相关性评分的公平性。</li>
<li><strong>实现简便：</strong> 使用 BM25S 需要对语料进行分词和建立索引。一旦建好索引对象，就可以高效地对任意查询进行检索。作为 RAG 系统的“第一道防线”，稀疏检索模块实现相对简单且稳定。</li>
</ul>
<p><strong>代码示例：</strong> 假设我们有一个证据句子库 <code>corpus</code>（每个元素为一条证据句文本），可以使用 BM25S 进行索引和检索：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> bm25s
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 构建BM25索引</span>
</span></span><span style="display:flex;"><span>retriever <span style="color:#f92672">=</span> bm25s<span style="color:#f92672">.</span>BM25(corpus<span style="color:#f92672">=</span>corpus)
</span></span><span style="display:flex;"><span>retriever<span style="color:#f92672">.</span>index(bm25s<span style="color:#f92672">.</span>tokenize(corpus))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 对查询主张执行检索</span>
</span></span><span style="display:flex;"><span>claim <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;The number of new cases of shingles per year extends from 1.2-3.4 per 1,000.&#34;</span>
</span></span><span style="display:flex;"><span>results, scores <span style="color:#f92672">=</span> retriever<span style="color:#f92672">.</span>retrieve(bm25s<span style="color:#f92672">.</span>tokenize(claim), k<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(results[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">3</span>])  <span style="color:#75715e"># 打印前3条最相关证据句</span>
</span></span></code></pre></div><p>上述代码将返回与 <code>claim</code> 内容高度重合的证据文本列表。BM25 会优先返回包含了诸如“shingles”（带状疱疹）、“1.2-3.4 per 1,000”等关键词的句子，因为这些稀疏特征直接匹配，提高了相关性得分。</p>
<h3 id="2-语义检索双塔模型与向量表示">2. 语义检索：双塔模型与向量表示<a hidden class="anchor" aria-hidden="true" href="#2-语义检索双塔模型与向量表示">#</a></h3>
<p>稀疏检索虽然高效，但它依赖表层词汇匹配，对同义表达和隐含语义无能为力。例如声明使用了“导致”，而证据用词是“引发”，BM25 可能因为字面不匹配而错失相关证据。为解决这个问题，我们需要借助稠密检索（Dense Retrieval）方法，即通过语义向量表示来捕捉文本的深层含义。</p>
<p>双塔模型（Dual Encoder）是常用的稠密检索架构。它包含两个塔（神经网络），分别将查询和文档编码为向量，并在向量空间中通过相似度（通常是余弦相似度）来衡量相关性。典型实现如 Facebook 提出的 DPR（Dense Passage Retriever）模型，或者更通用的 Sentence Transformers 库提供的双塔预训练模型。</p>
<p>在我们的系统中，我们采用预训练好的句向量模型（如 <code>sentence-transformers</code> 提供的 <code>all-MiniLM-L6-v2</code>）将文本映射为向量。其要点包括：</p>
<ul>
<li><strong>高维语义映射：</strong> 模型将每条文本（声明或证据句）映射到一个高维向量，使得语义相似的文本在向量空间中距离更近。这意味着哪怕两个句子没有任何相同的字词，只要意思相关，向量也会彼此靠拢。</li>
<li><strong>同义词与概念对齐：</strong> 向量表示可以自动学习到同义表达的接近性。如“引发”和“导致”两个词语，它们出现在相似语境中时，模型会将它们嵌入到相邻的位置。</li>
<li><strong>双塔编码：</strong> 查询和文档分别编码，计算相似度时无需交叉输入。这种结构便于离线预计算文档向量：我们可以提前算好整个证据库中每个句子的向量并存储。当有新查询时，只需算一次查询向量，然后与所有证据向量计算相似度即可，大幅提升检索效率（常用工具如 Faiss、Milvus 能支持快速向量近邻搜索）。</li>
<li><strong>混合检索：</strong> 实践中，我们常将 BM25 结果与向量检索结果融合，以兼顾精确匹配和语义召回。这样可以最大化召回率——既不会错过同义表述的相关证据，也确保精确实体匹配不被语义模型漏掉。</li>
</ul>
<p><strong>代码示例：</strong> 使用 SentenceTransformer 加载模型并检索相似语义证据：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sentence_transformers <span style="color:#f92672">import</span> SentenceTransformer, util
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 加载预训练的轻量级句向量模型</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SentenceTransformer(<span style="color:#e6db74">&#39;all-MiniLM-L6-v2&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算语料库证据句的向量表示（建议离线预计算并存储）</span>
</span></span><span style="display:flex;"><span>evidence_embeddings <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode(corpus, convert_to_tensor<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算查询主张的向量</span>
</span></span><span style="display:flex;"><span>claim_embedding <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode(claim, convert_to_tensor<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算余弦相似度并获取前10个最相近证据索引</span>
</span></span><span style="display:flex;"><span>cosine_scores <span style="color:#f92672">=</span> util<span style="color:#f92672">.</span>cos_sim(claim_embedding, evidence_embeddings)[<span style="color:#ae81ff">0</span>]  <span style="color:#75715e"># 余弦相似度向量</span>
</span></span><span style="display:flex;"><span>top_indices <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argpartition(<span style="color:#f92672">-</span>cosine_scores, range(<span style="color:#ae81ff">10</span>))[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">10</span>]         <span style="color:#75715e"># 前10高分索引（无序）</span>
</span></span><span style="display:flex;"><span>top_indices <span style="color:#f92672">=</span> top_indices[cosine_scores[top_indices]<span style="color:#f92672">.</span>argsort(descending<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)]  <span style="color:#75715e"># 按相似度排序</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 查看最相关的一条证据</span>
</span></span><span style="display:flex;"><span>top_evidence <span style="color:#f92672">=</span> corpus[top_indices[<span style="color:#ae81ff">0</span>]]
</span></span><span style="display:flex;"><span>print(top_evidence)
</span></span></code></pre></div><p>这段代码利用 <code>sentence-transformers</code> 库的 <code>util.cos_sim</code> 快捷计算了相似度，并选出前 10 条证据。相比 BM25，语义检索能够找出与主张意思相近但未必有明显词重叠的句子。例如，对于“疫苗接种会导致自闭症”这样的谣言主张，语义检索可以找到科学论文中的相关句子“研究已未发现疫苗与自闭症存在关联”之类的证据句，尽管表面词汇不同，但意义上直接反驳了谣言。</p>
<p><strong>性能评估：</strong> 为衡量检索模块的效果，可使用 <strong>MRR（Mean Reciprocal Rank）</strong> 指标。MRR 关注第一个相关结果出现在检索列表中的位置，位置越靠前得分越高。公式是对每个查询计算其相关结果的倒数排名，然后取平均值。以伪代码为例：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mean_reciprocal_rank</span>(results_list, ground_truth_list):
</span></span><span style="display:flex;"><span>    rs <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> results, true_doc <span style="color:#f92672">in</span> zip(results_list, ground_truth_list):
</span></span><span style="display:flex;"><span>        ranks <span style="color:#f92672">=</span> [idx <span style="color:#66d9ef">for</span> idx, doc <span style="color:#f92672">in</span> enumerate(results) <span style="color:#66d9ef">if</span> doc <span style="color:#f92672">==</span> true_doc]
</span></span><span style="display:flex;"><span>        rs<span style="color:#f92672">.</span>append(<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (ranks[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">if</span> ranks <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>mean(rs)
</span></span></code></pre></div><p>我们希望 MRR 越高越好，意味着大部分查询的正确证据在检索结果中排名很靠前。一般而言，将 BM25 和向量检索结合可以明显提升 MRR，因为两者互补弥补了各自盲区。</p>
<h3 id="3-自然语言推理蕴含式的真假判断">3. 自然语言推理：蕴含式的真假判断<a hidden class="anchor" aria-hidden="true" href="#3-自然语言推理蕴含式的真假判断">#</a></h3>
<p>完成证据检索后，下一步是判断证据与声明的关系。这被形式化为一个自然语言推理（NLI）任务：给定一句声明和一段证据文本，机器需要判断证据是否支持声明、反驳声明，或与声明无关。这一转换非常关键——它把事实核查问题提升为一种逻辑推理，从而赋予模型跨领域的一般推理能力。即使模型没见过特定谣言，也可以通过 NLI 的推理技能来判断证据和主张的矛盾与否。</p>
<p>我们通常采用已经在大规模 NLI 数据集（如 MultiNLI，SNLI 等）上训练微调过的 Transformer 模型来执行这一任务。零样本设置下，一个在 NLI 任务上训练良好的模型可以直接泛化到事实核查标签，无需额外针对谣言数据微调。例如，著名的 FEVER 论文就将事实核查标签映射到 NLI 三分类：Entailment 对应“Supports”，Contradiction 对应“Refutes”，Neutral 对应“Not Enough Info”。</p>
<p>NLI 模块的实现要点包括：</p>
<ul>
<li><strong>输入构造：</strong> 将声明和证据拼接成模型输入对。如标准做法是在 Transformer 输入中构造 <code>[CLS] 声明 [SEP] 证据 [SEP]</code> 的格式，让模型同时读入二者并利用注意力机制分析它们的关系。这样模型可以对照证据内容理解声明是否被支持或否定。</li>
<li><strong>交叉编码（Cross-Encoder）：</strong> 用于 NLI 判断的模型通常是单塔结构（不同于前述双塔）——即将声明和证据拼接后送入同一个 Transformer 编码。这种 Cross-Encoder 可以利用跨句的注意力捕捉微妙的逻辑关系，比独立编码再比对更加精细。这也使得 Cross-Encoder 常用于对检索结果重排序，因为它能更准确地区分细微相关性。</li>
<li><strong>标签映射：</strong> NLI 模型输出通常为三类之一：Entailment（蕴含）、Neutral（中立）、Contradiction（矛盾）。我们需要将其映射到事实核查的标签空间。例如，可对应映射为：Supports（支持）、Not Enough Info（信息不足）、Refutes（反驳）。这个映射策略确保模型判断与人类判定标准一致。</li>
</ul>
<p><strong>代码示例：</strong> 利用 🤗 Transformers 的 pipeline 快速进行 NLI 判断：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 加载在NLI数据上微调的RoBERTa大型模型（支持零样本推理）</span>
</span></span><span style="display:flex;"><span>nli_pipeline <span style="color:#f92672">=</span> pipeline(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;text-classification&#34;</span>,
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 示例：声明和证据</span>
</span></span><span style="display:flex;"><span>claim_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;The number of new cases of shingles per year extends from 1.2-3.4 per 1,000.&#34;</span>
</span></span><span style="display:flex;"><span>evidence_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;The number of new cases per year ranges from 1.2 -- 3.4 per 1,000...&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 构造输入对并预测</span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> nli_pipeline([{<span style="color:#e6db74">&#39;text&#39;</span>: claim_text, <span style="color:#e6db74">&#39;text_pair&#39;</span>: evidence_text}])[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>print(result[<span style="color:#e6db74">&#39;label&#39;</span>])  <span style="color:#75715e"># 可能输出 &#39;ENTAILMENT&#39; 或 &#39;CONTRADICTION&#39; 等</span>
</span></span><span style="display:flex;"><span>print(result[<span style="color:#e6db74">&#39;score&#39;</span>])  <span style="color:#75715e"># 概率分数</span>
</span></span></code></pre></div><p>在这个例子中，如果 <code>evidence_text</code> 内容与 <code>claim_text</code> 逻辑一致，模型应输出 Entailment（蕴含）；如果证据明显反驳了声明则输出 Contradiction。随后我们将 Entailment 映射为“支持”标签，Contradiction 映射为“反驳”，Neutral 映射为“信息不足”。</p>
<p>需要注意的是，证据可能有多条，模型应对每条证据分别判断，然后聚合结果。这属于下一节要讨论的策略。不过简单而言，一种常用规则是：若有任何一条证据被判定为 Contradiction（反驳），则综合判定声明为 False；否则如果至少一条证据是 Entailment 且无 Contradiction，则判定为 True；若既无 Entailment 也无 Contradiction，则判为未知。这类似于事实核查领域的 FEVER 评测标准。</p>
<p>除了使用专门微调的判别式模型，近年来还出现了利用 <strong>大语言模型（LLM）</strong> 做零样本 NLI 的思路。即通过提示（prompt）让 GPT-4 等模型直接阅读证据并给出支持/反驳判断。举例来说，可以构造如下 Prompt：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>You are a fact-checking assistant. Decide if the claim is supported, refuted, or not enough info based on the statements.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Claim: &lt;填入待判定声明&gt;
</span></span><span style="display:flex;"><span>Statements:
</span></span><span style="display:flex;"><span>1. &lt;证据句1&gt;
</span></span><span style="display:flex;"><span>2. &lt;证据句2&gt;
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Answer with one of: SUPPORTS / REFUTES / NOT ENOUGH INFO.
</span></span></code></pre></div><p>将此提示提交给强大的 LLM（如 GPT-4），理论上可以得到正确的标签。我们的教程实验中，使用一个小型 GPT-4 同架构模型（称为 GPT-4o mini）实现了零样本分类，结果达到约 79.6% 的准确率，接近专门微调的 RoBERTa 模型（83.2%）。这说明在特定任务上，经过微调的小型模型往往在性价比和效果上优于通用的大型模型。不过 LLM 的方法无需训练，易于扩展到新任务，仍具有很大实用价值。</p>
<h3 id="4-结果聚合与标签映射">4. 结果聚合与标签映射<a hidden class="anchor" aria-hidden="true" href="#4-结果聚合与标签映射">#</a></h3>
<p>当经过以上步骤后，我们对每个“声明-证据”对都有一个 NLI 模型输出结果。现实中，一个声明往往需要多条证据一起才能定论，因而最后需要聚合这些结果并输出最终标签。</p>
<p>标签映射我们已在上一节提到，即将 NLI 的三元分类映射回我们感兴趣的输出标签。以事实核查为例，一般输出三类：真实（True/Supported）、虚假（False/Refuted）、不确定（Not Enough Info/Neutral）。这个映射通常是一一对应的。但在其他任务中可能不同，比如有的谣言检测任务只分“谣言/非谣言”二类，则可能需要将 NLI 的 Entailment 视为“非谣言”，Contradiction 视为“谣言”，Neutral 也视作“谣言”（因为缺乏证据佐证，也不能确认其真）。具体取决于任务定义。</p>
<p>结果聚合则需要考虑多条证据的综合情况。常见策略包括：</p>
<ul>
<li><strong>证明式聚合：</strong> 如果至少有一条证据明确支持主张，且没有矛盾证据，则判定为真；若至少一条证据反驳主张，则判定为假；否则在无支持且无反驳的情况下为信息不足。这种策略类似于法律中的“存在一条有效证据即可定罪/免责”的思路，适用于多数事实核查任务。</li>
<li><strong>多数投票：</strong> 当证据质量可能参差不齐时，可让多条证据各自投票，以投票结果决定标签。例如 3 条证据中 2 条支持 1 条反驳，可决定支持。但这种简单投票未必可靠，因为证据并非独立同质，通常会优先考虑最权威或相关度最高的证据。</li>
<li><strong>加权融合：</strong> 根据证据的检索得分或来源可信度，对不同证据赋予权重，再累积它们属于各个类别的置信度。例如，如果一条高相关度证据显示反驳，那么即便其他几条模糊支持，也可能以高权重的反驳为准。</li>
<li><strong>学习融合模型：</strong> 在高级应用中，可以训练一个判决融合模型，输入是多条证据的 NLI logits 或嵌入，输出最终标签。这样模型可以学习如何在证据冲突时做决定。但这需要一定的训练数据支撑，不属零样本范畴。</li>
</ul>
<p>在我们的教程框架中，我们采用了简化的证明式规则：有反驳则 False，有支持无反驳则 True，都无则 NEI。这一规则虽然简单，但符合直觉且在 FEVER 任务中表现良好。事实上，FEVER 的评测也要求模型给出正确证据集合才能算判定正确，即所谓 FEVER 分数：只有标签正确且提交的证据包含完整支持/反驳链条，才计为成功。这鼓励系统在聚合时严谨考虑证据充分性，否则宁可输出“信息不足”。FEVER 分数衡量模型证明过程的准确率，因此比单纯标签准确率更苛刻，但也更实用。</p>
<p>小结一下技术模块：我们介绍了 BM25 稀疏检索保障了实体匹配，语义向量检索捕获了同义语义，NLI 推理进行逻辑判别，最后通过标签映射和结果聚合得到最终决策。接下来，我们将把这些模块串联起来，演示一个实际的自动化事实核查流程，并提供代码示例。</p>
<hr>
<h2 id="实操流程与代码演示">实操流程与代码演示<a hidden class="anchor" aria-hidden="true" href="#实操流程与代码演示">#</a></h2>
<p>下面我们按照实际建模的顺序，将前述模块依次串联，构建一个简化的谣言检测/事实核查系统，并提供关键代码片段。流程包括：主张检测（Check-Worthiness Detection）、证据检索（结合稀疏和稠密方法）、事实核查判断（NLI推理输出标签）、以及解释生成（由生成模型给出判定理由）。</p>
<h3 id="1-主张检测check-worthiness-classification">1. 主张检测（Check-worthiness Classification）<a hidden class="anchor" aria-hidden="true" href="#1-主张检测check-worthiness-classification">#</a></h3>
<p>在社交媒体帖子或政治演讲中，并非每一句话都值得去核查。大部分句子只是闲聊寒暄或表达观点，只有少数句子包含可验证的具体事实主张，需要我们进一步检索证据。这就提出了主张检测任务：从一段文本中识别出有核查价值的语句。这是一个二分类问题（可核查 vs 不需核查），但具有极端类别不平衡的特点——可核查的句子往往不到全部句子的 5%。</p>
<p>我们可以利用监督学习来训练一个主张检测模型。例如，使用一个预训练的 BERT 模型做二分类微调。为了应对类别不平衡，常用做法是对正例（可核查句）给予更大损失权重，或过采样正例等。在 HuggingFace 的 Trainer 框架下，可以自定义 loss 来实现。例如：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;bert-base-uncased&#34;</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForSequenceClassification<span style="color:#f92672">.</span>from_pretrained(model_name, num_labels<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 构造Dataset的数据映射</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">encode_examples</span>(examples):
</span></span><span style="display:flex;"><span>    outputs <span style="color:#f92672">=</span> tokenizer(examples[<span style="color:#e6db74">&#34;text&#34;</span>], truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;max_length&#34;</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>    outputs[<span style="color:#e6db74">&#34;labels&#34;</span>] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> lbl <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;Check-worthy&#34;</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">for</span> lbl <span style="color:#f92672">in</span> examples[<span style="color:#e6db74">&#34;label&#34;</span>]]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> outputs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>encoded_dataset <span style="color:#f92672">=</span> raw_dataset<span style="color:#f92672">.</span>map(encode_examples, batched<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义带加权损失的Trainer</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">WeightedLossTrainer</span>(Trainer):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, class_weights, <span style="color:#f92672">*</span>args, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>(<span style="color:#f92672">*</span>args, <span style="color:#f92672">**</span>kwargs)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>class_weights <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(class_weights)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_loss</span>(self, model, inputs, return_outputs<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>        labels <span style="color:#f92672">=</span> inputs<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;labels&#34;</span>)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> model(<span style="color:#f92672">**</span>inputs)
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>logits
</span></span><span style="display:flex;"><span>        loss_fct <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>CrossEntropyLoss(weight<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>class_weights)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fct(logits, labels)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> (loss, outputs) <span style="color:#66d9ef">if</span> return_outputs <span style="color:#66d9ef">else</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算类别权重，例如不核查:核查 ≈ 1:20 </span>
</span></span><span style="display:flex;"><span>weights <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">20.0</span>]
</span></span><span style="display:flex;"><span>trainer <span style="color:#f92672">=</span> WeightedLossTrainer(class_weights<span style="color:#f92672">=</span>weights, model<span style="color:#f92672">=</span>model, args<span style="color:#f92672">=</span>TrainingArguments(<span style="color:#f92672">...</span>),
</span></span><span style="display:flex;"><span>                              train_dataset<span style="color:#f92672">=</span>encoded_dataset[<span style="color:#e6db74">&#34;train&#34;</span>], eval_dataset<span style="color:#f92672">=</span>encoded_dataset[<span style="color:#e6db74">&#34;val&#34;</span>])
</span></span><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>train()
</span></span></code></pre></div><p>上述代码中，我们手动设置了类别权重为 1:20，意味着每个核查类样本的损失被放大 20 倍，从而引导模型更关注少数类。这种加权交叉熵是缓解类别失衡的有效手段。</p>
<p>训练完成的模型即可用于从输入文本中过滤出需要核查的句子列表。假定我们已经定位到若干值得核查的主张，接下来对每个主张进行事实核查。</p>
<h3 id="2-证据检索">2. 证据检索<a hidden class="anchor" aria-hidden="true" href="#2-证据检索">#</a></h3>
<p>拿到一个具体主张后，我们需要在海量非结构化文本库（知识库）中找到支持或反驳它的证据句子。我们将 BM25 稀疏检索和语义稠密检索结合使用。</p>
<ul>
<li><strong>BM25 检索：</strong> 首先用主张中的关键词去索引中搜索。代码示例如前文 BM25S 部分所示，这里不再重复。BM25 会返回一组候选证据句（如 Top 50）。由于 BM25 偏向精确匹配，这一步可以确保一些关键实体相关的文档不被遗漏。</li>
<li><strong>向量检索：</strong> 并行地，我们用语义向量对主张进行检索。代码示例在前文语义检索部分也给出。我们可以取向量相似度最高的一组候选句（如 Top 50）。向量检索会补充 BM25 遗漏的、语义相关但表述不同的证据。</li>
<li><strong>候选融合：</strong> 将两种方法得到的候选集合并，通常也就数十条句子，然后可能需要去重（因为有时两种方法会命中同一句）并简单排序。排序可以根据 BM25 分数或向量分数，或者训练一个学习排序模型。但出于简单，可按比如 BM25 得分标准排序。</li>
<li><strong>证据筛选：</strong> 为了减轻后续 NLI 模型负担，我们可以增加一层筛选。例如利用一个轻量的 Cross-Encoder 对候选证据再打分重排，选取 Top 5 送入下一步推理。这一步提升准确率但复杂度增加。若为了演示概念，可省略，直接使用所有候选。</li>
</ul>
<h3 id="3-事实核查判断nli推理">3. 事实核查判断（NLI推理）<a hidden class="anchor" aria-hidden="true" href="#3-事实核查判断nli推理">#</a></h3>
<p>对于每个主张，我们现在有若干候选证据句。我们让 NLI 模型对每对 主张-证据进行判断。如前文所述，使用经过训练的 NLI 模型可以直接给出“支持/反驳/无关”三分类结果。</p>
<p>伪代码如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>labels_map <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;ENTAILMENT&#34;</span>: <span style="color:#e6db74">&#34;SUPPORTS&#34;</span>, <span style="color:#e6db74">&#34;CONTRADICTION&#34;</span>: <span style="color:#e6db74">&#34;REFUTES&#34;</span>, <span style="color:#e6db74">&#34;NEUTRAL&#34;</span>: <span style="color:#e6db74">&#34;NOT ENOUGH INFO&#34;</span>}
</span></span><span style="display:flex;"><span>final_decisions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> claim <span style="color:#f92672">in</span> claims_to_check:
</span></span><span style="display:flex;"><span>    decisions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> evidence <span style="color:#f92672">in</span> candidate_evidences[claim]:
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> nli_pipeline({<span style="color:#e6db74">&#39;text&#39;</span>: claim, <span style="color:#e6db74">&#39;text_pair&#39;</span>: evidence})
</span></span><span style="display:flex;"><span>        decisions<span style="color:#f92672">.</span>append(labels_map[result[<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#39;label&#39;</span>]])
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 结果聚合</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;REFUTES&#34;</span> <span style="color:#f92672">in</span> decisions:
</span></span><span style="display:flex;"><span>        final <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;False&#34;</span>  <span style="color:#75715e"># 有证据反驳</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> <span style="color:#e6db74">&#34;SUPPORTS&#34;</span> <span style="color:#f92672">in</span> decisions:
</span></span><span style="display:flex;"><span>        final <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;True&#34;</span>   <span style="color:#75715e"># 有支持且无反驳</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        final <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Not Enough Info&#34;</span>
</span></span><span style="display:flex;"><span>    final_decisions<span style="color:#f92672">.</span>append((claim, final))
</span></span></code></pre></div><p>上述逻辑实现了简单的证据聚合策略。在得到最终标签后，我们就完成了对主张真伪的判断。不过，仅给出标签往往不够。在谣言治理中，我们还希望解释模型为何这么判定。为此，引入下一步：解释生成。</p>
<h3 id="4-解释生成">4. 解释生成<a hidden class="anchor" aria-hidden="true" href="#4-解释生成">#</a></h3>
<p>为了提高检测结果的可接受度和有效性，我们希望模型能生成一段自然语言解释，说明它基于哪些证据、如何推理得出了结论。例如，对于输出“False”的谣言，解释可能是：“因为证据 X 表明情况恰好相反，因此该主张不成立”。这样的解释不仅帮助受众理解真相，也对抗了谣言的说服力。</p>
<p>生成解释可被视为一个 NLG（自然语言生成）任务，我们可以使用大型生成模型（如 GPT）来完成。在实践中，我们可以设计一个 Prompt，让模型根据主张、标签和证据来生成解释。例如：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>You are tasked with creating an explanation of a fact check.
</span></span><span style="display:flex;"><span>Claim: &lt;主张&gt;
</span></span><span style="display:flex;"><span>Label: &lt;模型判定的标签（True/False/Not Enough Info）&gt;
</span></span><span style="display:flex;"><span>Statements: 
</span></span><span style="display:flex;"><span>- &lt;证据句1&gt;
</span></span><span style="display:flex;"><span>- &lt;证据句2&gt;
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Please provide a brief explanation as to why the label is correct based on the provided evidence.
</span></span></code></pre></div><p>将上述模板填入具体内容后提交给如 GPT-4o mini 模型，它会生成一段解释说明。例如，对于一个谣言主张“Maze Runner 是一项体育比赛”而证据显示 Maze Runner 是电影制片人的例子，它可能生成解释：</p>
<blockquote>
<p>“‘反驳’标签是正确的，因为证据明确指出《移动迷宫》（The Maze Runner）是一部由 Ellen Goldsmith-Vein 制作的电影，而该主张声称它是一项体育竞赛。”</p>
</blockquote>
<p>这个解释清晰地表述了证据与主张如何矛盾，从而支撑了“反驳”结论。</p>
<p><strong>解释评估：</strong> 生成的解释质量如何评估？传统的 NLG 评测指标如 BLEU、ROUGE 主要看字面重合，无法衡量解释与证据的逻辑一致性（即“忠实度”）。为此，我们引入 <strong>G-Eval</strong> 评估机制：让一个更强大的 LLM 充当评审，按照预先定义的标准打分。例如定义评分 1-5 分，其中 1 表示解释完全与证据不符，5 表示解释准确完备地反映了证据和主张的关系。</p>
<p>我们可以构造如下评价提示模板：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Evaluation Criteria:
</span></span><span style="display:flex;"><span>Faithfulness (1-5) - the factual alignment between the fact-checking explanation and the evidence.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Evidence Provided: &lt;证据文本&gt;
</span></span><span style="display:flex;"><span>Fact-Checking Explanation: &lt;模型生成的解释&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Evaluation: 请仅给出一个1到5的分数。
</span></span></code></pre></div><p>让 GPT-4 来多次（如 n=20 次）评估同一解释。由于 LLM 输出存在随机性，我们可以取多次评分的平均值作为最终得分，借此减少单次偏差。这样的评估方式利用 LLM 的理解和推理能力，被称为 LLM-as-a-judge 评估，也即 G-Eval 的基本思想。研究表明，G-Eval 能够在一定程度上以接近人类的一致性评价生成文本。</p>
<p>需要说明的是，解释生成是锦上添花的部分。在实际应用中，如果只是为了最终的自动判断，前三步已经可以完成谣言检测。但提供解释无疑会增强模型输出的可解释性和说服力，非常适合辅助人工审核、公众科普等场景。</p>
<p>至此，我们的自动化检测流程已经涵盖了从输入原始文本一直到输出带解释的判定结果的各个环节。下面通过实际案例来看看这一流程的作用。</p>
<hr>
<h2 id="案例研究疫苗谣言与气候变化叙事">案例研究：疫苗谣言与气候变化叙事<a hidden class="anchor" aria-hidden="true" href="#案例研究疫苗谣言与气候变化叙事">#</a></h2>
<p>为了更直观地理解上述模型如何应对“披着科学外衣”的虚假叙事，我们以两个社会热点为例演示。</p>
<h3 id="案例-1疫苗谣言">案例 1：疫苗谣言<a hidden class="anchor" aria-hidden="true" href="#案例-1疫苗谣言">#</a></h3>
<p><strong>情境：</strong> 某网络帖子声称：“研究证明接种 MMR（三联）疫苗会导致儿童患自闭症。” 这是一个经典的反疫苗叙事，它引用了“研究证明”以增加科学性。然而，该主张来源于 1998 年发表在《柳叶刀》上的一篇论文，后来已被证实是造假并正式撤稿。</p>
<ul>
<li><strong>主张检测：</strong> 模型会将这句话识别为可核查的主张，因为它涉及医学因果关系，影响重大且可查证。</li>
<li><strong>证据检索：</strong> BM25 可能使用关键词如“MMR”“autism”“study”检索到相关新闻或维基百科页面，向量检索则会捕捉“没有关联”“研究发现无证据”等语义相关的句子。实际证据很容易在医学权威来源找到，例如美国儿科学会（AAP）的新闻稿指出：“大量跨年代跨国家的研究均未发现疫苗与自闭症有可信关联。最初声称 MMR 致自闭症的报告因造假已被撤回，其作者也被吊销行医执照。”</li>
<li><strong>NLI判断：</strong> 给定上述证据句，模型会判定为“Contradiction”（矛盾），因为证据明确反驳了主张（证据说没有关联而主张说有因果）。标签映射后即“False”（虚假）。</li>
<li><strong>结果聚合：</strong> 如有多条证据都显示未发现关联，则一致指向 False，无冲突。模型最终输出该谣言为虚假信息。</li>
<li><strong>解释生成：</strong> 模型可以进一步生成解释，例如：“权威研究已多次证实 MMR 疫苗与自闭症之间没有因果关系，最初声称相关的论文因数据造假被撤回。因此该主张不成立。” 这段解释点出了证据的关键信息（“没有因果关系”“论文造假撤回”），有力地驳斥了谣言的科学伪装。</li>
</ul>
<h3 id="案例-2气候变化叙事">案例 2：气候变化叙事<a hidden class="anchor" aria-hidden="true" href="#案例-2气候变化叙事">#</a></h3>
<p><strong>情境：</strong> 一篇博客写道：“过去几年北极海冰面积呈增长趋势，全球变暖是场骗局。” 这是一种通过选择性时间段数据来质疑气候变暖的论调，看似引用科学观测，其实是误导性内容的一例。它并未彻底编造事实（某些年份北极海冰确有反弹），但通过断章取义忽略了长期趋势。</p>
<ul>
<li><strong>主张检测：</strong> 该句也属于可核查主张，涉及科学数据与结论，需要验证。</li>
<li><strong>证据检索：</strong> 检索可能返回 NASA 或 IPCC 关于北极海冰长期趋势的报告摘要。例如找到：“卫星数据显示，北极海冰覆盖的长期趋势依然在下降，尽管某些年份有短暂增长，但总体趋势支持全球变暖持续。” 模型还可能检索到对“全球变暖骗局”这种说法的科学辟谣文章。</li>
<li><strong>NLI判断：</strong> 证据与主张的关系有些微妙。证据并不直接说主张“错误”，而是提供了完整背景（短期波动不改长期变暖）。模型可能倾向于判定为“Contradiction”（因为主张称变暖是骗局，而证据强调变暖真实存在）。也可能部分证据被模型视为“Neutral”（信息不足），如果证据只是提供数据而未直接评价“骗局”这一论断。</li>
<li><strong>结果聚合：</strong> 综合权威证据，应该有充分理由反驳“全球变暖是骗局”。哪怕某条证据模型没判为 Contradiction，多条科学报告一起也能形成强力反证。因此最终应输出虚假。</li>
<li><strong>解释生成：</strong> 解释需要兼顾科学性和通俗性，例如：“虽然近年部分时间段北极海冰有所反弹，但总体趋势依然下降。权威气候报告明确指出全球变暖正在发生，并非骗局。因此断言‘全球变暖是骗局’属于断章取义的误导。” 这样的解释揭示了主张的技巧（选取短期增长混淆长期趋势），给出完整科学结论，帮助读者理解为什么主张是错的。</li>
</ul>
<p>通过以上案例可以看到，我们的模型流程不仅能识别完全虚假的声明，也能处理基于事实但结论错误的叙事。对于后者，模型依赖多个证据片段，从不同角度揭示主张的问题。这也是为啥多证据检索和 NLI 推理是必要的——它让 AI 有能力像专家一样对复杂的带框架偏见的说法做出判断，而非只能处理简单的是非题。</p>
<p>当然，这些案例也提醒我们模型的局限：如果主张以隐晦方式表达（如暗示性的错误因果），模型的 NLI 判定可能不自信；如果证据不足或知识库没有相关信息（长尾冷门谣言），模型只能输出“不确定”。下一节我们将深入讨论这些挑战和未来方向。</p>
<hr>
<h2 id="前沿探索与挑战">前沿探索与挑战<a hidden class="anchor" aria-hidden="true" href="#前沿探索与挑战">#</a></h2>
<p>当前的检索增强检测框架在实验中表现出较高的准确率和可解释性，但仍有诸多开放问题和改进空间。有些是技术前沿，有些是应用落地时必须克服的挑战。在本节，我们列举几个值得关注的方向：</p>
<ul>
<li><strong>检索增强生成（RAG）架构整合：</strong> RAG 是指将检索模块无缝集成到生成式模型中，使模型能一边调用外部知识库一边生成回答。将来我们可以将事实核查系统与大型语言模型更紧密结合，让模型在生成结论或解释时动态检索资料，确保输出内容的事实性和完整性。这一思路实际上已经在开放域问答等任务中验证有效，对于谣言检测的解释生成也大有裨益。RAG 架构还可以处理更长文档的多跳推理，通过逐步检索相关信息来应对复杂声明。</li>
<li><strong>解释质量评估（G-Eval）机制：</strong> 随着生成模型的应用，如何评估它们输出的解释是否可靠成为新挑战。G-Eval 框架提供了利用 LLM 进行评估的思路。未来可进一步研究多维度的评估标准，比如不仅考察解释的忠实度（是否严格基于证据），还考察可读性、说服力等。评估过程也可以更加自动化和稳健，比如结合多个不同 LLM 评审以抵消单一模型偏见。对于追求学术严谨性的传播学研究，用 AI 评估 AI 生成的解释需要特别谨慎，但这一方向有望大幅提升评估效率。</li>
<li><strong>零样本与域外泛化：</strong> 信息失序的一个难点在于其瞬息万变。新的阴谋论、俚语表达不断出现，不可能穷尽列入训练集。因此，零样本学习和跨领域泛化能力非常重要。我们已经通过 NLI 引入了一定零样本推理能力，但未来还可探索大模型 Prompting 更复杂的推理，或利用自监督预训练让模型掌握常识推理，从而在面对从未见过的谣言时也能有所判断。当然，完全的零样本是不现实的，因此持续学习（Continual Learning）也是方向，让模型随着新数据更新而进化，但要避免灾难性遗忘和谣言数据污染模型。</li>
<li><strong>多模态检测：</strong> 目前讨论的方法主要针对文本。而现实中的虚假信息常常是多模态的——假视频、伪造图片、音频片段等。Deepfake 视频的崛起就是一例，需要结合图像取证、声音识别等技术。多模态融合的检测系统亟待研究，比如当一条谣言通过图文并茂的方式传播时，如何将 NLP 模型与计算机视觉模型结合，共享判断。已有一些多模态事实核查数据集，但这一领域仍在起步，具有很大挑战性。</li>
<li><strong>对抗性攻击与鲁棒性：</strong> 生成式 AI 不仅是防守方工具，也被不法分子利用来产生更具迷惑性的谣言文本。这些对抗性文本可能专门针对检测器弱点，规避关键词或使用花哨句式，让模型难以判断。提高模型对这类攻击的鲁棒性是当务之急。可行方案包括：引入对抗训练，让模型见过被扰动的假文本；以及发展可验证的模型，如利用逻辑规则约束输出。与此同时，还要防范模型自身产生错误信息——在 RAG 架构中如检索到假信息模型可能被误导，因此知识来源可信度评估也很重要。</li>
<li><strong>知识缺口与长尾问题：</strong> 当谣言涉及非常新颖或冷门的话题，知识库中可能查不到直接证据。这时检索式方法会陷入“知识真空”。模型可能将这类主张一概标记为“信息不足”，但实际上在传播学研究中，我们有时需要预测性地判断风险。为此，可以拓展知识来源，例如接入科学文献、专家数据库，或者引入推演机制：基于常识和相关知识推理出结论。然而，让 AI 基于不完整知识推理真相存在风险，仍需深入探索。</li>
<li><strong>评估指标与误判代价：</strong> 对于检测系统来说，单纯追求总体准确率可能掩盖问题。例如，如果谣言仅占样本的 1%，模型全预测“非谣言”就有 99% 准确率，但毫无意义。因此更合适的评估是关注正类的召回率和 F1 分数。在实践中，漏掉一个真实谣言（假阴性）和错判一个真信息为谣言（假阳性）的代价是不同的，应根据应用场景调整模型阈值和优化目标。传播学者特别关心的是减少假阴性（别漏掉重要谣言）同时提供证据以便人工复核。这需要结合定量指标和定性分析全面评估模型性能。</li>
</ul>
<p><strong>优势与局限：</strong> 当前检索增强的检测模型相比纯分类器有明显优势：可解释、可扩展且领域无关（通过证据检索跨领域应用）。同时，它还能方便地融入人工流程——事实核查员可以直接查看模型检索的证据和判定辅助决策。然而局限也不可忽视：系统复杂度高、需大量外部知识支持，对知识库的新鲜度依赖强（知识库没收录的新假消息就无能为力），并且难以捕捉含蓄的误导（模型擅长显性真伪判断，但对技巧性误导如讽刺、夸张有时无从定性）。另外，模型输出的解释质量目前还不稳定，可能存在 AI 虚构（hallucination）现象，需要用人工校验或更可靠的生成约束。</p>
<p>综上，虚假信息检测是一个持续演进的攻防领域，需要跨学科的视角和多方面的创新。从传播学理论到人工智能技术，每一步发展都将有助于我们更好地识别和应对信息失序。希望本教程提供的理论-实操框架，能为相关研究提供有益参考，也期待未来在更复杂多变的环境中催生出更智能可靠的检测系统，共同守护健康的信息生态。</p>
<p>References (Selected)</p>
<p><strong>Author/Source Reference</strong>:  BM25S（BM25 Sparse）/ @汤圆键盘坏了不能写论文</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://llspublic.github.io/zh/tags/digital-humanities/">Digital Humanities</a></li>
      <li><a href="https://llspublic.github.io/zh/tags/computational-social-science/">Computational Social Science</a></li>
      <li><a href="https://llspublic.github.io/zh/tags/nlp/">NLP</a></li>
      <li><a href="https://llspublic.github.io/zh/tags/fake-news-detection/">Fake News Detection</a></li>
      <li><a href="https://llspublic.github.io/zh/tags/rag/">RAG</a></li>
      <li><a href="https://llspublic.github.io/zh/tags/python/">Python</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://llspublic.github.io/zh/">Lyuxi Liu</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

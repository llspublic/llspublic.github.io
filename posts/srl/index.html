<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Narrative Networks and SRL 101: A Paradigm Shift from &#39;Word Frequency Statistics&#39; to &#39;Structured Action Logic&#39;  | Lyuxi Liu</title>
<meta name="keywords" content="Digital Humanities, Computational Social Science, NLP, Narrative Networks, Python">
<meta name="description" content="This article provides an in-depth overview of the Narrative Network paradigm, explaining the shift from bag-of-words models to structured event triplets. It details core techniques such as Semantic Role Labeling (SRL), entity aggregation, and network backbone extraction, and includes Python code examples and empirical application analyses.">
<meta name="author" content="Lyuxi Liu">
<link rel="canonical" href="https://llspublic.github.io/posts/srl/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://llspublic.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://llspublic.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://llspublic.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://llspublic.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://llspublic.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://llspublic.github.io/posts/srl/">
<link rel="alternate" hreflang="zh" href="https://llspublic.github.io/zh/posts/srl/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://llspublic.github.io/posts/srl/">
  <meta property="og:site_name" content="Lyuxi Liu">
  <meta property="og:title" content="Narrative Networks and SRL 101: A Paradigm Shift from &#39;Word Frequency Statistics&#39; to &#39;Structured Action Logic&#39; ">
  <meta property="og:description" content="This article provides an in-depth overview of the Narrative Network paradigm, explaining the shift from bag-of-words models to structured event triplets. It details core techniques such as Semantic Role Labeling (SRL), entity aggregation, and network backbone extraction, and includes Python code examples and empirical application analyses.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-01-04T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-04T00:00:00+00:00">
    <meta property="article:tag" content="Digital Humanities">
    <meta property="article:tag" content="Computational Social Science">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Narrative Networks">
    <meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Narrative Networks and SRL 101: A Paradigm Shift from &#39;Word Frequency Statistics&#39; to &#39;Structured Action Logic&#39; ">
<meta name="twitter:description" content="This article provides an in-depth overview of the Narrative Network paradigm, explaining the shift from bag-of-words models to structured event triplets. It details core techniques such as Semantic Role Labeling (SRL), entity aggregation, and network backbone extraction, and includes Python code examples and empirical application analyses.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://llspublic.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Narrative Networks and SRL 101: A Paradigm Shift from 'Word Frequency Statistics' to 'Structured Action Logic' ",
      "item": "https://llspublic.github.io/posts/srl/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Narrative Networks and SRL 101: A Paradigm Shift from 'Word Frequency Statistics' to 'Structured Action Logic' ",
  "name": "Narrative Networks and SRL 101: A Paradigm Shift from \u0027Word Frequency Statistics\u0027 to \u0027Structured Action Logic\u0027 ",
  "description": "This article provides an in-depth overview of the Narrative Network paradigm, explaining the shift from bag-of-words models to structured event triplets. It details core techniques such as Semantic Role Labeling (SRL), entity aggregation, and network backbone extraction, and includes Python code examples and empirical application analyses.",
  "keywords": [
    "Digital Humanities", "Computational Social Science", "NLP", "Narrative Networks", "Python"
  ],
  "articleBody": " Introduction: Why Do We Need Narrative Networks?\nIn social science research, when facing massive text data, we often use Word Clouds or Topic Modeling (LDA) to extract trending topics. However, these methods only tell us “What” people are discussing, but struggle to reveal the core logic of social interaction: “Who did what to whom”. Narratives are not merely linear arrangements of high-frequency words; they are processes that construct social reality by connecting actors and recipients through events.\nTraditional Bag-of-Words models represent text as a collection of unordered vocabulary, making it impossible for us to reconstruct the exact relationship between roles and actions in a sentence. While word frequency or topic distribution can provide a corpus overview, “Keywords/Topics ≠ Complete Narrative”. For example, LDA topic models summarize text into several topics, but these topics are merely probability distributions of words, lacking an explicit description of narrative structure (such as causality and role interaction).\nNarrative Networks offer a new analytical paradigm: treating text as sequences of events rather than static sets of words, emphasizing the structured interaction between actors, actions, and recipients. In other words, narrative networks focus on “Who” (Agent) — “To Whom” (Patient) — “Did What” (Action), thereby capturing the causal context and attribution of responsibility within the text. This paradigm shift allows us to move beyond the level of topic discussion and deeply investigate how discourse shapes social cognition and power structures.\nThe following table summarizes the differences between traditional Co-word Analysis and the Narrative Network paradigm:\nAnalysis Dimension Co-word Network Narrative Network Basic Unit Word A – Word B (Co-occurrence relationship) Agent – Action – Patient (Event Triplet) Connection Basis Statistical Association (Frequency/Co-occurrence) Semantic Interaction (Role Relationship) Network Type Undirected Graph Directed Graph Interpretability Topic Relevance (Content of discussion) Action Logic (Who did what to whom) | | |\nNarrative networks significantly enhance the explanatory power regarding the construction process of social reality by preserving event structures. For example, Sudhahar et al. constructed networks using subject-verb-object relationships in news reports and found that two major camps—supporting and opposing—formed spontaneously in US election corpora. Overall, compared to traditional word co-occurrence analysis, narrative networks can better reveal the power relations and causal chains behind the text, making them a powerful tool in fields such as political communication and public opinion analysis.\nPart 1: Theoretical Perspective—From Topic Mining to Narrative Parsing\nText analysis is undergoing a paradigm shift from “identifying explicit topics” to “parsing implicit narratives”. Traditional methods focus on explicit content (such as high-frequency words, topics), while narrative parsing focuses on implicit structure (such as role interactions, event logic). There are several theoretical motivations behind this shift:\nHumans are “Storytelling Animals”: Narrative is viewed as the fundamental way humans understand and organize reality. Online narrative dynamics (such as the emergence, competition, and death of topics) can have profound effects on real-world society. Therefore, understanding the narrative structure in online public opinion helps interpret the direction and influence of public opinion.\nLimitations of the Bag-of-Words Model: Traditional text representation ignores word order and syntax, leading to an inability to distinguish between narratives with vastly different meanings, like “dog bites man” and “man bites dog”. Conversely, narrative analysis emphasizes predicates and their argument roles, precisely capturing the “who did what” relationship.\nExplaining Social Interaction: Narrative networks transform text into role-event maps, allowing researchers to examine interaction patterns from a global perspective. For instance, one can identify core actors in the public opinion field through network centrality, or discover narrative communities for specific issues through subgroup detection.\nRobert Allen et al. vividly pointed out that transforming the temporal flow of a plot into a “two-dimensional network graph” makes the structure clear at a glance. In summary, applying network methods to narrative analysis allows us to capture the symmetry and structural features of textual narratives at a macro level, and to quantitatively analyze power interactions between roles.\nPart 2: Core Technology—Semantic Role Labeling (SRL)\nSemantic Role Labeling (SRL) is the cornerstone of building narrative networks. The task of SRL is to parse the predicates (usually verbs) and their associated participant (argument) roles in a sentence, identifying “who did what to whom, when, and where”. This process can be seen as automatically annotating a script’s cast list for a sentence:\nPredicate: Usually the main verb of the sentence, representing an action or event.\nArguments: Participants and elements related to the action; each argument plays a specific Semantic Role.\nTaking the PropBank annotation system as an example, common semantic roles include:\nARG0: Agent, the initiator of the action, corresponding to the prototypical doer. For example, in the sentence “The government launched a new policy,” The government is ARG0.\nARG1: Patient, the receiver or object of the action. In the example above, a new policy is ARG1.\nARGM: Modifiers, including Time (TMP), Location (LOC), Manner (MNR), and other modifying information used to describe the context of the action.\nThrough SRL, unstructured natural language is converted into structured event descriptions. For example, the sentence “The CDC confirms the first coronavirus case”, after SRL parsing, yields the following structure:\nPredicate: confirm\nARG0: The CDC (Agent)\nARG1: the first coronavirus case (Patient)\nCombined, this forms [ARG0: The CDC] [V: confirms] [ARG1: the first coronavirus case], clearly expressing “who did what”.\nSRL Tools and Models\nEarly SRL systems like SENNA were already capable of automatically labeling semantic roles in simple sentences. In recent years, due to the emergence of pre-trained language models (such as BERT), SRL accuracy has significantly improved. For example, the AllenNLP platform provides an out-of-the-box SRL model —a BERT-based semantic role labeler. We can use this model to quickly perform semantic parsing on text without training from scratch:\nCode Example: Using AllenNLP’s pre-trained SRL model to parse sentences and extract semantic structures.\nfrom allennlp.predictors import Predictor # Load pre-trained BERT-SRL model (local path or URL) predictor = Predictor.from_path(\"srl-bert.2020.12.15.tar.gz\") # Input sentence to be parsed sentence = \"The CDC confirms the first coronavirus case.\" result = predictor.predict(sentence=sentence) # Output semantic role labeling results for verb in result['verbs']: print(verb['description']) Expected Output (Key section excerpted):\n[ARG0: The CDC] [V: confirms] [ARG1: the first coronavirus case] .\nIn the example above, the model successfully identified CDC as ARG0 (Agent), the first coronavirus case as ARG1 (Patient), and the verb confirm as the predicate V. This proves that pre-trained SRL models can automatically convert natural language into structured event triplets.\nTechnical Points: To improve SRL effectiveness, researchers have introduced various semantic resources and techniques. For example, using richer corpora (such as PropBank or FrameNet) for model training to capture verb polysemy; combining word embeddings with grammatical features to enhance the model’s ability to parse long sentences. In short, modern SRL technology is quite mature, laying a solid foundation for subsequent narrative network construction.\nPart 3: Narrative Aggregation—From Micro-Events to Macro-Networks\nAfter SRL parsing, we obtain massive micro-narrative fragments (event triplets). However, these raw triplets are often very sparse and full of noise—after all, natural language expression is ever-changing. To extract robust collective narratives from them, we need to aggregate and normalize the fragments, merging similar roles and synonymous actions into unified nodes or relationships.\nCore Steps of Aggregation Include:\nVerb Normalization: Merging verbs with similar meanings but different expressions in the corpus into a unified semantic framework. For example, mapping verbs like “support”, “endorse”, and “back” to a generic “Support” action. To achieve this, large lexical resources like VerbAtlas can be utilized. VerbAtlas manually constructs correspondences between verb synonyms and frames, summarizing 5,649 specific verbs into 466 generic predicate frames. For instance, “You backed Macron” and “You endorsed Macron”, although using different words, are both mapped by VerbAtlas (from back.01 and endorse.01) to the shared semantic framework FOLLOW_SUPPORT_SPONSOR_FUND. Through this standardization, we can compress 422,019 raw triplets in a corpus to 418,554 (using French election data as an example).\nArgument Clustering: Solving the problem of different expressions referring to the same entity. For example, “United States of America”, “The US”, and “USA” appearing in the corpus are clearly the same entity, but to a computer, they are different strings. We can use Semantic Embedding + Clustering methods to automatically merge these alias entities. In practice, Sentence-BERT (sentence-level BERT embeddings) is often used to vectorize each argument phrase. Sentence-BERT, proposed by Reimers and Gurevych, is a Siamese BERT network architecture capable of mapping sentences into a continuous vector space where semantically similar sentences are closer together. After embedding all ARG0/ARG1 arguments, Agglomerative Hierarchical Clustering or Density Clustering methods are used to group similar entities based on semantic distance. For example, we can set a threshold (e.g., cosine distance \u003c 0.4) to progressively merge argument phrases with close embedding distances, ultimately assigning a unified node ID to each cluster group.\nThe above two steps perform semantic compression on a large amount of redundant event fragments. Diverse verb expressions are compressed into limited frames, and entity aliases are merged into single nodes. After this processing, we obtain a relatively concise and normalized set of narrative events.\nCode Example: Overview of Narrative Aggregation Implementation (Verb Normalization + Argument Vector Clustering)\nfrom sentence_transformers import SentenceTransformer from sklearn.cluster import AgglomerativeClustering # Example entity list (ARG argument phrases to be clustered) entities = [\"United States\", \"USA\", \"U.S.\", \"every Asian person\", \"Asian people\", ...] # 1. Entity semantic vectorization embedder = SentenceTransformer('all-MiniLM-L6-v2') embeddings = embedder.encode(entities) # 2. Semantic clustering (Agglomerative Hierarchical Clustering) clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.4, affinity='cosine', linkage='average') labels = clustering.fit_predict(embeddings) # Output the cluster label for each entity for entity, label in zip(entities, labels): print(entity, \" -\u003e Cluster\", label) Output Example (Fictional Data):\nUnited States -\u003e Cluster 0 USA -\u003e Cluster 0 U.S. -\u003e Cluster 0 every Asian person -\u003e Cluster 5 Asian people -\u003e Cluster 5 ... As shown above, United States, USA, and U.S. are correctly grouped into the same cluster (Cluster 0), and “every Asian person” and “Asian people” are also merged into one class (Cluster 5). In subsequent network construction, we will use cluster IDs instead of raw text to represent nodes, thereby significantly reducing the number of nodes and network complexity.\nPart 4: Network Construction and Visual Analysis\nAfter aggregation and normalization of verbs and entities, we obtain a relatively “clean” list of events. Next, we need to organize these events into a Narrative Network structure for further quantitative analysis and visualization.\nDefinition of Narrative Network A Narrative Network is essentially a directed weighted graph. In our framework:\nNodes: Nodes in the network represent Actors or Actants in the narrative. Typically, these are the entity categories obtained after argument aggregation. For example, a node might represent an entity like “US Government” or “Vaccine.” Node importance can be measured by metrics such as degree centrality and closeness centrality.\nEdges: Directed connections between nodes represent a narrative relationship, usually corresponding to a predicate frame. An edge points from ARG0 to ARG1 and carries a verb label, indicating “the subject performed an action on the object.” For example, a directed edge A → B (label attack) indicates character A “attacked” character B.\nWeights: Each edge can be assigned a weight representing the importance or strength of the relationship. The simplest weight is the frequency of occurrence of the corresponding event in the corpus. More rigorous methods can use statistical metrics like Log-Odds Ratio to measure the significance of an event relative to a baseline. For example, the log-odds ratio with an informative Dirichlet prior proposed by Monroe et al. can be used to calculate the significance score of a triplet appearing in a specific sub-corpus.\nThe structure defined above enables a rich analysis space: we can study nodes with the highest in-degree (those most acted upon), or investigate the most frequent actions (potentially representing the core narrative in that context). Additionally, through network subgroup detection, we can discover camp differentiation in narratives (e.g., two groups of characters: support vs. opposition).\nBackbone Extraction and Denoising Initial networks constructed from real-world corpora are usually huge and complex, filled with many edges that appear only a few times. To highlight the main narrative threads, we need to perform denoising on the network. An effective method is to extract the Multiscale Backbone of the network.\nMultiscale Backbone Extraction (Serrano et al., 2009): This algorithm filters out edges with significant connection strength from a weighted network based on statistical significance. Specifically, for each node, assuming the weight distribution of its connected edges follows a certain random process, it calculates the probability that each edge’s weight “exceeds random expectation,” and retains edges using a significance level α as a threshold. By adjusting α, researchers can significantly reduce the number of weak edges while preserving the main structure of the network, making the network structure clearer.\nCode Example: Simplified Network Construction and Backbone Filtering Flow\nimport networkx as nx # Construct directed weighted graph G = nx.DiGraph() for (subj, verb, obj) in aggregated_triplets: # List of triplets G.add_edge(subj, obj, label=verb, weight=G[subj][obj]['weight']+1 if G.has_edge(subj, obj) else 1) # Initial network statistics print(\"Original Nodes:\", G.number_of_nodes()) print(\"Original Edges:\", G.number_of_edges()) # Backbone extraction: Retain only edges with weight above average (Simple example; use Serrano algorithm for real apps) avg_weight = sum([d['weight'] for _,_,d in G.edges(data=True)]) / G.number_of_edges() backbone_edges = [(u,v,d) for u,v,d in G.edges(data=True) if d['weight'] \u003e= avg_weight] G_core = nx.DiGraph() G_core.add_edges_from(backbone_edges) # Post-filtering network statistics print(\"Backbone Nodes:\", G_core.number_of_nodes()) print(\"Backbone Edges:\", G_core.number_of_edges()) The code above illustrates the filtering operation in a simplified way: we calculate the average weight of all edges and only retain edges with a weight greater than that average as the network backbone. In practical applications, stricter statistical thresholds or backbone extraction algorithms can be used to ensure the retained edges have a sufficiently high confidence level. After filtering, the network size is significantly reduced, containing only the most representative role interaction relationships, allowing us to focus more on the main narrative lines.\nNetwork Visualization and Interactive Analysis The constructed narrative network can be intuitively displayed through visualization to show its structural features. Traditional node-link diagrams are suitable for depicting narrative networks, where nodes represent entities and arrows represent action directions. To enhance analysis, we can introduce the following strategies:\nInteractive Graphs: Use libraries like PyVis or D3.js to generate interactive network graphs, allowing users to zoom, hover over nodes for details, etc.. Interactive views are particularly suitable for exploring large networks, where users can discover hidden patterns through filtering or dragging.\nPhysics Engine Layout: Treat nodes as charged particles, introducing attraction and repulsion forces for layout, causing tightly connected nodes to cluster automatically and sparsely related nodes to move apart. This physics-based layout often highlights community structures (clusters) in the network, such as gathering characters from the same narrative camp in close proximity in the graph.\nStyle Encoding: Node size and color can encode node degree centrality or category; edge color and thickness can represent different types of verbs or association strengths. For example, Willaert et al. categorized edges in a narrative network into two types: support (blue) and conflict (red), to distinguish between positive and negative relationships.\nThrough these means, we can ultimately obtain a clear, readable Narrative Network Map. Researchers can visually identify narrative cores (highly connected central nodes) and narrative subplots (small clusters at the periphery) within the map. For example, in a narrative network of political corpora, candidates might serve as the centers of two large red/blue clusters, while specific policy issues form independent small groups distributed around them.\nPart 5: Empirical Applications and Findings\nNarrative networks constructed based on the above methods have demonstrated powerful analytical capabilities in multiple studies. Zhao et al. applied this framework to the 2017 French Election and COVID-19 Twitter corpora, verifying its effectiveness. Here are several key findings:\nCharacter Image and Narrative Differentiation: In the French election tweet network, the narrative positions of the two main candidates (Macron and Le Pen) were distinctly different. Le Pen’s node was surrounded by a large number of actions with negative connotations (such as “attack,” “accuse”), reflecting that she was highly controversial in public opinion narratives; in contrast, Macron’s node was mainly connected to neutral actions or those related to campaign activities (such as “hold rally,” “visit voters”), with his narrative network presenting more discussions on policies and actions themselves. The contrast highlights how public opinion shapes candidate images through different verb frames.\nEvent-Driven Narrative Shifts: By observing narrative networks across different time windows, researchers found that narrative turning points often coincide highly with major real-world events. For example, following a TV debate or a scandal during the election, the frequency and structure of key triplets (such as “voter → vote → candidate”) in the network changed abruptly, corresponding to a restructuring of the collective narrative. Zhao et al. used change point detection algorithms to capture these narrative shift moments and proved that major narrative changes almost always correspond to the timeline of real events. For instance, narrative turning points detected in French election tweets corresponded exactly to key nodes like the #MacronLeaks scandal outbreak (2017/05/03) and the official inauguration day (2017/05/14).\nNetwork Structure Features: The topological structure of narrative networks reveals a Core-Periphery distribution in the public opinion field. A small number of high-centrality nodes (usually core actors like heads of state, popular issues) constitute the “core” of the network, tightly connected by multiple directed edges, forming the backbone of public opinion narratives. Conversely, other secondary roles or specific events are at the “periphery,” often attached to core nodes in star or tree structures. For example, in the COVID-19 network, nodes like “Government,” “Virus,” and “Vaccine” were at the core, surrounded by secondary node groups like hospitals, the public, and the economy, corresponding to discussions on different aspects (medical, social, economic impacts). This center-periphery map morphology reveals how public opinion unfolds around a few key roles while covering multidimensional issues.\nIn summary, empirical applications show that the narrative network method can provide both micro-level semantic interpretation (role relationships in single events) and macro-level structural insight (overall narrative landscape). This makes it a powerful tool for understanding online public opinion and social narrative dynamics, not only describing “what happened” but further explaining “how the public understands these events and weaves stories”.\nPart 6: Frontiers and Future Challenges\nAs an emerging paradigm, narrative network analysis still has vast room for expansion and challenges to be solved.\nCross-Modal Narrative Parsing Human narratives exist not only in text but also spread through images, videos, and other multimedia forms. To comprehensively capture narratives in the modern information environment, new Multimodal Narrative Extraction technologies are emerging:\nVisual Semantic Role Labeling (Visual SRL): Also known as “Situation Recognition,” the goal is for a model to generate a concise event description from an image, including the main action (verb), participants (objects, people), and the roles they play in the action. For example, given a photo of sheep shearing, the model should identify “Man shearing sheep” and label the roles: “Man (Shearer),” “Sheep (Sheared object),” “Shears (Tool),” “Pasture (Location),” etc.. Yatskar et al. (2016) established the first large-scale Visual SRL dataset, covering 500 activities and 1700 semantic roles, proving the feasibility of introducing semantic role labeling into image understanding.\nVideo Event Narrative (Video SRL): Furthermore, videos contain complete event processes. How to identify and link multiple continuous events from a video is a challenging topic. Recent research (such as Sadhu et al., 2021) proposed the VidSitu dataset and task framework : dividing a video into several time segments, annotating a verb and multiple semantic roles for each segment, and then linking causal/temporal relationships between segments. For example, in a 10-second video, the model needs to sequentially identify an event sequence like “Person 1 opens door,” “Person 1 calls pet,” “Pet runs to Person 1,” and infer continuous relationships between roles. With the development of video understanding technology, it is expected that these events extracted from multimedia will be integrated into the overall narrative network in the future, forming a Cross-Modal Narrative Map. This will allow us to analyze, for instance, the narrative consistency between news text and accompanying images/videos, discovering if there are “mismatches between text and image” or reinforcing narrative pairings. Technical Limitations and R\u0026D Challenges Although narrative network analysis has demonstrated great power, it still faces many challenges in practical applications that require continuous research:\nUnderstanding Metaphor and Irony: Online public opinion is filled with metaphors, irony, memes, and other non-literal expressions. Current SRL models mainly target explicit semantics and are often helpless against implicit meanings where “there is more than meets the eye.” For example, an ironic sentence “That’s just great (actually complaining)” has no explicit semantic cues, leading the model to struggle in judging the true inclination. This may introduce bias in the narrative network, mistaking sarcasm for support or ignoring implied narrative clues. Addressing this may require combining Sentiment Analysis and Sarcasm Detection technologies in the future to assist in interpreting implicit semantics.\nCross-Sentence Narrative Chain Extraction: Existing methods mostly extract events within the scope of a single sentence, yet many narrative threads span multiple sentences or even paragraphs. For example, news reports often introduce the event background in the first sentence and give specific action details later; similarly, causal relationships in storytelling may take multiple sentences to be fully expressed. How to enable models to connect these clues scattered across multiple sentences to build more complete Cross-Sentence Narrative Chains is a problem that needs to be solved next. One potential approach is to introduce Discourse-Level Analysis (such as coreference resolution, plot curve modeling) or teach models to summarize key events from multiple sentences.\nCoreference Resolution and Role Consistency: Narrative network construction relies heavily on the accuracy of entity recognition. If a text contains many pronouns (such as “he,” “they,” “this”), the model needs to correctly parse the specific objects they refer to in order to map events to the correct nodes. However, long-distance coreference resolution remains one of the difficult problems in NLP. Coreference errors can lead to node splitting (the same character seen as multiple nodes) or misconnection (different characters mistakenly merged) in the narrative network. Solving this problem requires more powerful coreference resolution models or introducing manual correction steps before network construction.\nLarge Models and Generative Methods: The rise of Large Pre-trained Language Models (LLMs) offers a new path for narrative parsing. Instead of extracting sentence by sentence and then aggregating, it is better to let the model read the complete document and directly Generate a structured narrative map. For example, Ash et al. (2023) tried letting GPT-3 summarize text into a list of “who did what to whom” relationships. Generative methods hold the promise of utilizing the powerful context understanding capabilities of LLMs to directly produce high-level narrative representations. However, this is currently still in the exploratory stage. We need to solve the problems of controllability and consistency of generation results, ensuring the model does not fabricate non-existent relationships, while converting the output into a standardized network format.\nConclusion\nNarrative network analysis, as an emerging method in the field of text mining, is leading us from focusing on vocabulary frequency to focusing on action logic. With the help of technologies like SRL, we are able to automatically capture “who-did what-to whom” from large-scale text, and subsequently construct a quantifiable and visualizable narrative panorama. Despite facing many challenges, with the integration of multimodal data and stronger AI models, the prospects for narrative network methods are very broad. We have reason to believe that in the near future, cross-text, image, and video narrative maps will become powerful tools for social scientists to understand public opinion and cultural narratives, helping to reveal the story threads and human behavioral logic hidden behind massive amounts of information.\nReferences (Selected)\nAuthor/Source Reference: IC2S2_OSoMe_tutorial_2024 / @汤圆键盘坏了不能写论文\nZhao, W. et al. (2024). Discovering Collective Narratives Shifts in Online Discussions. ICWSM 2024.\nSerrano, M. Á. et al. (2009). Extracting the multiscale backbone of complex weighted networks. PNAS, 106(16): 6483–6488.\nYatskar, M. et al. (2016). Situation Recognition: Visual Semantic Role Labeling for Image Understanding. CVPR 2016.\nSadhu, A. et al. (2021). Visual Semantic Role Labeling for Video Understanding. CVPR 2021.\nReimers, N., Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. EMNLP 2019.\nPalmer, M. et al. (2005). The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics.\nWillaert, T. et al. (2023). Extracting narrative signals from public discourse: a network-based approach. Humanit. Soc. Sci. Commun..\nCollobert, R. et al. (2011). Natural Language Processing (almost) from Scratch. J. Mach. Learn. Res..\nRelated Links:\n[1] [2] [5] [6] [7] [11] [12] [17] [18] [23] Extracting narrative signals from public discourse: a network-based approach | Humanities and Social Sciences Communications: https://www.nature.com/articles/s41599-025-06017-x?error=cookies_not_supported\u0026code=3b56dc64-ea30-46da-a5bf-a565930795f7\n[3] [4] [8] [9] [10] [14] [15] [16] [19] [20] [21] [25] [26] [27] [29] [30] [31] [32] [35] Discovering Collective Narratives Shifts in Online Discussions: https://www.yongyeol.com/papers/zhao2024discovering.pdf\n[13] VerbAtlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling: https://aclanthology.org/D19-1058.pdf\n[22] Sentence Embeddings using Siamese BERT-Networks: https://aclanthology.org/D19-1410/\n[24] Relatio: Text Semantics Capture Political and Economic Narratives: https://www.cambridge.org/core/journals/political-analysis/article/relatio-text-semantics-capture-political-and-economic-narratives/E72C0482A44C9A817E381B394A73E2D6\n[28] GitHub - wanyingzhao/collective_narrative_shift: https://github.com/wanyingzhao/collective_narrative_shift\n[33] CVPR 2016 Open Access Repository: https://openaccess.thecvf.com/content_cvpr_2016/html/Yatskar_Situation_Recognition_Visual_CVPR_2016_paper.html\n[34] CVPR 2021 Open Access Repository: https://openaccess.thecvf.com/content/CVPR2021/html/Sadhu_Visual_Semantic_Role_Labeling_for_Video_Understanding_CVPR_2021_paper.html\n",
  "wordCount" : "4151",
  "inLanguage": "en",
  "datePublished": "2026-01-04T00:00:00Z",
  "dateModified": "2026-01-04T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lyuxi Liu"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://llspublic.github.io/posts/srl/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lyuxi Liu",
    "logo": {
      "@type": "ImageObject",
      "url": "https://llspublic.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://llspublic.github.io/" accesskey="h" title="Lyuxi Liu (Alt + H)">Lyuxi Liu</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                    <ul class="lang-switch"><li>|</li>
                        <li>
                            <a href="https://llspublic.github.io/zh/" title="中文"
                                aria-label="中文">Zh</a>
                        </li>
                    </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://llspublic.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://llspublic.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Narrative Networks and SRL 101: A Paradigm Shift from &#39;Word Frequency Statistics&#39; to &#39;Structured Action Logic&#39; 
    </h1>
    <div class="post-meta"><span title='2026-01-04 00:00:00 +0000 UTC'>January 4, 2026</span>&nbsp;·&nbsp;<span>Lyuxi Liu</span>&nbsp;|&nbsp;<span>Translations:</span>
<ul class="i18n_list">
    <li>
        <a href="https://llspublic.github.io/zh/posts/srl/">Zh</a>
    </li>
</ul>

</div>
  </header> 

  <div class="post-content"><hr>
<p>Introduction: Why Do We Need Narrative Networks?</p>
<p>In social science research, when facing massive text data, we often use Word Clouds or Topic Modeling (LDA) to extract trending topics. However, these methods only tell us &ldquo;What&rdquo; people are discussing, but struggle to reveal the core logic of social interaction: &ldquo;<strong>Who did what to whom</strong>&rdquo;. Narratives are not merely linear arrangements of high-frequency words; they are processes that construct social reality by connecting actors and recipients through events.</p>
<p>Traditional Bag-of-Words models represent text as a collection of unordered vocabulary, making it impossible for us to reconstruct the exact relationship between roles and actions in a sentence. While word frequency or topic distribution can provide a corpus overview, <strong>&ldquo;Keywords/Topics ≠ Complete Narrative&rdquo;</strong>. For example, LDA topic models summarize text into several topics, but these topics are merely probability distributions of words, lacking an explicit description of narrative structure (such as causality and role interaction).</p>
<p><strong>Narrative Networks</strong> offer a new analytical paradigm: treating text as <strong>sequences of events</strong> rather than static sets of words, emphasizing the <strong>structured interaction</strong> between actors, actions, and recipients. In other words, narrative networks focus on <strong>&ldquo;Who&rdquo; (Agent)</strong> — <strong>&ldquo;To Whom&rdquo; (Patient)</strong> — <strong>&ldquo;Did What&rdquo; (Action)</strong>, thereby capturing the causal context and attribution of responsibility within the text. This paradigm shift allows us to move beyond the level of topic discussion and deeply investigate how discourse shapes social cognition and power structures.</p>
<p>The following table summarizes the differences between traditional Co-word Analysis and the Narrative Network paradigm:</p>
<table>
  <thead>
      <tr>
          <th>Analysis Dimension</th>
          <th>Co-word Network</th>
          <th>Narrative Network</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Basic Unit</strong></td>
          <td>Word A – Word B (Co-occurrence relationship)</td>
          <td>Agent – Action – Patient (Event Triplet)</td>
      </tr>
      <tr>
          <td><strong>Connection Basis</strong></td>
          <td>Statistical Association (Frequency/Co-occurrence)</td>
          <td>Semantic Interaction (Role Relationship)</td>
      </tr>
      <tr>
          <td><strong>Network Type</strong></td>
          <td>Undirected Graph</td>
          <td>Directed Graph</td>
      </tr>
      <tr>
          <td><strong>Interpretability</strong></td>
          <td>Topic Relevance (Content of discussion)</td>
          <td>Action Logic (Who did what to whom)</td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>|  |  |</p>
<p>Narrative networks significantly enhance the explanatory power regarding the construction process of social reality by preserving event structures. For example, Sudhahar et al. constructed networks using subject-verb-object relationships in news reports and found that two major camps—supporting and opposing—formed spontaneously in US election corpora. Overall, compared to traditional word co-occurrence analysis, narrative networks can better reveal the power relations and causal chains behind the text, making them a powerful tool in fields such as political communication and public opinion analysis.</p>
<p>Part 1: Theoretical Perspective—From Topic Mining to Narrative Parsing</p>
<p>Text analysis is undergoing a paradigm shift from &ldquo;identifying explicit topics&rdquo; to &ldquo;parsing implicit narratives&rdquo;. Traditional methods focus on <strong>explicit content</strong> (such as high-frequency words, topics), while narrative parsing focuses on <strong>implicit structure</strong> (such as role interactions, event logic). There are several theoretical motivations behind this shift:</p>
<ul>
<li></li>
</ul>
<p><strong>Humans are &ldquo;Storytelling Animals&rdquo;</strong>: Narrative is viewed as the fundamental way humans understand and organize reality. Online narrative dynamics (such as the emergence, competition, and death of topics) can have profound effects on real-world society. Therefore, understanding the narrative structure in online public opinion helps interpret the direction and influence of public opinion.</p>
<ul>
<li></li>
</ul>
<p><strong>Limitations of the Bag-of-Words Model</strong>: Traditional text representation ignores word order and syntax, leading to an inability to distinguish between narratives with vastly different meanings, like &ldquo;dog bites man&rdquo; and &ldquo;man bites dog&rdquo;. Conversely, narrative analysis emphasizes predicates and their argument roles, precisely capturing the &ldquo;who did what&rdquo; relationship.</p>
<ul>
<li></li>
</ul>
<p><strong>Explaining Social Interaction</strong>: Narrative networks transform text into role-event maps, allowing researchers to examine interaction patterns from a global perspective. For instance, one can identify core actors in the public opinion field through network centrality, or discover narrative communities for specific issues through subgroup detection.</p>
<p>Robert Allen et al. vividly pointed out that transforming the temporal flow of a plot into a &ldquo;two-dimensional network graph&rdquo; makes the structure clear at a glance. In summary, applying network methods to narrative analysis allows us to capture the <strong>symmetry</strong> and <strong>structural features</strong> of textual narratives at a macro level, and to quantitatively analyze power interactions between roles.</p>
<p>Part 2: Core Technology—Semantic Role Labeling (SRL)</p>
<p><strong>Semantic Role Labeling (SRL)</strong> is the cornerstone of building narrative networks. The task of SRL is to parse the predicates (usually verbs) and their associated participant (argument) roles in a sentence, identifying &ldquo;who did what to whom, when, and where&rdquo;. This process can be seen as automatically annotating a script&rsquo;s cast list for a sentence:</p>
<ul>
<li></li>
</ul>
<p><strong>Predicate</strong>: Usually the main verb of the sentence, representing an action or event.</p>
<ul>
<li></li>
</ul>
<p><strong>Arguments</strong>: Participants and elements related to the action; each argument plays a specific Semantic Role.</p>
<p>Taking the PropBank annotation system as an example, common semantic roles include:</p>
<ul>
<li></li>
</ul>
<p><strong>ARG0</strong>: Agent, the initiator of the action, corresponding to the prototypical doer. For example, in the sentence &ldquo;The government launched a new policy,&rdquo; <strong>The government</strong> is ARG0.</p>
<ul>
<li></li>
</ul>
<p><strong>ARG1</strong>: Patient, the receiver or object of the action. In the example above, <strong>a new policy</strong> is ARG1.</p>
<ul>
<li></li>
</ul>
<p><strong>ARGM</strong>: Modifiers, including Time (TMP), Location (LOC), Manner (MNR), and other modifying information used to describe the context of the action.</p>
<p>Through SRL, unstructured natural language is converted into structured event descriptions. For example, the sentence <em>&ldquo;The CDC confirms the first coronavirus case&rdquo;</em>, after SRL parsing, yields the following structure:</p>
<ul>
<li></li>
</ul>
<p><strong>Predicate</strong>: confirm</p>
<ul>
<li></li>
</ul>
<p><strong>ARG0</strong>: The CDC (Agent)</p>
<ul>
<li></li>
</ul>
<p><strong>ARG1</strong>: the first coronavirus case (Patient)</p>
<p>Combined, this forms <code>[ARG0: The CDC] [V: confirms] [ARG1: the first coronavirus case]</code>, clearly expressing &ldquo;who did what&rdquo;.</p>
<p><strong>SRL Tools and Models</strong></p>
<p>Early SRL systems like <strong>SENNA</strong>  were already capable of automatically labeling semantic roles in simple sentences. In recent years, due to the emergence of pre-trained language models (such as BERT), SRL accuracy has significantly improved. For example, the <strong>AllenNLP</strong> platform provides an out-of-the-box SRL model —a BERT-based semantic role labeler. We can use this model to quickly perform semantic parsing on text without training from scratch:</p>
<p><strong>Code Example</strong>: Using AllenNLP&rsquo;s pre-trained SRL model to parse sentences and extract semantic structures.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> allennlp.predictors <span style="color:#f92672">import</span> Predictor
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load pre-trained BERT-SRL model (local path or URL)</span>
</span></span><span style="display:flex;"><span>predictor <span style="color:#f92672">=</span> Predictor<span style="color:#f92672">.</span>from_path(<span style="color:#e6db74">&#34;srl-bert.2020.12.15.tar.gz&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input sentence to be parsed</span>
</span></span><span style="display:flex;"><span>sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;The CDC confirms the first coronavirus case.&#34;</span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> predictor<span style="color:#f92672">.</span>predict(sentence<span style="color:#f92672">=</span>sentence)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output semantic role labeling results</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> verb <span style="color:#f92672">in</span> result[<span style="color:#e6db74">&#39;verbs&#39;</span>]:
</span></span><span style="display:flex;"><span>    print(verb[<span style="color:#e6db74">&#39;description&#39;</span>])
</span></span></code></pre></div><p><strong>Expected Output</strong> (Key section excerpted):</p>
<p><code>[ARG0: The CDC] [V: confirms] [ARG1: the first coronavirus case] .</code></p>
<p>In the example above, the model successfully identified <strong>CDC</strong> as <strong>ARG0</strong> (Agent), <strong>the first coronavirus case</strong> as <strong>ARG1</strong> (Patient), and the verb <strong>confirm</strong> as the predicate <strong>V</strong>. This proves that pre-trained SRL models can automatically convert natural language into structured event triplets.</p>
<p><strong>Technical Points</strong>: To improve SRL effectiveness, researchers have introduced various semantic resources and techniques. For example, using richer corpora (such as PropBank or FrameNet) for model training to capture verb polysemy; combining word embeddings with grammatical features to enhance the model&rsquo;s ability to parse long sentences. In short, modern SRL technology is quite mature, laying a solid foundation for subsequent narrative network construction.</p>
<p>Part 3: Narrative Aggregation—From Micro-Events to Macro-Networks</p>
<p>After SRL parsing, we obtain massive <strong>micro-narrative fragments</strong> (event triplets). However, these raw triplets are often very sparse and full of noise—after all, natural language expression is ever-changing. To extract robust <strong>collective narratives</strong> from them, we need to aggregate and normalize the fragments, merging <strong>similar roles</strong> and <strong>synonymous actions</strong> into unified nodes or relationships.</p>
<p><strong>Core Steps of Aggregation Include</strong>:</p>
<ul>
<li></li>
</ul>
<p><strong>Verb Normalization</strong>: Merging verbs with similar meanings but different expressions in the corpus into a unified semantic framework. For example, mapping verbs like <strong>&ldquo;support&rdquo;</strong>, <strong>&ldquo;endorse&rdquo;</strong>, and <strong>&ldquo;back&rdquo;</strong> to a generic &ldquo;Support&rdquo; action. To achieve this, large lexical resources like <strong>VerbAtlas</strong> can be utilized. VerbAtlas manually constructs correspondences between verb synonyms and frames, summarizing 5,649 specific verbs into 466 generic predicate frames. For instance, <em>&ldquo;You backed Macron&rdquo;</em> and <em>&ldquo;You endorsed Macron&rdquo;</em>, although using different words, are both mapped by VerbAtlas (from <code>back.01</code> and <code>endorse.01</code>) to the shared semantic framework <code>FOLLOW_SUPPORT_SPONSOR_FUND</code>. Through this standardization, we can compress 422,019 raw triplets in a corpus to 418,554 (using French election data as an example).</p>
<ul>
<li></li>
</ul>
<p><strong>Argument Clustering</strong>: Solving the problem of different expressions referring to the same entity. For example, <strong>&ldquo;United States of America&rdquo;</strong>, <strong>&ldquo;The US&rdquo;</strong>, and <strong>&ldquo;USA&rdquo;</strong> appearing in the corpus are clearly the same entity, but to a computer, they are different strings. We can use <strong>Semantic Embedding + Clustering</strong> methods to automatically merge these alias entities. In practice, <strong>Sentence-BERT</strong> (sentence-level BERT embeddings) is often used to vectorize each argument phrase. Sentence-BERT, proposed by Reimers and Gurevych, is a Siamese BERT network architecture capable of mapping sentences into a continuous vector space where semantically similar sentences are closer together. After embedding all ARG0/ARG1 arguments, <strong>Agglomerative Hierarchical Clustering</strong> or <strong>Density Clustering</strong> methods are used to group similar entities based on semantic distance. For example, we can set a threshold (e.g., cosine distance &lt; 0.4) to progressively merge argument phrases with close embedding distances, ultimately assigning a unified node ID to each cluster group.</p>
<p>The above two steps perform <strong>semantic compression</strong> on a large amount of redundant event fragments. Diverse verb expressions are compressed into limited frames, and entity aliases are merged into single nodes. After this processing, we obtain a relatively <strong>concise and normalized set of narrative events</strong>.</p>
<p><strong>Code Example</strong>: Overview of Narrative Aggregation Implementation (Verb Normalization + Argument Vector Clustering)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sentence_transformers <span style="color:#f92672">import</span> SentenceTransformer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.cluster <span style="color:#f92672">import</span> AgglomerativeClustering
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example entity list (ARG argument phrases to be clustered)</span>
</span></span><span style="display:flex;"><span>entities <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;United States&#34;</span>, <span style="color:#e6db74">&#34;USA&#34;</span>, <span style="color:#e6db74">&#34;U.S.&#34;</span>, <span style="color:#e6db74">&#34;every Asian person&#34;</span>, <span style="color:#e6db74">&#34;Asian people&#34;</span>, <span style="color:#f92672">...</span>]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. Entity semantic vectorization</span>
</span></span><span style="display:flex;"><span>embedder <span style="color:#f92672">=</span> SentenceTransformer(<span style="color:#e6db74">&#39;all-MiniLM-L6-v2&#39;</span>)
</span></span><span style="display:flex;"><span>embeddings <span style="color:#f92672">=</span> embedder<span style="color:#f92672">.</span>encode(entities)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. Semantic clustering (Agglomerative Hierarchical Clustering)</span>
</span></span><span style="display:flex;"><span>clustering <span style="color:#f92672">=</span> AgglomerativeClustering(n_clusters<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, distance_threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>, affinity<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cosine&#39;</span>, linkage<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;average&#39;</span>)
</span></span><span style="display:flex;"><span>labels <span style="color:#f92672">=</span> clustering<span style="color:#f92672">.</span>fit_predict(embeddings)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output the cluster label for each entity</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> entity, label <span style="color:#f92672">in</span> zip(entities, labels):
</span></span><span style="display:flex;"><span>    print(entity, <span style="color:#e6db74">&#34; -&gt; Cluster&#34;</span>, label)
</span></span></code></pre></div><p><strong>Output Example</strong> (Fictional Data):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>United States  -&gt; Cluster 0
</span></span><span style="display:flex;"><span>USA            -&gt; Cluster 0
</span></span><span style="display:flex;"><span>U.S.           -&gt; Cluster 0
</span></span><span style="display:flex;"><span>every Asian person -&gt; Cluster 5
</span></span><span style="display:flex;"><span>Asian people   -&gt; Cluster 5
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>As shown above, <code>United States</code>, <code>USA</code>, and <code>U.S.</code> are correctly grouped into the <strong>same cluster</strong> (Cluster 0), and <strong>&ldquo;every Asian person&rdquo;</strong> and <strong>&ldquo;Asian people&rdquo;</strong> are also merged into one class (Cluster 5). In subsequent network construction, we will use cluster IDs instead of raw text to represent nodes, thereby significantly reducing the number of nodes and network complexity.</p>
<p>Part 4: Network Construction and Visual Analysis</p>
<p>After aggregation and normalization of verbs and entities, we obtain a relatively &ldquo;clean&rdquo; list of events. Next, we need to organize these events into a <strong>Narrative Network</strong> structure for further quantitative analysis and visualization.</p>
<ol>
<li>Definition of Narrative Network</li>
</ol>
<p>A <strong>Narrative Network</strong> is essentially a <strong>directed weighted graph</strong>. In our framework:</p>
<ul>
<li></li>
</ul>
<p><strong>Nodes</strong>: Nodes in the network represent <strong>Actors</strong> or <strong>Actants</strong> in the narrative. Typically, these are the entity categories obtained after argument aggregation. For example, a node might represent an entity like &ldquo;US Government&rdquo; or &ldquo;Vaccine.&rdquo; Node importance can be measured by metrics such as degree centrality and closeness centrality.</p>
<ul>
<li></li>
</ul>
<p><strong>Edges</strong>: Directed connections between nodes represent a <strong>narrative relationship</strong>, usually corresponding to a predicate frame. An edge points from ARG0 to ARG1 and carries a verb label, indicating &ldquo;the subject performed an action on the object.&rdquo; For example, a directed edge <strong>A → B</strong> (label <strong>attack</strong>) indicates character A &ldquo;attacked&rdquo; character B.</p>
<ul>
<li></li>
</ul>
<p><strong>Weights</strong>: Each edge can be assigned a weight representing the importance or strength of the relationship. The simplest weight is the <strong>frequency of occurrence</strong> of the corresponding event in the corpus. More rigorous methods can use statistical metrics like <strong>Log-Odds Ratio</strong> to measure the significance of an event relative to a baseline. For example, the log-odds ratio with an informative Dirichlet prior proposed by Monroe et al. can be used to calculate the significance score of a triplet appearing in a specific sub-corpus.</p>
<p>The structure defined above enables a rich analysis space: we can study nodes with the highest in-degree (those most acted upon), or investigate the most frequent actions (potentially representing the core narrative in that context). Additionally, through network <strong>subgroup detection</strong>, we can discover camp differentiation in narratives (e.g., two groups of characters: support vs. opposition).</p>
<ol start="2">
<li>Backbone Extraction and Denoising</li>
</ol>
<p>Initial networks constructed from real-world corpora are usually huge and complex, filled with many edges that appear only a few times. To highlight the main narrative threads, we need to perform <strong>denoising</strong> on the network. An effective method is to extract the <strong>Multiscale Backbone</strong> of the network.</p>
<p><strong>Multiscale Backbone Extraction</strong> (Serrano et al., 2009): This algorithm filters out edges with significant connection strength from a weighted network based on statistical significance. Specifically, for each node, assuming the weight distribution of its connected edges follows a certain random process, it calculates the probability that each edge&rsquo;s weight &ldquo;exceeds random expectation,&rdquo; and retains edges using a significance level α as a threshold. By adjusting α, researchers can significantly reduce the number of weak edges while preserving the main structure of the network, making the network structure clearer.</p>
<p><strong>Code Example</strong>: Simplified Network Construction and Backbone Filtering Flow</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> networkx <span style="color:#66d9ef">as</span> nx
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Construct directed weighted graph</span>
</span></span><span style="display:flex;"><span>G <span style="color:#f92672">=</span> nx<span style="color:#f92672">.</span>DiGraph()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (subj, verb, obj) <span style="color:#f92672">in</span> aggregated_triplets:  <span style="color:#75715e"># List of triplets</span>
</span></span><span style="display:flex;"><span>    G<span style="color:#f92672">.</span>add_edge(subj, obj, label<span style="color:#f92672">=</span>verb, weight<span style="color:#f92672">=</span>G[subj][obj][<span style="color:#e6db74">&#39;weight&#39;</span>]<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> G<span style="color:#f92672">.</span>has_edge(subj, obj) <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initial network statistics</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Original Nodes:&#34;</span>, G<span style="color:#f92672">.</span>number_of_nodes())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Original Edges:&#34;</span>, G<span style="color:#f92672">.</span>number_of_edges())
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Backbone extraction: Retain only edges with weight above average (Simple example; use Serrano algorithm for real apps)</span>
</span></span><span style="display:flex;"><span>avg_weight <span style="color:#f92672">=</span> sum([d[<span style="color:#e6db74">&#39;weight&#39;</span>] <span style="color:#66d9ef">for</span> _,_,d <span style="color:#f92672">in</span> G<span style="color:#f92672">.</span>edges(data<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)]) <span style="color:#f92672">/</span> G<span style="color:#f92672">.</span>number_of_edges()
</span></span><span style="display:flex;"><span>backbone_edges <span style="color:#f92672">=</span> [(u,v,d) <span style="color:#66d9ef">for</span> u,v,d <span style="color:#f92672">in</span> G<span style="color:#f92672">.</span>edges(data<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#66d9ef">if</span> d[<span style="color:#e6db74">&#39;weight&#39;</span>] <span style="color:#f92672">&gt;=</span> avg_weight]
</span></span><span style="display:flex;"><span>G_core <span style="color:#f92672">=</span> nx<span style="color:#f92672">.</span>DiGraph()
</span></span><span style="display:flex;"><span>G_core<span style="color:#f92672">.</span>add_edges_from(backbone_edges)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Post-filtering network statistics</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Backbone Nodes:&#34;</span>, G_core<span style="color:#f92672">.</span>number_of_nodes())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Backbone Edges:&#34;</span>, G_core<span style="color:#f92672">.</span>number_of_edges())
</span></span></code></pre></div><p>The code above illustrates the filtering operation in a simplified way: we calculate the average weight of all edges and only retain edges with a weight greater than that average as the network backbone. In practical applications, stricter statistical thresholds or backbone extraction algorithms  can be used to ensure the retained edges have a sufficiently high confidence level. After filtering, the network size is significantly reduced, containing only the most representative role interaction relationships, allowing us to focus more on the main narrative lines.</p>
<ol start="3">
<li>Network Visualization and Interactive Analysis</li>
</ol>
<p>The constructed narrative network can be intuitively displayed through <strong>visualization</strong> to show its structural features. Traditional node-link diagrams are suitable for depicting narrative networks, where nodes represent entities and arrows represent action directions. To enhance analysis, we can introduce the following strategies:</p>
<ul>
<li></li>
</ul>
<p><strong>Interactive Graphs</strong>: Use libraries like PyVis or D3.js to generate interactive network graphs, allowing users to zoom, hover over nodes for details, etc.. Interactive views are particularly suitable for exploring large networks, where users can discover hidden patterns through filtering or dragging.</p>
<ul>
<li></li>
</ul>
<p><strong>Physics Engine Layout</strong>: Treat nodes as charged particles, introducing attraction and repulsion forces for layout, causing tightly connected nodes to cluster automatically and sparsely related nodes to move apart. This physics-based layout often highlights <strong>community structures</strong> (clusters) in the network, such as gathering characters from the same narrative camp in close proximity in the graph.</p>
<ul>
<li></li>
</ul>
<p><strong>Style Encoding</strong>: Node size and color can encode node degree centrality or category; edge color and thickness can represent different types of verbs or association strengths. For example, Willaert et al. categorized edges in a narrative network into two types: support (blue) and conflict (red), to distinguish between positive and negative relationships.</p>
<p>Through these means, we can ultimately obtain a clear, readable <strong>Narrative Network Map</strong>. Researchers can visually identify <strong>narrative cores</strong> (highly connected central nodes) and <strong>narrative subplots</strong> (small clusters at the periphery) within the map. For example, in a narrative network of political corpora, candidates might serve as the centers of two large red/blue clusters, while specific policy issues form independent small groups distributed around them.</p>
<p>Part 5: Empirical Applications and Findings</p>
<p>Narrative networks constructed based on the above methods have demonstrated powerful analytical capabilities in multiple studies. Zhao et al. applied this framework to the <strong>2017 French Election</strong> and <strong>COVID-19 Twitter corpora</strong>, verifying its effectiveness. Here are several key findings:</p>
<ul>
<li></li>
</ul>
<p><strong>Character Image and Narrative Differentiation</strong>: In the French election tweet network, the narrative positions of the two main candidates (Macron and Le Pen) were distinctly different. Le Pen&rsquo;s node was surrounded by a large number of actions with negative connotations (such as &ldquo;attack,&rdquo; &ldquo;accuse&rdquo;), reflecting that she was highly controversial in public opinion narratives; in contrast, Macron&rsquo;s node was mainly connected to neutral actions or those related to campaign activities (such as &ldquo;hold rally,&rdquo; &ldquo;visit voters&rdquo;), with his narrative network presenting more discussions on policies and actions themselves. The contrast highlights how public opinion shapes candidate images through different verb frames.</p>
<ul>
<li></li>
</ul>
<p><strong>Event-Driven Narrative Shifts</strong>: By observing narrative networks across different time windows, researchers found that <strong>narrative turning points</strong> often coincide highly with major real-world events. For example, following a TV debate or a scandal during the election, the frequency and structure of key triplets (such as &ldquo;voter → vote → candidate&rdquo;) in the network changed abruptly, corresponding to a restructuring of the collective narrative. Zhao et al. used change point detection algorithms to capture these narrative shift moments and proved that major narrative changes almost always correspond to the timeline of real events. For instance, narrative turning points detected in French election tweets corresponded exactly to key nodes like the <code>#MacronLeaks</code> scandal outbreak (2017/05/03) and the official inauguration day (2017/05/14).</p>
<ul>
<li></li>
</ul>
<p><strong>Network Structure Features</strong>: The topological structure of narrative networks reveals a <strong>Core-Periphery</strong> distribution in the public opinion field. A small number of high-centrality nodes (usually core actors like heads of state, popular issues) constitute the &ldquo;core&rdquo; of the network, tightly connected by multiple directed edges, forming the backbone of public opinion narratives. Conversely, other secondary roles or specific events are at the &ldquo;periphery,&rdquo; often attached to core nodes in star or tree structures. For example, in the COVID-19 network, nodes like &ldquo;Government,&rdquo; &ldquo;Virus,&rdquo; and &ldquo;Vaccine&rdquo; were at the core, surrounded by secondary node groups like hospitals, the public, and the economy, corresponding to discussions on different aspects (medical, social, economic impacts). This center-periphery map morphology reveals how public opinion unfolds around a few key roles while covering multidimensional issues.</p>
<p>In summary, empirical applications show that the narrative network method can provide both <strong>micro-level semantic interpretation</strong> (role relationships in single events) and <strong>macro-level structural insight</strong> (overall narrative landscape). This makes it a powerful tool for understanding online public opinion and social narrative dynamics, not only describing &ldquo;what happened&rdquo; but further explaining &ldquo;how the public understands these events and weaves stories&rdquo;.</p>
<p>Part 6: Frontiers and Future Challenges</p>
<p>As an emerging paradigm, narrative network analysis still has vast room for expansion and challenges to be solved.</p>
<ol>
<li>Cross-Modal Narrative Parsing</li>
</ol>
<p>Human narratives exist not only in text but also spread through images, videos, and other multimedia forms. To comprehensively capture narratives in the modern information environment, new <strong>Multimodal Narrative Extraction</strong> technologies are emerging:</p>
<ul>
<li></li>
</ul>
<p><strong>Visual Semantic Role Labeling (Visual SRL)</strong>: Also known as &ldquo;Situation Recognition,&rdquo; the goal is for a model to generate a concise event description from an image, including the main action (verb), participants (objects, people), and the roles they play in the action. For example, given a photo of sheep shearing, the model should identify &ldquo;Man shearing sheep&rdquo; and label the roles: &ldquo;Man (Shearer),&rdquo; &ldquo;Sheep (Sheared object),&rdquo; &ldquo;Shears (Tool),&rdquo; &ldquo;Pasture (Location),&rdquo; etc.. Yatskar et al. (2016) established the first large-scale Visual SRL dataset, covering 500 activities and 1700 semantic roles, proving the feasibility of introducing semantic role labeling into image understanding.</p>
<ul>
<li><strong>Video Event Narrative (Video SRL)</strong>: Furthermore, videos contain complete event processes. How to identify and link multiple continuous events from a video is a challenging topic. Recent research (such as Sadhu et al., 2021) proposed the VidSitu dataset and task framework : dividing a video into several time segments, annotating a verb and multiple semantic roles for each segment, and then linking causal/temporal relationships between segments. For example, in a 10-second video, the model needs to sequentially identify an event sequence like &ldquo;Person 1 opens door,&rdquo; &ldquo;Person 1 calls pet,&rdquo; &ldquo;Pet runs to Person 1,&rdquo; and infer continuous relationships between roles. With the development of video understanding technology, it is expected that these events extracted from multimedia will be integrated into the overall narrative network in the future, forming a <strong>Cross-Modal Narrative Map</strong>. This will allow us to analyze, for instance, the narrative consistency between news text and accompanying images/videos, discovering if there are &ldquo;mismatches between text and image&rdquo; or reinforcing narrative pairings.</li>
</ul>
<ol start="2">
<li>Technical Limitations and R&amp;D Challenges</li>
</ol>
<p>Although narrative network analysis has demonstrated great power, it still faces many challenges in practical applications that require continuous research:</p>
<ul>
<li></li>
</ul>
<p><strong>Understanding Metaphor and Irony</strong>: Online public opinion is filled with metaphors, irony, memes, and other non-literal expressions. Current SRL models mainly target explicit semantics and are often helpless against implicit meanings where &ldquo;there is more than meets the eye.&rdquo; For example, an ironic sentence &ldquo;That&rsquo;s just great (actually complaining)&rdquo; has no explicit semantic cues, leading the model to struggle in judging the true inclination. This may introduce bias in the narrative network, mistaking sarcasm for support or ignoring implied narrative clues. Addressing this may require combining <strong>Sentiment Analysis</strong> and <strong>Sarcasm Detection</strong> technologies in the future to assist in interpreting implicit semantics.</p>
<ul>
<li></li>
</ul>
<p><strong>Cross-Sentence Narrative Chain Extraction</strong>: Existing methods mostly extract events within the scope of a single sentence, yet many narrative threads span multiple sentences or even paragraphs. For example, news reports often introduce the event background in the first sentence and give specific action details later; similarly, causal relationships in storytelling may take multiple sentences to be fully expressed. How to enable models to connect these clues scattered across multiple sentences to build more complete <strong>Cross-Sentence Narrative Chains</strong> is a problem that needs to be solved next. One potential approach is to introduce <strong>Discourse-Level Analysis</strong> (such as coreference resolution, plot curve modeling) or teach models to summarize key events from multiple sentences.</p>
<ul>
<li></li>
</ul>
<p><strong>Coreference Resolution and Role Consistency</strong>: Narrative network construction relies heavily on the accuracy of entity recognition. If a text contains many pronouns (such as &ldquo;he,&rdquo; &ldquo;they,&rdquo; &ldquo;this&rdquo;), the model needs to correctly parse the specific objects they refer to in order to map events to the correct nodes. However, long-distance coreference resolution remains one of the difficult problems in NLP. Coreference errors can lead to node splitting (the same character seen as multiple nodes) or misconnection (different characters mistakenly merged) in the narrative network. Solving this problem requires more powerful coreference resolution models or introducing manual correction steps before network construction.</p>
<ul>
<li></li>
</ul>
<p><strong>Large Models and Generative Methods</strong>: The rise of Large Pre-trained Language Models (LLMs) offers a new path for narrative parsing. Instead of extracting sentence by sentence and then aggregating, it is better to let the model read the complete document and directly <strong>Generate</strong> a structured narrative map. For example, Ash et al. (2023) tried letting GPT-3 summarize text into a list of &ldquo;who did what to whom&rdquo; relationships. Generative methods hold the promise of utilizing the powerful context understanding capabilities of LLMs to directly produce high-level narrative representations. However, this is currently still in the exploratory stage. We need to solve the problems of controllability and consistency of generation results, ensuring the model does not fabricate non-existent relationships, while converting the output into a standardized network format.</p>
<p>Conclusion</p>
<p>Narrative network analysis, as an emerging method in the field of text mining, is leading us from focusing on vocabulary frequency to focusing on action logic. With the help of technologies like SRL, we are able to automatically capture &ldquo;who-did what-to whom&rdquo; from large-scale text, and subsequently construct a quantifiable and visualizable narrative panorama. Despite facing many challenges, with the integration of multimodal data and stronger AI models, the prospects for narrative network methods are very broad. We have reason to believe that in the near future, cross-text, image, and video narrative maps will become powerful tools for social scientists to understand public opinion and cultural narratives, helping to reveal the story threads and human behavioral logic hidden behind massive amounts of information.</p>
<p>References (Selected)</p>
<p><strong>Author/Source Reference</strong>: IC2S2_OSoMe_tutorial_2024 / @汤圆键盘坏了不能写论文</p>
<ul>
<li>
<p>Zhao, W. et al. (2024). <strong>Discovering Collective Narratives Shifts in Online Discussions</strong>. ICWSM 2024.</p>
</li>
<li>
<p>Serrano, M. Á. et al. (2009). <strong>Extracting the multiscale backbone of complex weighted networks</strong>. PNAS, 106(16): 6483–6488.</p>
</li>
<li>
<p>Yatskar, M. et al. (2016). <strong>Situation Recognition: Visual Semantic Role Labeling for Image Understanding</strong>. CVPR 2016.</p>
</li>
<li>
<p>Sadhu, A. et al. (2021). <strong>Visual Semantic Role Labeling for Video Understanding</strong>. CVPR 2021.</p>
</li>
<li>
<p>Reimers, N., Gurevych, I. (2019). <strong>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</strong>. EMNLP 2019.</p>
</li>
<li>
<p>Palmer, M. et al. (2005). <strong>The Proposition Bank: An Annotated Corpus of Semantic Roles</strong>. Computational Linguistics.</p>
</li>
<li>
<p>Willaert, T. et al. (2023). <strong>Extracting narrative signals from public discourse: a network-based approach</strong>. Humanit. Soc. Sci. Commun..</p>
</li>
<li>
<p>Collobert, R. et al. (2011). <strong>Natural Language Processing (almost) from Scratch</strong>. J. Mach. Learn. Res..</p>
</li>
</ul>
<p><strong>Related Links:</strong></p>
<ul>
<li>
<p>[1] [2] [5] [6] [7] [11] [12] [17] [18] [23] Extracting narrative signals from public discourse: a network-based approach | Humanities and Social Sciences Communications: <a href="https://www.nature.com/articles/s41599-025-06017-x?error=cookies_not_supported&amp;code=3b56dc64-ea30-46da-a5bf-a565930795f7">https://www.nature.com/articles/s41599-025-06017-x?error=cookies_not_supported&amp;code=3b56dc64-ea30-46da-a5bf-a565930795f7</a></p>
</li>
<li>
<p>[3] [4] [8] [9] [10] [14] [15] [16] [19] [20] [21] [25] [26] [27] [29] [30] [31] [32] [35] Discovering Collective Narratives Shifts in Online Discussions: <a href="https://www.yongyeol.com/papers/zhao2024discovering.pdf">https://www.yongyeol.com/papers/zhao2024discovering.pdf</a></p>
</li>
<li>
<p>[13] VerbAtlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling: <a href="https://aclanthology.org/D19-1058.pdf">https://aclanthology.org/D19-1058.pdf</a></p>
</li>
<li>
<p>[22] Sentence Embeddings using Siamese BERT-Networks: <a href="https://aclanthology.org/D19-1410/">https://aclanthology.org/D19-1410/</a></p>
</li>
<li>
<p>[24] Relatio: Text Semantics Capture Political and Economic Narratives: <a href="https://www.cambridge.org/core/journals/political-analysis/article/relatio-text-semantics-capture-political-and-economic-narratives/E72C0482A44C9A817E381B394A73E2D6">https://www.cambridge.org/core/journals/political-analysis/article/relatio-text-semantics-capture-political-and-economic-narratives/E72C0482A44C9A817E381B394A73E2D6</a></p>
</li>
<li>
<p>[28] GitHub - wanyingzhao/collective_narrative_shift: <a href="https://github.com/wanyingzhao/collective_narrative_shift">https://github.com/wanyingzhao/collective_narrative_shift</a></p>
</li>
<li>
<p>[33] CVPR 2016 Open Access Repository: <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/Yatskar_Situation_Recognition_Visual_CVPR_2016_paper.html">https://openaccess.thecvf.com/content_cvpr_2016/html/Yatskar_Situation_Recognition_Visual_CVPR_2016_paper.html</a></p>
</li>
<li>
<p>[34] CVPR 2021 Open Access Repository: <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Sadhu_Visual_Semantic_Role_Labeling_for_Video_Understanding_CVPR_2021_paper.html">https://openaccess.thecvf.com/content/CVPR2021/html/Sadhu_Visual_Semantic_Role_Labeling_for_Video_Understanding_CVPR_2021_paper.html</a></p>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://llspublic.github.io/tags/digital-humanities/">Digital Humanities</a></li>
      <li><a href="https://llspublic.github.io/tags/computational-social-science/">Computational Social Science</a></li>
      <li><a href="https://llspublic.github.io/tags/nlp/">NLP</a></li>
      <li><a href="https://llspublic.github.io/tags/narrative-networks/">Narrative Networks</a></li>
      <li><a href="https://llspublic.github.io/tags/python/">Python</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://llspublic.github.io/">Lyuxi Liu</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial | Lyuxi Liu</title>
<meta name="keywords" content="Digital Humanities, Computational Social Science, NLP, Fake News Detection, RAG, Python">
<meta name="description" content="This article provides a systematic introduction to the theoretical mechanisms of Information Disorder for communication researchers, detailing the evolution from end-to-end classification to the &lsquo;Retrieve-then-Verify&rsquo; paradigm. It covers core technologies including BM25S sparse retrieval, semantic vector retrieval, NLI inference, and G-Eval explanation generation, accompanied by complete Python practical code and case studies.">
<meta name="author" content="Lyuxi Liu">
<link rel="canonical" href="https://llspublic.github.io/posts/rag/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://llspublic.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://llspublic.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://llspublic.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://llspublic.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://llspublic.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://llspublic.github.io/posts/rag/">
<link rel="alternate" hreflang="zh" href="https://llspublic.github.io/zh/posts/rag/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://llspublic.github.io/posts/rag/">
  <meta property="og:site_name" content="Lyuxi Liu">
  <meta property="og:title" content="Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial">
  <meta property="og:description" content="This article provides a systematic introduction to the theoretical mechanisms of Information Disorder for communication researchers, detailing the evolution from end-to-end classification to the ‚ÄòRetrieve-then-Verify‚Äô paradigm. It covers core technologies including BM25S sparse retrieval, semantic vector retrieval, NLI inference, and G-Eval explanation generation, accompanied by complete Python practical code and case studies.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-01-04T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-04T00:00:00+00:00">
    <meta property="article:tag" content="Digital Humanities">
    <meta property="article:tag" content="Computational Social Science">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Fake News Detection">
    <meta property="article:tag" content="RAG">
    <meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial">
<meta name="twitter:description" content="This article provides a systematic introduction to the theoretical mechanisms of Information Disorder for communication researchers, detailing the evolution from end-to-end classification to the &lsquo;Retrieve-then-Verify&rsquo; paradigm. It covers core technologies including BM25S sparse retrieval, semantic vector retrieval, NLI inference, and G-Eval explanation generation, accompanied by complete Python practical code and case studies.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://llspublic.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial",
      "item": "https://llspublic.github.io/posts/rag/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial",
  "name": "Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial",
  "description": "This article provides a systematic introduction to the theoretical mechanisms of Information Disorder for communication researchers, detailing the evolution from end-to-end classification to the \u0026lsquo;Retrieve-then-Verify\u0026rsquo; paradigm. It covers core technologies including BM25S sparse retrieval, semantic vector retrieval, NLI inference, and G-Eval explanation generation, accompanied by complete Python practical code and case studies.",
  "keywords": [
    "Digital Humanities", "Computational Social Science", "NLP", "Fake News Detection", "RAG", "Python"
  ],
  "articleBody": "Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial Introduction As generative artificial intelligence reshapes the information ecosystem, information dissemination in the ‚Äúpost-truth‚Äù era is becoming unprecedentedly complex. From deepfake videos that blur the line between reality and fiction to conspiracy theories proliferating in public health, Information Disorder has evolved into a complex socio-technical phenomenon, rather than a simple binary issue of ‚Äútrue‚Äù or ‚Äúfalse.‚Äù The traditional term ‚Äúfake news‚Äù has lost its precise meaning due to excessive politicization and struggles to encapsulate the diverse types of inaccurate information present today. To effectively research and govern information disorder, we need to establish a rigorous conceptual framework and technical system that covers both theoretical classification/cognitive mechanisms and practical automated detection workflows.\nThis article is aimed at researchers in the field of communication. It systematically introduces the theoretical background and psychological mechanisms of information disorder, elaborates on the evolution of detection paradigms, details key technical modules (including sparse retrieval, semantic retrieval, Natural Language Inference, and result aggregation), and demonstrates how to build an automated fact-checking system through practical code. Subsequently, we will use case studies (such as vaccine rumors and climate change narratives) to illustrate how models deal with false narratives cloaked in ‚Äúscience.‚Äù Finally, we discuss frontier explorations and challenges in current methods, including the integration of Retrieval-Augmented Generation (RAG) architectures, explanation evaluation mechanisms using LLMs (G-Eval), zero-shot and multimodal detection difficulties, as well as the advantages and limitations of these models. The entire article aims to be structurally rigorous and concise, helping communication researchers build a complete understanding from theory to practice.\nTheoretical Background: Concepts and Psychological Mechanisms of Information Disorder 1. Concepts and Classification of Information Disorder To study disinformation, one must first clarify conceptual boundaries. The Information Disorder framework, proposed by Wardle and Derakhshan in 2017, strictly defines types of inaccurate information using two dimensions: truthfulness and intent. This framework categorizes online rumors into three types:\nMisinformation: Content that is objectively false, but the sharer has no subjective malicious intent. For example, spreading wrong information due to a lack of knowledge or statistical oversight. Disinformation: Content that is false and intentionally created or spread to mislead or harm the public, carrying clear malicious motives, including fabricated political conspiracy theories or forged propaganda. Malinformation: Content that may be true in itself but is taken out of context or maliciously leaked to cause harm. For example, leaking private truths to smear others. The above classification provides a rigorous ontological basis for researchers and a standard for data annotation. On this basis, Wardle further subdivided seven common forms of information disorder, forming a spectrum from mild misleading to complete fabrication:\nSatire/Parody: No intention to cause harm but has the potential to fool. False Connection: Headlines, visuals, or captions do not support the content (clickbait). Misleading Content: Misleading use of information to frame an issue or individual (cherry-picking). False Context: Genuine content is shared with false contextual information. Imposter Content: Genuine sources are impersonated. Manipulated Content: Genuine information or imagery is manipulated to deceive (e.g., photoshopping). Fabricated Content: New content is 100% false, designed to deceive and do harm. These classifications emphasize that the characteristics of different types of false information vary widely. For instance, satire often contains humorous elements, while fabricated content may be carefully disguised as news reports. If researchers mix them indiscriminately (collectively calling them ‚Äúfake news‚Äù), model training will only learn noise. Therefore, detection strategies and feature engineering should be customized for specific categories to improve model precision.\n2. Psychological Mechanisms of Information Disorder Communication studies are concerned not only with how information spreads but also why individuals believe in false information. Research shows that people do not believe rumors solely due to ignorance; highly educated populations can also be deceived. This is due to a series of biases and heuristics in human cognition. Integrating these psychological mechanisms into models can help predict who is more susceptible to false information.\nSeveral core psychological factors include:\nIdentity Protective Cognition (Motivated Reasoning): Also known as group identity bias. People tend to selectively accept information based on their partisan or group identity, automatically filtering for content that aligns with their stance. This motivated cognitive processing causes people to stick to their original beliefs even in the face of contradictory evidence to maintain group identity. In short, political stance ‚Äúfilters‚Äù facts; highly partisan individuals are more likely to believe misinformation that fits their ideology. Cognitive Laziness: The tendency to lack cognitive reflection or deep thinking. Psychology divides human thinking into ‚ÄúSystem 1‚Äù (intuition-dependent) and ‚ÄúSystem 2‚Äù (rational analysis). Some people rely more on intuition and are unwilling to expend mental effort, thus lacking skepticism toward rumors. Research finds that people with low analytical thinking skills are more likely to believe fake news, and their susceptibility is closely related to being ‚Äúlazy thinkers.‚Äù This means the spread of fake news is often not due to excessive partisan bias, but a lack of habit for rational thought. Illusory Truth Effect: Simply hearing a statement repeated multiple times makes people more inclined to believe it is true, even if it is objectively false. Cognitive psychology calls this the illusory truth effect. Repetition makes information familiar, and familiarity is misused by the brain as a cue for truthfulness, thereby increasing the credibility of false information. Even if the audience originally knew the truth, repeated exposure can weaken their judgment. Emotional Arousal: Information with strong emotions (such as fear, anger, moral outrage) is more likely to attract attention and spread widely. High-arousal information can bypass rational analysis and directly affect judgment. When people are emotionally charged, their ability to distinguish between true and false information declines, making them more likely to rashly forward or believe statements that match their emotions. In addition to the above psychological mechanisms, communication data also reveals some counter-intuitive phenomena. For example, we easily overestimate the impact of disinformation on the entire population, as if everyone is being ‚Äúbrainwashed,‚Äù yet underestimate human agency. Empirical research shows: most ordinary users are exposed to very few rumors; the large-scale spread of disinformation is often concentrated in a very small number of active fringe groups. The proliferation of disinformation is more like a ‚Äúcarnival of a small group‚Äù rather than ‚Äúeveryone getting infected.‚Äù Therefore, governance should shift focus from scanning the entire web to targeted interventions for high-risk susceptible populations. This reminds us not to fall into the trap of ‚Äúalgorithmic determinism‚Äù‚Äîsocial media algorithms are not the sole culprit for rumors; human needs and choices are equally important.\nFinally, special attention must be paid to misleading narratives cloaked in science. Disinformation on scientific issues is usually more subtle; it does not directly lie about facts but distorts cognition through framing. For example, on the issue of climate change, different wording (‚Äúglobal warming‚Äù vs. ‚Äúclimate change‚Äù) affects the acceptance of different groups. Research found that US conservatives have less resistance to the term ‚Äúclimate change‚Äù compared to ‚Äúglobal warming.‚Äù This semantic difference essentially reflects a difference in stance. Similarly, vaccine rumors often cite ‚Äúscientific-sounding‚Äù rhetoric (e.g., citing the retracted Wakefield pseudo-study) to gain trust. These contents do not fabricate new facts but use existing scientific terminology or data to mislead, making traditional truth verification more difficult. This poses higher requirements for automated detection systems: they must not only judge truth or falsehood but also capture whether the stance and frame are biased.\nIn summary, information disorder is not a simple rumor problem but a complex phenomenon containing multi-dimensional classifications and deep psychological mechanisms. Understanding these theoretical backgrounds helps design more targeted detection paradigms and models.\nEvolution of Detection Paradigms: From Classifiers to ‚ÄúRetrieve-then-Verify‚Äù Faced with emerging false information, early research often attempted to train end-to-end text classification models (e.g., judging whether a piece of news is fake). While these traditional binary classification methods are intuitive, they have significant flaws: they make truth judgments based solely on the input text itself, lacking an explanation for the basis of the judgment. In practice, this leads to a crisis of trust‚Äîthe model gives a ‚ÄúTrue‚Äù or ‚ÄúFalse‚Äù label but cannot explain ‚Äúwhy,‚Äù leaving users unconvinced.\nTo improve the credibility and effectiveness of detection, the ‚ÄúRetrieve-then-Verify‚Äù paradigm has emerged in recent years. This paradigm simulates the workflow of human fact-checkers: first retrieving external evidence for the claim to be verified, and then judging the truth of the claim based on the evidence. This process has proven to be more interpretable and robust than simple text classification:\nEvidence Retrieval: For each claim that needs verification, retrieve relevant documents or sentences from a large trusted knowledge base as evidence sources. For example, for a political statement, retrieve snippets from Wikipedia or authoritative news reports mentioning the same event or data. Natural Language Inference (NLI): Compare the retrieved evidence with the original claim, and use an NLI model to judge the relationship of the evidence to the claim‚Äîwhether it Supports (entails), Refutes (contradicts), or is Neutral (unrelated). This process is equivalent to logical reasoning, determining whether the claim stands up in the face of evidence. Result Aggregation: Synthesize the inference results of multiple pieces of evidence to give a final verdict. For instance, if most evidence supports the claim without contradiction, it is judged as True; if reliable evidence refutes the claim, it is judged as False; if evidence is insufficient, it is marked as Not Enough Info. This aggregation strategy ensures model decisions are based on a sufficient set of evidence rather than an isolated single source. The advantage of the ‚ÄúRetrieve-then-Verify‚Äù paradigm lies in its process interpretability. The model not only outputs a conclusion but can also provide the evidence it referenced (e.g., citing a sentence from a Wikipedia article), making the basis of the judgment clear to the user. For example, the model can say: ‚ÄúThis health rumor is false because authoritative medical study X indicates the opposite.‚Äù Such detection systems are more likely to gain the trust of users and fact-checkers. Additionally, this paradigm handles open-domain unknown claims better: even if the claim itself is not in the training data, the model can make a judgment by retrieving new information, possessing certain zero-shot capabilities.\nIt is important to emphasize that under the ‚ÄúRetrieve-then-Verify‚Äù framework, we no longer encourage training an opaque end-to-end fake news classifier. Instead, we build a modular pipeline: each step (retrieval, reasoning, aggregation) can be optimized independently and produce intermediate interpretable results (such as retrieval lists, evidence annotations), thereby improving the transparency and interpretability of the system as a whole. This is particularly important for communication research‚Äîwhen we use AI models to analyze rumor propagation in public opinion, we need to be clear about which factual basis the model used to reach its conclusion, in order to align with journalistic professionalism and social science theories.\nIn summary, the detection paradigm is evolving from ‚Äúblack-box‚Äù text classification to a human-like ‚ÄúEvidence Retrieval ‚Üí Logical Verification‚Äù flow. The next section will delve into the key technical modules in this process.\nDetailed Explanation of Key Technical Modules To implement a retrieval-augmented detection system, multiple NLP technical modules need to be organically combined. Below we detail the core components: Sparse Retrieval, Semantic Retrieval, Natural Language Inference (NLI), and Result Aggregation/Label Mapping strategies.\n1. Sparse Retrieval: BM25S Algorithm Sparse Retrieval refers to traditional keyword-matching retrieval methods, represented by BM25. BM25 is a probabilistic retrieval model that improves upon TF-IDF by adding saturation handling for term frequency and length normalization. Simply put, BM25 calculates a relevance score based on shared vocabulary between the query and the document, the frequency of terms in the document, and the inverse document frequency across the entire corpus. It excels at exact matching of query terms and is often very effective for entity words like names and locations.\nIn the system introduced in this article, we use an efficient implementation variant of BM25 called BM25S (BM25 Sparse). BM25S is available as a Python library (e.g., bm25s) and features the following:\nExact Matching: Excellent performance for queries requiring exact matches of proper nouns, numbers, etc. If a specific name or place appears in the claim, BM25 can quickly lock onto documents containing these entities. In contrast, some semantic vector models might miss these exact matches due to over-generalization. Efficient Indexing: BM25S uses an inverted index structure, offering extremely fast retrieval speeds for large corpora, achieving millisecond-level query responses even with millions of documents. Length Normalization: The BM25 formula introduces document length penalties to prevent long documents from getting unfairly high scores simply because they contain more words. This improves the fairness of relevance scoring. Easy Implementation: Using BM25S requires tokenizing the corpus and building an index. Once the index object is built, efficient retrieval can be performed for any query. As the ‚Äúfirst line of defense‚Äù for RAG systems, the sparse retrieval module is relatively simple to implement and stable. Code Example: Assuming we have an evidence sentence library corpus (where each element is an evidence sentence string), we can use BM25S for indexing and retrieval:\nimport bm25s # Build BM25 index retriever = bm25s.BM25(corpus=corpus) retriever.index(bm25s.tokenize(corpus)) # Execute retrieval for the query claim claim = \"The number of new cases of shingles per year extends from 1.2-3.4 per 1,000.\" results, scores = retriever.retrieve(bm25s.tokenize(claim), k=10) print(results[0:3]) # Print the top 3 most relevant evidence sentences The above code will return a list of evidence texts highly overlapping with the content of the claim. BM25 will prioritize returning sentences containing keywords such as ‚Äúshingles,‚Äù ‚Äú1.2-3.4 per 1,000,‚Äù etc., as these sparse features match directly, increasing the relevance score.\n2. Semantic Retrieval: Dual Encoder and Vector Representation While sparse retrieval is efficient, it relies on surface-level lexical matching and is powerless against synonymous expressions and implied semantics. For example, if a claim uses ‚Äúcauses‚Äù but the evidence uses ‚Äútriggers,‚Äù BM25 might miss relevant evidence due to the literal mismatch. To solve this, we need Dense Retrieval methods, capturing the deep meaning of text through semantic vector representations.\nThe Dual Encoder is a common dense retrieval architecture. It contains two towers (neural networks) that encode the query and the document into vectors respectively, and measure relevance through similarity (usually cosine similarity) in vector space. Typical implementations include Facebook‚Äôs DPR (Dense Passage Retriever) model, or pre-trained models provided by the more general sentence-transformers library.\nIn our system, we use a pre-trained sentence vector model (such as all-MiniLM-L6-v2 provided by sentence-transformers) to map text to vectors. Key points include:\nHigh-Dimensional Semantic Mapping: The model maps each text (claim or evidence sentence) to a high-dimensional vector, placing semantically similar texts closer in vector space. This means that even if two sentences share no identical words, their vectors will be close if their meanings are related. Synonym and Concept Alignment: Vector representations can automatically learn the proximity of synonymous expressions. For example, ‚Äútrigger‚Äù and ‚Äúcause‚Äù will be embedded in adjacent positions when they appear in similar contexts. Dual Tower Encoding: Queries and documents are encoded separately; cross-input is not required when calculating similarity. This structure facilitates offline pre-computation of document vectors: we can calculate the vectors for every sentence in the evidence library in advance and store them. When a new query comes in, we only need to calculate the query vector once and compute similarity with all evidence vectors, drastically improving retrieval efficiency (supported by tools like Faiss or Milvus for fast vector neighbor search). Hybrid Retrieval: In practice, we often fuse BM25 results with vector retrieval results to balance exact matching and semantic recall. This maximizes recall rate‚Äîensuring neither relevant evidence with synonymous phrasing nor exact entity matches are missed. Code Example: Using SentenceTransformer to load a model and retrieve semantically similar evidence:\nfrom sentence_transformers import SentenceTransformer, util import numpy as np # Load a lightweight pre-trained sentence embedding model model = SentenceTransformer('all-MiniLM-L6-v2') # Calculate vector representations for corpus evidence sentences (recommended to pre-compute and store offline) evidence_embeddings = model.encode(corpus, convert_to_tensor=True) # Calculate vector for the query claim claim_embedding = model.encode(claim, convert_to_tensor=True) # Calculate cosine similarity and get the top 10 closest evidence indices cosine_scores = util.cos_sim(claim_embedding, evidence_embeddings)[0] # Cosine similarity vector top_indices = np.argpartition(-cosine_scores, range(10))[0:10] # Top 10 high-score indices (unsorted) top_indices = top_indices[cosine_scores[top_indices].argsort(descending=True)] # Sort by similarity # View the most relevant piece of evidence top_evidence = corpus[top_indices[0]] print(top_evidence) This code uses util.cos_sim from the sentence-transformers library to quickly calculate similarity and select the top 10 pieces of evidence. Compared to BM25, semantic retrieval can find sentences close in meaning to the claim but not necessarily overlapping in words. For example, for a rumor claim like ‚ÄúVaccination causes autism,‚Äù semantic retrieval can find relevant sentences in scientific papers like ‚ÄúStudies have found no link between vaccines and autism,‚Äù directly refuting the rumor despite different surface vocabulary.\nPerformance Evaluation: To measure the effectiveness of retrieval modules, the MRR (Mean Reciprocal Rank) metric can be used. MRR focuses on the position of the first relevant result in the retrieval list; the higher the position, the higher the score. The formula calculates the reciprocal rank of the relevant result for each query and then takes the average. Pseudo-code example:\ndef mean_reciprocal_rank(results_list, ground_truth_list): rs = [] for results, true_doc in zip(results_list, ground_truth_list): ranks = [idx for idx, doc in enumerate(results) if doc == true_doc] rs.append(1.0 / (ranks[0] + 1) if ranks else 0.0) return np.mean(rs) We want MRR to be as high as possible, meaning correct evidence ranks highly for most queries. generally, combining BM25 and vector retrieval can significantly improve MRR, as the two complement each other‚Äôs blind spots.\n3. Natural Language Inference: Entailment-based Truth Judgment After completing evidence retrieval, the next step is to judge the relationship between the evidence and the claim. This is formalized as a Natural Language Inference (NLI) task: given a claim sentence and an evidence text, the machine needs to judge whether the evidence supports the claim, refutes the claim, or is unrelated to the claim. This transformation is crucial‚Äîit elevates the fact-checking problem to one of logical reasoning, granting the model general reasoning capabilities across domains. Even if the model hasn‚Äôt seen a specific rumor, it can use NLI reasoning skills to judge contradictions between evidence and claims.\nWe typically employ Transformer models fine-tuned on large-scale NLI datasets (like MultiNLI, SNLI, etc.) to perform this task. In a zero-shot setting, a model well-trained on NLI tasks can generalize directly to fact-checking labels without extra fine-tuning on rumor data. For example, the famous FEVER paper maps fact-checking labels to NLI‚Äôs three classes: Entailment corresponds to ‚ÄúSupports,‚Äù Contradiction to ‚ÄúRefutes,‚Äù and Neutral to ‚ÄúNot Enough Info.‚Äù\nImplementation points for the NLI module include:\nInput Construction: Concatenate the claim and evidence into a model input pair. The standard approach is the [CLS] Claim [SEP] Evidence [SEP] format in Transformer input, allowing the model to read both and use attention mechanisms to analyze their relationship. Cross-Encoder: Models used for NLI judgment are usually single-tower structures (unlike the dual encoders mentioned earlier)‚Äîmeaning the claim and evidence are concatenated and sent into the same Transformer for encoding. This Cross-Encoder can use cross-sentence attention to capture subtle logical relationships, being much more precise than independent encoding followed by comparison. This also makes Cross-Encoders common for re-ranking retrieval results. Label Mapping: NLI model output is typically one of three classes: Entailment, Neutral, Contradiction. We need to map these to the fact-checking label space. For example: Supports, Not Enough Info, Refutes. This mapping strategy ensures model judgments align with human verification standards. Code Example: Using ü§ó Transformers pipeline for quick NLI judgment:\nfrom transformers import pipeline # Load a RoBERTa large model fine-tuned on NLI data (supports zero-shot inference) nli_pipeline = pipeline( \"text-classification\", model=\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\" ) # Example: Claim and Evidence claim_text = \"The number of new cases of shingles per year extends from 1.2-3.4 per 1,000.\" evidence_text = \"The number of new cases per year ranges from 1.2 -- 3.4 per 1,000...\" # Construct input pair and predict result = nli_pipeline([{'text': claim_text, 'text_pair': evidence_text}])[0] print(result['label']) # Likely outputs 'ENTAILMENT' or 'CONTRADICTION' etc. print(result['score']) # Probability score In this example, if the content of evidence_text is logically consistent with claim_text, the model should output Entailment; if the evidence clearly refutes the claim, it outputs Contradiction. We then map Entailment to ‚ÄúSupports,‚Äù Contradiction to ‚ÄúRefutes,‚Äù and Neutral to ‚ÄúNot Enough Info.‚Äù\nNote that there may be multiple pieces of evidence; the model should judge each piece separately and then aggregate the results. This belongs to the strategy discussed in the next section. However, simply put, a common rule is: if any piece of evidence is judged as Contradiction (Refutes), the overall judgment is False; otherwise, if at least one piece is Entailment and there are no Contradictions, it is True; if neither, it is unknown. This is similar to the FEVER evaluation standard.\nBesides using specialized discriminative models, recent approaches use Large Language Models (LLMs) for zero-shot NLI. That is, using prompts to let models like GPT-4 directly read the evidence and give a support/refute judgment. For example:\nYou are a fact-checking assistant. Decide if the claim is supported, refuted, or not enough info based on the statements. Claim: Statements: 1. 2. ... Answer with one of: SUPPORTS / REFUTES / NOT ENOUGH INFO. Submitting this prompt to a powerful LLM (like GPT-4) can theoretically yield correct labels. In our tutorial experiments, using a smaller GPT-4 architecture model (called GPT-4o mini) achieved zero-shot classification with about 79.6% accuracy, close to the specialized RoBERTa model (83.2%). This suggests that on specific tasks, fine-tuned smaller models often outperform general large models in cost-effectiveness. However, LLM methods require no training and are easy to extend to new tasks, holding great practical value.\n4. Result Aggregation and Label Mapping After the above steps, we have an NLI model output for each ‚ÄúClaim-Evidence‚Äù pair. In reality, a claim often requires multiple pieces of evidence to reach a conclusion, so we need to aggregate these results to output a final label.\nLabel mapping was mentioned in the previous section (mapping NLI‚Äôs ternary classification back to our output labels). For fact-checking, the output is generally three classes: True/Supported, False/Refuted, Not Enough Info/Neutral.\nResult aggregation needs to consider the synthesis of multiple pieces of evidence. Common strategies include:\nProof-based Aggregation: If at least one piece of evidence clearly supports the claim and there is no contradictory evidence, judge as True; if at least one piece refutes the claim, judge as False; otherwise, if neither supported nor refuted, judge as Not Enough Info. This strategy is similar to the legal idea of ‚Äúone valid piece of evidence is enough to convict/acquit,‚Äù suitable for most fact-checking tasks. Majority Vote: When evidence quality varies, let multiple pieces of evidence vote. For example, if 2 out of 3 pieces support and 1 refutes, decide Support. However, simple voting isn‚Äôt always reliable because evidence is not independent and homogeneous; usually, the most authoritative or relevant evidence should be prioritized. Weighted Fusion: Assign weights to different evidence based on retrieval scores or source credibility, then accumulate their confidence for each category. For example, if one highly relevant piece of evidence refutes, even if several others vaguely support, the high-weight refutation may prevail. Learned Fusion Model: In advanced applications, a verdict fusion model can be trained, taking NLI logits or embeddings of multiple pieces of evidence as input and outputting the final label. This allows the model to learn how to decide when evidence conflicts. But this requires training data and is not zero-shot. In our tutorial framework, we adopt a simplified proof-based rule: Refutes -\u003e False; Supports (no Refutes) -\u003e True; neither -\u003e NEI. While simple, this rule aligns with intuition and performs well in FEVER tasks. In fact, FEVER evaluation requires the model to provide the correct evidence set to be considered correct (the FEVER Score): success is counted only if the label is correct AND the submitted evidence contains a complete chain of support/refutation. This encourages systems to rigorously consider evidence sufficiency during aggregation, otherwise preferring ‚ÄúNot Enough Info.‚Äù The FEVER score measures the accuracy of the model‚Äôs proof process, making it stricter but more practical than simple label accuracy.\nSummary of technical modules: We introduced BM25 sparse retrieval for entity matching, semantic vector retrieval for synonymous semantics, NLI inference for logical discrimination, and finally label mapping and result aggregation for decision-making. Next, we will link these modules to demonstrate an actual automated fact-checking workflow with code examples.\nPractical Workflow and Code Demonstration Below, following the actual modeling order, we link the aforementioned modules to build a simplified rumor detection/fact-checking system, providing key code snippets. The flow includes: Check-Worthiness Detection, Evidence Retrieval (combined sparse and dense), Fact-Checking Judgment (NLI inference output), and Explanation Generation (reasons provided by a generative model).\n1. Check-Worthiness Detection In social media posts or political speeches, not every sentence is worth checking. Most sentences are just chitchat or opinions; only a few contain verifiable specific factual claims that require further evidence retrieval. This poses the Check-Worthiness Detection task: identifying sentences worth checking from a text. This is a binary classification problem (Check-worthy vs. Non-check-worthy), characterized by extreme class imbalance‚Äîcheckable sentences are often less than 5% of the total.\nWe can use supervised learning to train a detection model. For example, fine-tuning a pre-trained BERT model for binary classification. To handle class imbalance, common practices include giving positive examples (checkable sentences) larger loss weights or oversampling positive examples. In the HuggingFace Trainer framework, this can be implemented by customizing the loss. For example:\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments import torch model_name = \"bert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # Map dataset encoding def encode_examples(examples): outputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128) outputs[\"labels\"] = [1 if lbl == \"Check-worthy\" else 0 for lbl in examples[\"label\"]] return outputs encoded_dataset = raw_dataset.map(encode_examples, batched=True) # Define Trainer with Weighted Loss class WeightedLossTrainer(Trainer): def __init__(self, class_weights, *args, **kwargs): super().__init__(*args, **kwargs) self.class_weights = torch.tensor(class_weights).to(self.model.device) def compute_loss(self, model, inputs, return_outputs=False): labels = inputs.get(\"labels\") outputs = model(**inputs) logits = outputs.logits loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights) loss = loss_fct(logits, labels) return (loss, outputs) if return_outputs else loss # Calculate class weights, e.g., Non-check:Check ‚âà 1:20 weights = [1.0, 20.0] trainer = WeightedLossTrainer(class_weights=weights, model=model, args=TrainingArguments(...), train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[\"val\"]) trainer.train() In the code above, we manually set class weights to 1:20, meaning the loss for each check-worthy sample is amplified 20 times, guiding the model to pay more attention to the minority class. This weighted cross-entropy is an effective means to alleviate class imbalance.\nThe trained model can then filter out a list of sentences requiring checking from input text. Assuming we have located several claims worth checking, we proceed to fact-check each claim.\n2. Evidence Retrieval Having a specific claim, we need to find supporting or refuting evidence sentences in a massive unstructured text base (knowledge base). We combine BM25 sparse retrieval and semantic dense retrieval.\nBM25 Retrieval: First, search the index using keywords from the claim. Code example is as shown in the BM25S section above. BM25 will return a set of candidate evidence sentences (e.g., Top 50). Since BM25 favors exact matching, this step ensures documents related to key entities are not missed. Vector Retrieval: In parallel, retrieve using semantic vectors of the claim. Code example is in the semantic retrieval section. We can take the group of candidate sentences with the highest vector similarity (e.g., Top 50). Vector retrieval supplements evidence missed by BM25 that is semantically relevant but phrased differently. Candidate Fusion: Merge the candidate sets from both methods (usually totaling dozens of sentences), remove duplicates (as methods may hit the same sentence), and simply sort. Sorting can be based on BM25 scores or vector scores, or by training a learning-to-rank model. For simplicity, sorting by BM25 score is acceptable. Evidence Filtering: To reduce the burden on the subsequent NLI model, we can add a filtering layer. For example, use a lightweight Cross-Encoder to re-score and re-rank candidate evidence, selecting the Top 5 to send to the next inference step. This improves accuracy but increases complexity. For demonstration, this can be omitted, using all candidates. 3. Fact-Checking Judgment (NLI Inference) For each claim, we now have several candidate evidence sentences. We let the NLI model judge each Claim-Evidence pair. As mentioned, using a trained NLI model can directly give ‚ÄúSupports/Refutes/Irrelevant‚Äù classification results.\nPseudo-code follows:\nlabels_map = {\"ENTAILMENT\": \"SUPPORTS\", \"CONTRADICTION\": \"REFUTES\", \"NEUTRAL\": \"NOT ENOUGH INFO\"} final_decisions = [] for claim in claims_to_check: decisions = [] for evidence in candidate_evidences[claim]: result = nli_pipeline({'text': claim, 'text_pair': evidence}) decisions.append(labels_map[result[0]['label']]) # Result Aggregation if \"REFUTES\" in decisions: final = \"False\" # Evidence refutes elif \"SUPPORTS\" in decisions: final = \"True\" # Supported and not refuted else: final = \"Not Enough Info\" final_decisions.append((claim, final)) This logic implements a simple evidence aggregation strategy. After getting the final label, we have completed the truth judgment of the claim. However, giving just a label is often insufficient. In rumor governance, we also want to explain why the model made such a judgment. To this end, we introduce the next step: Explanation Generation.\n4. Explanation Generation To improve the acceptability and effectiveness of detection results, we want the model to generate a natural language explanation stating which evidence it based its conclusion on and how it reasoned. For example, for a rumor labeled ‚ÄúFalse,‚Äù the explanation might be: ‚ÄúBecause Evidence X indicates the exact opposite, the claim is unfounded.‚Äù Such explanations not only help the audience understand the truth but also counter the persuasiveness of the rumor.\nGenerating explanations can be viewed as an NLG (Natural Language Generation) task, which we can complete using large generative models (like GPT). In practice, we can design a Prompt to let the model generate an explanation based on the claim, label, and evidence. For example:\nYou are tasked with creating an explanation of a fact check. Claim: Label: Statements: - - ... Please provide a brief explanation as to why the label is correct based on the provided evidence. Submitting this template filled with specific content to a model like GPT-4o mini will generate an explanatory statement. For instance, for a rumor claim ‚ÄúMaze Runner is a sports competition‚Äù where evidence shows Maze Runner is a movie produced by Ellen Goldsmith-Vein, it might generate:\n‚ÄúThe ‚ÄòRefutes‚Äô label is correct because the evidence clearly states that The Maze Runner is a film produced by Ellen Goldsmith-Vein, whereas the claim asserts it is a sports competition.‚Äù\nThis explanation clearly articulates how the evidence contradicts the claim, supporting the ‚ÄúRefutes‚Äù conclusion.\nExplanation Evaluation: How do we evaluate the quality of generated explanations? Traditional NLG metrics like BLEU and ROUGE look mainly at lexical overlap and cannot measure the logical consistency (i.e., ‚Äúfaithfulness‚Äù) between explanation and evidence. For this, we introduce the G-Eval mechanism: letting a more powerful LLM act as a judge, scoring according to pre-defined standards. For example, defining a scale of 1-5, where 1 means the explanation completely contradicts the evidence, and 5 means the explanation accurately and completely reflects the relationship between evidence and claim.\nWe can construct an evaluation prompt template:\nEvaluation Criteria: Faithfulness (1-5) - the factual alignment between the fact-checking explanation and the evidence. Evidence Provided: Fact-Checking Explanation: Evaluation: Please provide a score from 1 to 5 only. Let GPT-4 evaluate the same explanation multiple times (e.g., n=20). Due to LLM output randomness, we can average the scores to reduce single-instance bias. This evaluation method leverages the understanding and reasoning capabilities of LLMs, known as LLM-as-a-judge or the basic idea of G-Eval. Research shows G-Eval can evaluate generated text with consistency approaching human levels.\nIt should be noted that explanation generation is the icing on the cake. In practical applications, if the goal is just automated judgment, the first three steps are sufficient. But providing explanations undoubtedly enhances the system‚Äôs interpretability and persuasiveness, making it very suitable for assisting human moderation or public education scenarios.\nSo far, our automated detection workflow has covered every link from inputting raw text to outputting a judgment result with an explanation. Let‚Äôs look at the utility of this flow through actual cases.\nCase Studies: Vaccine Rumors and Climate Change Narratives To intuitively understand how the above model deals with ‚Äúfalse narratives cloaked in science,‚Äù we demonstrate with two social hotspots.\nCase 1: Vaccine Rumor Scenario: An online post claims: ‚ÄúResearch proves that the MMR (measles, mumps, rubella) vaccine causes autism in children.‚Äù This is a classic anti-vaccine narrative that cites ‚Äúresearch proves‚Äù to add scientific legitimacy. However, the claim originates from a 1998 paper in The Lancet that was later proven fraudulent and retracted.\nCheck-Worthiness: The model identifies this sentence as check-worthy because it involves medical causality, has significant impact, and is verifiable. Evidence Retrieval: BM25 might use keywords like ‚ÄúMMR,‚Äù ‚Äúautism,‚Äù ‚Äústudy‚Äù to retrieve relevant news or Wikipedia pages; vector retrieval captures semantically related sentences like ‚Äúno link found,‚Äù ‚Äústudies find no evidence.‚Äù Actual evidence is easily found in authoritative medical sources; e.g., an AAP press release stating: ‚ÄúNumerous studies across time and countries have found no credible link between vaccines and autism. The report originally claiming MMR causes autism was retracted for fraud, and its author lost his medical license.‚Äù NLI Judgment: Given the evidence above, the model determines ‚ÄúContradiction‚Äù because the evidence explicitly refutes the claim (evidence says no link, claim says causal). Mapped label is ‚ÄúFalse.‚Äù Result Aggregation: If multiple pieces of evidence all show no link found, they consistently point to False with no conflict. The model outputs the rumor as False. Explanation Generation: The model generates an explanation, e.g., ‚ÄúAuthoritative research has repeatedly confirmed no causal link between the MMR vaccine and autism; the paper originally claiming a link was retracted for data fraud. Therefore, the claim is unfounded.‚Äù This explanation highlights key evidence information (‚Äúno causal link,‚Äù ‚Äúpaper fraud retracted‚Äù), powerfully refuting the scientific camouflage of the rumor. Case 2: Climate Change Narrative Scenario: A blog writes: ‚ÄúArctic sea ice area has been trending upwards in the past few years; global warming is a hoax.‚Äù This argument questions global warming by selecting data from a specific time period; it looks like it cites scientific observations but is an example of misleading content. It doesn‚Äôt fabricate facts entirely (Arctic sea ice did rebound in certain years) but ignores long-term trends through cherry-picking.\nCheck-Worthiness: This sentence is also check-worthy, involving scientific data and conclusions needing verification. Evidence Retrieval: Retrieval might return NASA or IPCC report summaries on long-term Arctic sea ice trends. For example: ‚ÄúSatellite data shows the long-term trend of Arctic sea ice coverage is still declining; despite brief increases in some years, the overall trend supports continued global warming.‚Äù The model might also retrieve scientific debunking articles regarding ‚Äúglobal warming hoaxes.‚Äù NLI Judgment: The relationship is subtle. The evidence doesn‚Äôt say the specific data point is ‚Äúwrong,‚Äù but provides the full context (short-term fluctuation doesn‚Äôt change long-term warming). The model might lean towards ‚ÄúContradiction‚Äù (because the claim calls warming a hoax, while evidence emphasizes warming is real). Some evidence might be seen as ‚ÄúNeutral‚Äù if it only provides data without directly addressing the ‚Äúhoax‚Äù claim. Result Aggregation: Synthesizing authoritative evidence, there should be sufficient reason to refute ‚Äúglobal warming is a hoax.‚Äù Even if the model doesn‚Äôt judge every piece of evidence as Contradiction, multiple scientific reports together form a strong counter-proof. Thus, the final output should be False. Explanation Generation: The explanation needs to balance scientific accuracy and accessibility, e.g., ‚ÄúAlthough Arctic sea ice rebounded in some recent periods, the overall trend remains downward. Authoritative climate reports state global warming is happening and is not a hoax. Therefore, asserting ‚Äòglobal warming is a hoax‚Äô is misleading through cherry-picking.‚Äù Such an explanation reveals the claim‚Äôs technique (confusing long-term trends with short-term growth) and gives the complete scientific conclusion. These cases show that our model pipeline can not only identify completely false statements but also handle narratives based on facts but with wrong conclusions. For the latter, the model relies on multiple evidence fragments to reveal issues from different angles. This is why multi-evidence retrieval and NLI reasoning are necessary‚Äîthey empower AI to judge complex framed/biased statements like an expert, rather than just handling simple True/False questions.\nOf course, these cases also remind us of model limitations: if the claim is expressed implicitly (e.g., suggestive wrong causality), the model‚Äôs NLI judgment might lack confidence; if evidence is insufficient or the knowledge base lacks relevant info (long-tail rumors), the model can only output ‚ÄúNot Sure.‚Äù The next section discusses these challenges and future directions.\nFrontiers and Challenges Current retrieval-augmented detection frameworks show high accuracy and interpretability in experiments, but there are still many open questions and room for improvement. Some are technical frontiers, others are challenges in application deployment. In this section, we list several directions worth watching:\nIntegration of Retrieval-Augmented Generation (RAG) Architecture: RAG refers to seamlessly integrating retrieval modules into generative models, enabling the model to retrieve external knowledge while generating answers. In the future, we can more tightly combine fact-checking systems with Large Language Models, allowing the model to dynamically retrieve materials when generating conclusions or explanations, ensuring the factualness and completeness of the output. This approach has been proven effective in tasks like open-domain QA and is highly beneficial for explanation generation in rumor detection. RAG architectures can also handle multi-hop reasoning for longer documents by iteratively retrieving relevant information to deal with complex claims. Explanation Quality Evaluation (G-Eval) Mechanism: With the application of generative models, how to evaluate whether their output explanations are reliable becomes a new challenge. The G-Eval framework offers the idea of using LLMs for evaluation. Future research can further explore multi-dimensional evaluation standards, such as examining not just faithfulness (strictly based on evidence) but also readability, persuasiveness, etc. The evaluation process can be made more automated and robust, for instance, by combining multiple different LLM judges to offset single-model bias. for communication research pursuing academic rigor, using AI to evaluate AI-generated explanations requires caution, but this direction promises to drastically improve evaluation efficiency. Zero-Shot and Out-of-Domain Generalization: A difficulty of information disorder is its rapid change. New conspiracy theories and slang expressions appear constantly and cannot be exhaustively included in training sets. Therefore, zero-shot learning and cross-domain generalization capabilities are crucial. We have introduced some zero-shot reasoning capability via NLI, but future work can explore prompting large models for more complex reasoning, or using self-supervised pre-training to let models master common sense reasoning, thus being able to judge rumors never seen before. Of course, complete zero-shot is unrealistic, so Continual Learning is also a direction‚Äîletting models evolve as new data updates, while avoiding catastrophic forgetting and rumor data polluting the model. Multimodal Detection: The methods discussed currently focus mainly on text. However, real-world disinformation is often multimodal‚Äîfake videos, forged images, audio clips, etc. The rise of Deepfakes is an example, requiring integration with image forensics, voice recognition, and other technologies. Multimodal fusion detection systems are urgently needed; for example, when a rumor spreads via image and text, how to combine NLP models with Computer Vision models for a shared judgment. Some multimodal fact-checking datasets exist, but this field is still in its infancy and holds great challenges. Adversarial Attacks and Robustness: Generative AI is not only a tool for defenders but is also used by bad actors to produce more deceptive rumor texts. These adversarial texts might specifically target detector weaknesses, avoiding keywords or using flowery syntax to confuse models. Improving model robustness against such attacks is urgent. Feasible solutions include: introducing adversarial training (letting the model see perturbed fake text); and developing verifiable models, such as using logical rules to constrain output. Meanwhile, we must prevent the model itself from generating misinformation‚Äîin RAG architectures, if false information is retrieved, the model might be misled, so source credibility assessment is also vital. Knowledge Gaps and the Long-Tail Problem: When rumors involve very novel or obscure topics, direct evidence might not be found in the knowledge base. Retrieval methods fall into a ‚Äúknowledge vacuum.‚Äù The model might label such claims generally as ‚ÄúNot Enough Info,‚Äù but in communication research, we sometimes need to predictively judge risk. To this end, knowledge sources can be expanded (e.g., accessing scientific literature, expert databases) or deduction mechanisms introduced: inferring conclusions based on common sense and related knowledge. However, letting AI infer truth based on incomplete knowledge carries risks and needs deep exploration. Evaluation Metrics and Misjudgment Costs: For detection systems, simply pursuing overall accuracy might mask problems. For instance, if rumors are only 1% of the sample, a model predicting ‚Äúnon-rumor‚Äù for everything has 99% accuracy but is meaningless. Therefore, more appropriate assessment focuses on Recall of the positive class and F1 score. In practice, the cost of missing a real rumor (False Negative) differs from misjudging true information as a rumor (False Positive); model thresholds and optimization goals should be adjusted according to the application scenario. Communication scholars are particularly concerned with reducing False Negatives (don‚Äôt miss important rumors) while providing evidence for human review. This requires comprehensive performance evaluation combining quantitative metrics and qualitative analysis. Advantages and Limitations: Current retrieval-augmented detection models have clear advantages over pure classifiers: interpretable, scalable, and domain-agnostic (via evidence retrieval). At the same time, they easily integrate into human workflows‚Äîfact-checkers can directly view model-retrieved evidence and judgments to assist decisions. However, limitations cannot be ignored: high system complexity, need for massive external knowledge support, strong dependence on knowledge base freshness (helpless against new fake news not yet indexed), and difficulty capturing implicit misleading (models excel at explicit truth judgment but sometimes struggle to qualify technical misleading like satire or exaggeration). Additionally, the quality of explanations output by models is currently unstable, with potential AI hallucination, requiring human verification or more reliable generation constraints.\nIn conclusion, disinformation detection is a continuously evolving field of offense and defense, requiring interdisciplinary perspectives and multi-faceted innovation. From communication theory to artificial intelligence technology, every step of development will help us better identify and respond to information disorder. We hope the theoretical-practical framework provided in this tutorial offers a beneficial reference for related research and look forward to more intelligent and reliable detection systems emerging in complex and changing environments to jointly safeguard a healthy information ecosystem.\n",
  "wordCount" : "7025",
  "inLanguage": "en",
  "datePublished": "2026-01-04T00:00:00Z",
  "dateModified": "2026-01-04T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lyuxi Liu"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://llspublic.github.io/posts/rag/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lyuxi Liu",
    "logo": {
      "@type": "ImageObject",
      "url": "https://llspublic.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://llspublic.github.io/" accesskey="h" title="Lyuxi Liu (Alt + H)">Lyuxi Liu</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                    <ul class="lang-switch"><li>|</li>
                        <li>
                            <a href="https://llspublic.github.io/zh/" title="‰∏≠Êñá"
                                aria-label="‰∏≠Êñá">Zh</a>
                        </li>
                    </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://llspublic.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://llspublic.github.io/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial
    </h1>
    <div class="post-meta"><span title='2026-01-04 00:00:00 +0000 UTC'>January 4, 2026</span>&nbsp;¬∑&nbsp;<span>Lyuxi Liu</span>&nbsp;|&nbsp;<span>Translations:</span>
<ul class="i18n_list">
    <li>
        <a href="https://llspublic.github.io/zh/posts/rag/">Zh</a>
    </li>
</ul>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#retrieval-augmented-detection-of-disinformation-theory-technology-and-practical-tutorial" aria-label="Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial">Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial</a><ul>
                        
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#theoretical-background-concepts-and-psychological-mechanisms-of-information-disorder" aria-label="Theoretical Background: Concepts and Psychological Mechanisms of Information Disorder">Theoretical Background: Concepts and Psychological Mechanisms of Information Disorder</a><ul>
                        
                <li>
                    <a href="#1-concepts-and-classification-of-information-disorder" aria-label="1. Concepts and Classification of Information Disorder">1. Concepts and Classification of Information Disorder</a></li>
                <li>
                    <a href="#2-psychological-mechanisms-of-information-disorder" aria-label="2. Psychological Mechanisms of Information Disorder">2. Psychological Mechanisms of Information Disorder</a></li></ul>
                </li>
                <li>
                    <a href="#evolution-of-detection-paradigms-from-classifiers-to-retrieve-then-verify" aria-label="Evolution of Detection Paradigms: From Classifiers to &ldquo;Retrieve-then-Verify&rdquo;">Evolution of Detection Paradigms: From Classifiers to &ldquo;Retrieve-then-Verify&rdquo;</a></li>
                <li>
                    <a href="#detailed-explanation-of-key-technical-modules" aria-label="Detailed Explanation of Key Technical Modules">Detailed Explanation of Key Technical Modules</a><ul>
                        
                <li>
                    <a href="#1-sparse-retrieval-bm25s-algorithm" aria-label="1. Sparse Retrieval: BM25S Algorithm">1. Sparse Retrieval: BM25S Algorithm</a></li>
                <li>
                    <a href="#2-semantic-retrieval-dual-encoder-and-vector-representation" aria-label="2. Semantic Retrieval: Dual Encoder and Vector Representation">2. Semantic Retrieval: Dual Encoder and Vector Representation</a></li>
                <li>
                    <a href="#3-natural-language-inference-entailment-based-truth-judgment" aria-label="3. Natural Language Inference: Entailment-based Truth Judgment">3. Natural Language Inference: Entailment-based Truth Judgment</a></li>
                <li>
                    <a href="#4-result-aggregation-and-label-mapping" aria-label="4. Result Aggregation and Label Mapping">4. Result Aggregation and Label Mapping</a></li></ul>
                </li>
                <li>
                    <a href="#practical-workflow-and-code-demonstration" aria-label="Practical Workflow and Code Demonstration">Practical Workflow and Code Demonstration</a><ul>
                        
                <li>
                    <a href="#1-check-worthiness-detection" aria-label="1. Check-Worthiness Detection">1. Check-Worthiness Detection</a></li>
                <li>
                    <a href="#2-evidence-retrieval" aria-label="2. Evidence Retrieval">2. Evidence Retrieval</a></li>
                <li>
                    <a href="#3-fact-checking-judgment-nli-inference" aria-label="3. Fact-Checking Judgment (NLI Inference)">3. Fact-Checking Judgment (NLI Inference)</a></li>
                <li>
                    <a href="#4-explanation-generation" aria-label="4. Explanation Generation">4. Explanation Generation</a></li></ul>
                </li>
                <li>
                    <a href="#case-studies-vaccine-rumors-and-climate-change-narratives" aria-label="Case Studies: Vaccine Rumors and Climate Change Narratives">Case Studies: Vaccine Rumors and Climate Change Narratives</a><ul>
                        
                <li>
                    <a href="#case-1-vaccine-rumor" aria-label="Case 1: Vaccine Rumor">Case 1: Vaccine Rumor</a></li>
                <li>
                    <a href="#case-2-climate-change-narrative" aria-label="Case 2: Climate Change Narrative">Case 2: Climate Change Narrative</a></li></ul>
                </li>
                <li>
                    <a href="#frontiers-and-challenges" aria-label="Frontiers and Challenges">Frontiers and Challenges</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="retrieval-augmented-detection-of-disinformation-theory-technology-and-practical-tutorial">Retrieval-Augmented Detection of Disinformation: Theory, Technology, and Practical Tutorial<a hidden class="anchor" aria-hidden="true" href="#retrieval-augmented-detection-of-disinformation-theory-technology-and-practical-tutorial">#</a></h1>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>As generative artificial intelligence reshapes the information ecosystem, information dissemination in the &ldquo;post-truth&rdquo; era is becoming unprecedentedly complex. From deepfake videos that blur the line between reality and fiction to conspiracy theories proliferating in public health, <strong>Information Disorder</strong> has evolved into a complex socio-technical phenomenon, rather than a simple binary issue of &ldquo;true&rdquo; or &ldquo;false.&rdquo; The traditional term &ldquo;fake news&rdquo; has lost its precise meaning due to excessive politicization and struggles to encapsulate the diverse types of inaccurate information present today. To effectively research and govern information disorder, we need to establish a rigorous conceptual framework and technical system that covers both theoretical classification/cognitive mechanisms and practical automated detection workflows.</p>
<p>This article is aimed at researchers in the field of communication. It systematically introduces the theoretical background and psychological mechanisms of information disorder, elaborates on the evolution of detection paradigms, details key technical modules (including sparse retrieval, semantic retrieval, Natural Language Inference, and result aggregation), and demonstrates how to build an automated fact-checking system through practical code. Subsequently, we will use case studies (such as vaccine rumors and climate change narratives) to illustrate how models deal with false narratives cloaked in &ldquo;science.&rdquo; Finally, we discuss frontier explorations and challenges in current methods, including the integration of Retrieval-Augmented Generation (RAG) architectures, explanation evaluation mechanisms using LLMs (G-Eval), zero-shot and multimodal detection difficulties, as well as the advantages and limitations of these models. The entire article aims to be structurally rigorous and concise, helping communication researchers build a complete understanding from theory to practice.</p>
<hr>
<h2 id="theoretical-background-concepts-and-psychological-mechanisms-of-information-disorder">Theoretical Background: Concepts and Psychological Mechanisms of Information Disorder<a hidden class="anchor" aria-hidden="true" href="#theoretical-background-concepts-and-psychological-mechanisms-of-information-disorder">#</a></h2>
<h3 id="1-concepts-and-classification-of-information-disorder">1. Concepts and Classification of Information Disorder<a hidden class="anchor" aria-hidden="true" href="#1-concepts-and-classification-of-information-disorder">#</a></h3>
<p>To study disinformation, one must first clarify conceptual boundaries. The <strong>Information Disorder</strong> framework, proposed by Wardle and Derakhshan in 2017, strictly defines types of inaccurate information using two dimensions: truthfulness and intent. This framework categorizes online rumors into three types:</p>
<ul>
<li><strong>Misinformation:</strong> Content that is objectively false, but the sharer has no subjective malicious intent. For example, spreading wrong information due to a lack of knowledge or statistical oversight.</li>
<li><strong>Disinformation:</strong> Content that is false and intentionally created or spread to mislead or harm the public, carrying clear malicious motives, including fabricated political conspiracy theories or forged propaganda.</li>
<li><strong>Malinformation:</strong> Content that may be true in itself but is taken out of context or maliciously leaked to cause harm. For example, leaking private truths to smear others.</li>
</ul>
<p>The above classification provides a rigorous ontological basis for researchers and a standard for data annotation. On this basis, Wardle further subdivided seven common forms of information disorder, forming a spectrum from mild misleading to complete fabrication:</p>
<ol>
<li><strong>Satire/Parody:</strong> No intention to cause harm but has the potential to fool.</li>
<li><strong>False Connection:</strong> Headlines, visuals, or captions do not support the content (clickbait).</li>
<li><strong>Misleading Content:</strong> Misleading use of information to frame an issue or individual (cherry-picking).</li>
<li><strong>False Context:</strong> Genuine content is shared with false contextual information.</li>
<li><strong>Imposter Content:</strong> Genuine sources are impersonated.</li>
<li><strong>Manipulated Content:</strong> Genuine information or imagery is manipulated to deceive (e.g., photoshopping).</li>
<li><strong>Fabricated Content:</strong> New content is 100% false, designed to deceive and do harm.</li>
</ol>
<p>These classifications emphasize that the characteristics of different types of false information vary widely. For instance, satire often contains humorous elements, while fabricated content may be carefully disguised as news reports. If researchers mix them indiscriminately (collectively calling them &ldquo;fake news&rdquo;), model training will only learn noise. Therefore, detection strategies and feature engineering should be customized for specific categories to improve model precision.</p>
<h3 id="2-psychological-mechanisms-of-information-disorder">2. Psychological Mechanisms of Information Disorder<a hidden class="anchor" aria-hidden="true" href="#2-psychological-mechanisms-of-information-disorder">#</a></h3>
<p>Communication studies are concerned not only with how information spreads but also why individuals believe in false information. Research shows that people do not believe rumors solely due to ignorance; highly educated populations can also be deceived. This is due to a series of biases and heuristics in human cognition. Integrating these psychological mechanisms into models can help predict who is more susceptible to false information.</p>
<p>Several core psychological factors include:</p>
<ul>
<li><strong>Identity Protective Cognition (Motivated Reasoning):</strong> Also known as group identity bias. People tend to selectively accept information based on their partisan or group identity, automatically filtering for content that aligns with their stance. This motivated cognitive processing causes people to stick to their original beliefs even in the face of contradictory evidence to maintain group identity. In short, political stance &ldquo;filters&rdquo; facts; highly partisan individuals are more likely to believe misinformation that fits their ideology.</li>
<li><strong>Cognitive Laziness:</strong> The tendency to lack cognitive reflection or deep thinking. Psychology divides human thinking into &ldquo;System 1&rdquo; (intuition-dependent) and &ldquo;System 2&rdquo; (rational analysis). Some people rely more on intuition and are unwilling to expend mental effort, thus lacking skepticism toward rumors. Research finds that people with low analytical thinking skills are more likely to believe fake news, and their susceptibility is closely related to being &ldquo;lazy thinkers.&rdquo; This means the spread of fake news is often not due to excessive partisan bias, but a lack of habit for rational thought.</li>
<li><strong>Illusory Truth Effect:</strong> Simply hearing a statement repeated multiple times makes people more inclined to believe it is true, even if it is objectively false. Cognitive psychology calls this the <em>illusory truth effect</em>. Repetition makes information familiar, and familiarity is misused by the brain as a cue for truthfulness, thereby increasing the credibility of false information. Even if the audience originally knew the truth, repeated exposure can weaken their judgment.</li>
<li><strong>Emotional Arousal:</strong> Information with strong emotions (such as fear, anger, moral outrage) is more likely to attract attention and spread widely. High-arousal information can bypass rational analysis and directly affect judgment. When people are emotionally charged, their ability to distinguish between true and false information declines, making them more likely to rashly forward or believe statements that match their emotions.</li>
</ul>
<p>In addition to the above psychological mechanisms, communication data also reveals some counter-intuitive phenomena. For example, we easily overestimate the impact of disinformation on the entire population, as if everyone is being &ldquo;brainwashed,&rdquo; yet underestimate human agency. Empirical research shows: most ordinary users are exposed to very few rumors; the large-scale spread of disinformation is often concentrated in a very small number of active fringe groups. The proliferation of disinformation is more like a &ldquo;carnival of a small group&rdquo; rather than &ldquo;everyone getting infected.&rdquo; Therefore, governance should shift focus from scanning the entire web to targeted interventions for high-risk susceptible populations. This reminds us not to fall into the trap of &ldquo;algorithmic determinism&rdquo;‚Äîsocial media algorithms are not the sole culprit for rumors; human needs and choices are equally important.</p>
<p>Finally, special attention must be paid to misleading narratives cloaked in science. Disinformation on scientific issues is usually more subtle; it does not directly lie about facts but distorts cognition through framing. For example, on the issue of climate change, different wording (&ldquo;global warming&rdquo; vs. &ldquo;climate change&rdquo;) affects the acceptance of different groups. Research found that US conservatives have less resistance to the term &ldquo;climate change&rdquo; compared to &ldquo;global warming.&rdquo; This semantic difference essentially reflects a difference in stance. Similarly, vaccine rumors often cite &ldquo;scientific-sounding&rdquo; rhetoric (e.g., citing the retracted Wakefield pseudo-study) to gain trust. These contents do not fabricate new facts but use existing scientific terminology or data to mislead, making traditional truth verification more difficult. This poses higher requirements for automated detection systems: they must not only judge truth or falsehood but also capture whether the stance and frame are biased.</p>
<p>In summary, information disorder is not a simple rumor problem but a complex phenomenon containing multi-dimensional classifications and deep psychological mechanisms. Understanding these theoretical backgrounds helps design more targeted detection paradigms and models.</p>
<hr>
<h2 id="evolution-of-detection-paradigms-from-classifiers-to-retrieve-then-verify">Evolution of Detection Paradigms: From Classifiers to &ldquo;Retrieve-then-Verify&rdquo;<a hidden class="anchor" aria-hidden="true" href="#evolution-of-detection-paradigms-from-classifiers-to-retrieve-then-verify">#</a></h2>
<p>Faced with emerging false information, early research often attempted to train end-to-end text classification models (e.g., judging whether a piece of news is fake). While these traditional binary classification methods are intuitive, they have significant flaws: they make truth judgments based solely on the input text itself, lacking an explanation for the basis of the judgment. In practice, this leads to a crisis of trust‚Äîthe model gives a &ldquo;True&rdquo; or &ldquo;False&rdquo; label but cannot explain &ldquo;why,&rdquo; leaving users unconvinced.</p>
<p>To improve the credibility and effectiveness of detection, the <strong>&ldquo;Retrieve-then-Verify&rdquo; paradigm</strong> has emerged in recent years. This paradigm simulates the workflow of human fact-checkers: first retrieving external evidence for the claim to be verified, and then judging the truth of the claim based on the evidence. This process has proven to be more interpretable and robust than simple text classification:</p>
<ol>
<li><strong>Evidence Retrieval:</strong> For each claim that needs verification, retrieve relevant documents or sentences from a large trusted knowledge base as evidence sources. For example, for a political statement, retrieve snippets from Wikipedia or authoritative news reports mentioning the same event or data.</li>
<li><strong>Natural Language Inference (NLI):</strong> Compare the retrieved evidence with the original claim, and use an NLI model to judge the relationship of the evidence to the claim‚Äîwhether it <strong>Supports</strong> (entails), <strong>Refutes</strong> (contradicts), or is <strong>Neutral</strong> (unrelated). This process is equivalent to logical reasoning, determining whether the claim stands up in the face of evidence.</li>
<li><strong>Result Aggregation:</strong> Synthesize the inference results of multiple pieces of evidence to give a final verdict. For instance, if most evidence supports the claim without contradiction, it is judged as True; if reliable evidence refutes the claim, it is judged as False; if evidence is insufficient, it is marked as Not Enough Info. This aggregation strategy ensures model decisions are based on a sufficient set of evidence rather than an isolated single source.</li>
</ol>
<p>The advantage of the &ldquo;Retrieve-then-Verify&rdquo; paradigm lies in its process interpretability. The model not only outputs a conclusion but can also provide the evidence it referenced (e.g., citing a sentence from a Wikipedia article), making the basis of the judgment clear to the user. For example, the model can say: &ldquo;This health rumor is false because authoritative medical study X indicates the opposite.&rdquo; Such detection systems are more likely to gain the trust of users and fact-checkers. Additionally, this paradigm handles open-domain unknown claims better: even if the claim itself is not in the training data, the model can make a judgment by retrieving new information, possessing certain zero-shot capabilities.</p>
<p>It is important to emphasize that under the &ldquo;Retrieve-then-Verify&rdquo; framework, we no longer encourage training an opaque end-to-end fake news classifier. Instead, we build a modular pipeline: each step (retrieval, reasoning, aggregation) can be optimized independently and produce intermediate interpretable results (such as retrieval lists, evidence annotations), thereby improving the transparency and interpretability of the system as a whole. This is particularly important for communication research‚Äîwhen we use AI models to analyze rumor propagation in public opinion, we need to be clear about which factual basis the model used to reach its conclusion, in order to align with journalistic professionalism and social science theories.</p>
<p>In summary, the detection paradigm is evolving from &ldquo;black-box&rdquo; text classification to a human-like &ldquo;Evidence Retrieval ‚Üí Logical Verification&rdquo; flow. The next section will delve into the key technical modules in this process.</p>
<hr>
<h2 id="detailed-explanation-of-key-technical-modules">Detailed Explanation of Key Technical Modules<a hidden class="anchor" aria-hidden="true" href="#detailed-explanation-of-key-technical-modules">#</a></h2>
<p>To implement a retrieval-augmented detection system, multiple NLP technical modules need to be organically combined. Below we detail the core components: Sparse Retrieval, Semantic Retrieval, Natural Language Inference (NLI), and Result Aggregation/Label Mapping strategies.</p>
<h3 id="1-sparse-retrieval-bm25s-algorithm">1. Sparse Retrieval: BM25S Algorithm<a hidden class="anchor" aria-hidden="true" href="#1-sparse-retrieval-bm25s-algorithm">#</a></h3>
<p>Sparse Retrieval refers to traditional keyword-matching retrieval methods, represented by BM25. BM25 is a probabilistic retrieval model that improves upon TF-IDF by adding saturation handling for term frequency and length normalization. Simply put, BM25 calculates a relevance score based on shared vocabulary between the query and the document, the frequency of terms in the document, and the inverse document frequency across the entire corpus. It excels at exact matching of query terms and is often very effective for entity words like names and locations.</p>
<p>In the system introduced in this article, we use an efficient implementation variant of BM25 called <strong>BM25S (BM25 Sparse)</strong>. BM25S is available as a Python library (e.g., <code>bm25s</code>) and features the following:</p>
<ul>
<li><strong>Exact Matching:</strong> Excellent performance for queries requiring exact matches of proper nouns, numbers, etc. If a specific name or place appears in the claim, BM25 can quickly lock onto documents containing these entities. In contrast, some semantic vector models might miss these exact matches due to over-generalization.</li>
<li><strong>Efficient Indexing:</strong> BM25S uses an inverted index structure, offering extremely fast retrieval speeds for large corpora, achieving millisecond-level query responses even with millions of documents.</li>
<li><strong>Length Normalization:</strong> The BM25 formula introduces document length penalties to prevent long documents from getting unfairly high scores simply because they contain more words. This improves the fairness of relevance scoring.</li>
<li><strong>Easy Implementation:</strong> Using BM25S requires tokenizing the corpus and building an index. Once the index object is built, efficient retrieval can be performed for any query. As the &ldquo;first line of defense&rdquo; for RAG systems, the sparse retrieval module is relatively simple to implement and stable.</li>
</ul>
<p><strong>Code Example:</strong> Assuming we have an evidence sentence library <code>corpus</code> (where each element is an evidence sentence string), we can use BM25S for indexing and retrieval:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> bm25s
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build BM25 index</span>
</span></span><span style="display:flex;"><span>retriever <span style="color:#f92672">=</span> bm25s<span style="color:#f92672">.</span>BM25(corpus<span style="color:#f92672">=</span>corpus)
</span></span><span style="display:flex;"><span>retriever<span style="color:#f92672">.</span>index(bm25s<span style="color:#f92672">.</span>tokenize(corpus))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Execute retrieval for the query claim</span>
</span></span><span style="display:flex;"><span>claim <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;The number of new cases of shingles per year extends from 1.2-3.4 per 1,000.&#34;</span>
</span></span><span style="display:flex;"><span>results, scores <span style="color:#f92672">=</span> retriever<span style="color:#f92672">.</span>retrieve(bm25s<span style="color:#f92672">.</span>tokenize(claim), k<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(results[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">3</span>])  <span style="color:#75715e"># Print the top 3 most relevant evidence sentences</span>
</span></span></code></pre></div><p>The above code will return a list of evidence texts highly overlapping with the content of the <code>claim</code>. BM25 will prioritize returning sentences containing keywords such as &ldquo;shingles,&rdquo; &ldquo;1.2-3.4 per 1,000,&rdquo; etc., as these sparse features match directly, increasing the relevance score.</p>
<h3 id="2-semantic-retrieval-dual-encoder-and-vector-representation">2. Semantic Retrieval: Dual Encoder and Vector Representation<a hidden class="anchor" aria-hidden="true" href="#2-semantic-retrieval-dual-encoder-and-vector-representation">#</a></h3>
<p>While sparse retrieval is efficient, it relies on surface-level lexical matching and is powerless against synonymous expressions and implied semantics. For example, if a claim uses &ldquo;causes&rdquo; but the evidence uses &ldquo;triggers,&rdquo; BM25 might miss relevant evidence due to the literal mismatch. To solve this, we need <strong>Dense Retrieval</strong> methods, capturing the deep meaning of text through semantic vector representations.</p>
<p>The <strong>Dual Encoder</strong> is a common dense retrieval architecture. It contains two towers (neural networks) that encode the query and the document into vectors respectively, and measure relevance through similarity (usually cosine similarity) in vector space. Typical implementations include Facebook&rsquo;s DPR (Dense Passage Retriever) model, or pre-trained models provided by the more general <code>sentence-transformers</code> library.</p>
<p>In our system, we use a pre-trained sentence vector model (such as <code>all-MiniLM-L6-v2</code> provided by <code>sentence-transformers</code>) to map text to vectors. Key points include:</p>
<ul>
<li><strong>High-Dimensional Semantic Mapping:</strong> The model maps each text (claim or evidence sentence) to a high-dimensional vector, placing semantically similar texts closer in vector space. This means that even if two sentences share no identical words, their vectors will be close if their meanings are related.</li>
<li><strong>Synonym and Concept Alignment:</strong> Vector representations can automatically learn the proximity of synonymous expressions. For example, &ldquo;trigger&rdquo; and &ldquo;cause&rdquo; will be embedded in adjacent positions when they appear in similar contexts.</li>
<li><strong>Dual Tower Encoding:</strong> Queries and documents are encoded separately; cross-input is not required when calculating similarity. This structure facilitates <strong>offline pre-computation</strong> of document vectors: we can calculate the vectors for every sentence in the evidence library in advance and store them. When a new query comes in, we only need to calculate the query vector once and compute similarity with all evidence vectors, drastically improving retrieval efficiency (supported by tools like Faiss or Milvus for fast vector neighbor search).</li>
<li><strong>Hybrid Retrieval:</strong> In practice, we often fuse BM25 results with vector retrieval results to balance exact matching and semantic recall. This maximizes recall rate‚Äîensuring neither relevant evidence with synonymous phrasing nor exact entity matches are missed.</li>
</ul>
<p><strong>Code Example:</strong> Using SentenceTransformer to load a model and retrieve semantically similar evidence:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sentence_transformers <span style="color:#f92672">import</span> SentenceTransformer, util
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load a lightweight pre-trained sentence embedding model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SentenceTransformer(<span style="color:#e6db74">&#39;all-MiniLM-L6-v2&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculate vector representations for corpus evidence sentences (recommended to pre-compute and store offline)</span>
</span></span><span style="display:flex;"><span>evidence_embeddings <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode(corpus, convert_to_tensor<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculate vector for the query claim</span>
</span></span><span style="display:flex;"><span>claim_embedding <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode(claim, convert_to_tensor<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculate cosine similarity and get the top 10 closest evidence indices</span>
</span></span><span style="display:flex;"><span>cosine_scores <span style="color:#f92672">=</span> util<span style="color:#f92672">.</span>cos_sim(claim_embedding, evidence_embeddings)[<span style="color:#ae81ff">0</span>]  <span style="color:#75715e"># Cosine similarity vector</span>
</span></span><span style="display:flex;"><span>top_indices <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argpartition(<span style="color:#f92672">-</span>cosine_scores, range(<span style="color:#ae81ff">10</span>))[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">10</span>]         <span style="color:#75715e"># Top 10 high-score indices (unsorted)</span>
</span></span><span style="display:flex;"><span>top_indices <span style="color:#f92672">=</span> top_indices[cosine_scores[top_indices]<span style="color:#f92672">.</span>argsort(descending<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)]  <span style="color:#75715e"># Sort by similarity</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># View the most relevant piece of evidence</span>
</span></span><span style="display:flex;"><span>top_evidence <span style="color:#f92672">=</span> corpus[top_indices[<span style="color:#ae81ff">0</span>]]
</span></span><span style="display:flex;"><span>print(top_evidence)
</span></span></code></pre></div><p>This code uses <code>util.cos_sim</code> from the <code>sentence-transformers</code> library to quickly calculate similarity and select the top 10 pieces of evidence. Compared to BM25, semantic retrieval can find sentences close in meaning to the claim but not necessarily overlapping in words. For example, for a rumor claim like &ldquo;Vaccination causes autism,&rdquo; semantic retrieval can find relevant sentences in scientific papers like &ldquo;Studies have found no link between vaccines and autism,&rdquo; directly refuting the rumor despite different surface vocabulary.</p>
<p><strong>Performance Evaluation:</strong> To measure the effectiveness of retrieval modules, the <strong>MRR (Mean Reciprocal Rank)</strong> metric can be used. MRR focuses on the position of the first relevant result in the retrieval list; the higher the position, the higher the score. The formula calculates the reciprocal rank of the relevant result for each query and then takes the average. Pseudo-code example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mean_reciprocal_rank</span>(results_list, ground_truth_list):
</span></span><span style="display:flex;"><span>    rs <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> results, true_doc <span style="color:#f92672">in</span> zip(results_list, ground_truth_list):
</span></span><span style="display:flex;"><span>        ranks <span style="color:#f92672">=</span> [idx <span style="color:#66d9ef">for</span> idx, doc <span style="color:#f92672">in</span> enumerate(results) <span style="color:#66d9ef">if</span> doc <span style="color:#f92672">==</span> true_doc]
</span></span><span style="display:flex;"><span>        rs<span style="color:#f92672">.</span>append(<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (ranks[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">if</span> ranks <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>mean(rs)
</span></span></code></pre></div><p>We want MRR to be as high as possible, meaning correct evidence ranks highly for most queries. generally, combining BM25 and vector retrieval can significantly improve MRR, as the two complement each other&rsquo;s blind spots.</p>
<h3 id="3-natural-language-inference-entailment-based-truth-judgment">3. Natural Language Inference: Entailment-based Truth Judgment<a hidden class="anchor" aria-hidden="true" href="#3-natural-language-inference-entailment-based-truth-judgment">#</a></h3>
<p>After completing evidence retrieval, the next step is to judge the relationship between the evidence and the claim. This is formalized as a <strong>Natural Language Inference (NLI)</strong> task: given a claim sentence and an evidence text, the machine needs to judge whether the evidence supports the claim, refutes the claim, or is unrelated to the claim. This transformation is crucial‚Äîit elevates the fact-checking problem to one of logical reasoning, granting the model general reasoning capabilities across domains. Even if the model hasn&rsquo;t seen a specific rumor, it can use NLI reasoning skills to judge contradictions between evidence and claims.</p>
<p>We typically employ Transformer models fine-tuned on large-scale NLI datasets (like MultiNLI, SNLI, etc.) to perform this task. In a zero-shot setting, a model well-trained on NLI tasks can generalize directly to fact-checking labels without extra fine-tuning on rumor data. For example, the famous FEVER paper maps fact-checking labels to NLI&rsquo;s three classes: Entailment corresponds to &ldquo;Supports,&rdquo; Contradiction to &ldquo;Refutes,&rdquo; and Neutral to &ldquo;Not Enough Info.&rdquo;</p>
<p>Implementation points for the NLI module include:</p>
<ul>
<li><strong>Input Construction:</strong> Concatenate the claim and evidence into a model input pair. The standard approach is the <code>[CLS] Claim [SEP] Evidence [SEP]</code> format in Transformer input, allowing the model to read both and use attention mechanisms to analyze their relationship.</li>
<li><strong>Cross-Encoder:</strong> Models used for NLI judgment are usually single-tower structures (unlike the dual encoders mentioned earlier)‚Äîmeaning the claim and evidence are concatenated and sent into the same Transformer for encoding. This Cross-Encoder can use cross-sentence attention to capture subtle logical relationships, being much more precise than independent encoding followed by comparison. This also makes Cross-Encoders common for re-ranking retrieval results.</li>
<li><strong>Label Mapping:</strong> NLI model output is typically one of three classes: Entailment, Neutral, Contradiction. We need to map these to the fact-checking label space. For example: Supports, Not Enough Info, Refutes. This mapping strategy ensures model judgments align with human verification standards.</li>
</ul>
<p><strong>Code Example:</strong> Using ü§ó Transformers pipeline for quick NLI judgment:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load a RoBERTa large model fine-tuned on NLI data (supports zero-shot inference)</span>
</span></span><span style="display:flex;"><span>nli_pipeline <span style="color:#f92672">=</span> pipeline(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;text-classification&#34;</span>,
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example: Claim and Evidence</span>
</span></span><span style="display:flex;"><span>claim_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;The number of new cases of shingles per year extends from 1.2-3.4 per 1,000.&#34;</span>
</span></span><span style="display:flex;"><span>evidence_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;The number of new cases per year ranges from 1.2 -- 3.4 per 1,000...&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Construct input pair and predict</span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> nli_pipeline([{<span style="color:#e6db74">&#39;text&#39;</span>: claim_text, <span style="color:#e6db74">&#39;text_pair&#39;</span>: evidence_text}])[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>print(result[<span style="color:#e6db74">&#39;label&#39;</span>])  <span style="color:#75715e"># Likely outputs &#39;ENTAILMENT&#39; or &#39;CONTRADICTION&#39; etc.</span>
</span></span><span style="display:flex;"><span>print(result[<span style="color:#e6db74">&#39;score&#39;</span>])  <span style="color:#75715e"># Probability score</span>
</span></span></code></pre></div><p>In this example, if the content of <code>evidence_text</code> is logically consistent with <code>claim_text</code>, the model should output Entailment; if the evidence clearly refutes the claim, it outputs Contradiction. We then map Entailment to &ldquo;Supports,&rdquo; Contradiction to &ldquo;Refutes,&rdquo; and Neutral to &ldquo;Not Enough Info.&rdquo;</p>
<p>Note that there may be multiple pieces of evidence; the model should judge each piece separately and then aggregate the results. This belongs to the strategy discussed in the next section. However, simply put, a common rule is: if any piece of evidence is judged as Contradiction (Refutes), the overall judgment is False; otherwise, if at least one piece is Entailment and there are no Contradictions, it is True; if neither, it is unknown. This is similar to the FEVER evaluation standard.</p>
<p>Besides using specialized discriminative models, recent approaches use <strong>Large Language Models (LLMs)</strong> for zero-shot NLI. That is, using prompts to let models like GPT-4 directly read the evidence and give a support/refute judgment. For example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>You are a fact-checking assistant. Decide if the claim is supported, refuted, or not enough info based on the statements.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Claim: &lt;Insert Claim&gt;
</span></span><span style="display:flex;"><span>Statements:
</span></span><span style="display:flex;"><span>1. &lt;Evidence Sentence 1&gt;
</span></span><span style="display:flex;"><span>2. &lt;Evidence Sentence 2&gt;
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Answer with one of: SUPPORTS / REFUTES / NOT ENOUGH INFO.
</span></span></code></pre></div><p>Submitting this prompt to a powerful LLM (like GPT-4) can theoretically yield correct labels. In our tutorial experiments, using a smaller GPT-4 architecture model (called GPT-4o mini) achieved zero-shot classification with about 79.6% accuracy, close to the specialized RoBERTa model (83.2%). This suggests that on specific tasks, fine-tuned smaller models often outperform general large models in cost-effectiveness. However, LLM methods require no training and are easy to extend to new tasks, holding great practical value.</p>
<h3 id="4-result-aggregation-and-label-mapping">4. Result Aggregation and Label Mapping<a hidden class="anchor" aria-hidden="true" href="#4-result-aggregation-and-label-mapping">#</a></h3>
<p>After the above steps, we have an NLI model output for each &ldquo;Claim-Evidence&rdquo; pair. In reality, a claim often requires multiple pieces of evidence to reach a conclusion, so we need to aggregate these results to output a final label.</p>
<p>Label mapping was mentioned in the previous section (mapping NLI&rsquo;s ternary classification back to our output labels). For fact-checking, the output is generally three classes: True/Supported, False/Refuted, Not Enough Info/Neutral.</p>
<p>Result aggregation needs to consider the synthesis of multiple pieces of evidence. Common strategies include:</p>
<ul>
<li><strong>Proof-based Aggregation:</strong> If at least one piece of evidence clearly supports the claim and there is no contradictory evidence, judge as True; if at least one piece refutes the claim, judge as False; otherwise, if neither supported nor refuted, judge as Not Enough Info. This strategy is similar to the legal idea of &ldquo;one valid piece of evidence is enough to convict/acquit,&rdquo; suitable for most fact-checking tasks.</li>
<li><strong>Majority Vote:</strong> When evidence quality varies, let multiple pieces of evidence vote. For example, if 2 out of 3 pieces support and 1 refutes, decide Support. However, simple voting isn&rsquo;t always reliable because evidence is not independent and homogeneous; usually, the most authoritative or relevant evidence should be prioritized.</li>
<li><strong>Weighted Fusion:</strong> Assign weights to different evidence based on retrieval scores or source credibility, then accumulate their confidence for each category. For example, if one highly relevant piece of evidence refutes, even if several others vaguely support, the high-weight refutation may prevail.</li>
<li><strong>Learned Fusion Model:</strong> In advanced applications, a verdict fusion model can be trained, taking NLI logits or embeddings of multiple pieces of evidence as input and outputting the final label. This allows the model to learn how to decide when evidence conflicts. But this requires training data and is not zero-shot.</li>
</ul>
<p>In our tutorial framework, we adopt a simplified proof-based rule: Refutes -&gt; False; Supports (no Refutes) -&gt; True; neither -&gt; NEI. While simple, this rule aligns with intuition and performs well in FEVER tasks. In fact, FEVER evaluation requires the model to provide the correct evidence set to be considered correct (the FEVER Score): success is counted only if the label is correct AND the submitted evidence contains a complete chain of support/refutation. This encourages systems to rigorously consider evidence sufficiency during aggregation, otherwise preferring &ldquo;Not Enough Info.&rdquo; The FEVER score measures the accuracy of the model&rsquo;s proof process, making it stricter but more practical than simple label accuracy.</p>
<p>Summary of technical modules: We introduced BM25 sparse retrieval for entity matching, semantic vector retrieval for synonymous semantics, NLI inference for logical discrimination, and finally label mapping and result aggregation for decision-making. Next, we will link these modules to demonstrate an actual automated fact-checking workflow with code examples.</p>
<hr>
<h2 id="practical-workflow-and-code-demonstration">Practical Workflow and Code Demonstration<a hidden class="anchor" aria-hidden="true" href="#practical-workflow-and-code-demonstration">#</a></h2>
<p>Below, following the actual modeling order, we link the aforementioned modules to build a simplified rumor detection/fact-checking system, providing key code snippets. The flow includes: Check-Worthiness Detection, Evidence Retrieval (combined sparse and dense), Fact-Checking Judgment (NLI inference output), and Explanation Generation (reasons provided by a generative model).</p>
<h3 id="1-check-worthiness-detection">1. Check-Worthiness Detection<a hidden class="anchor" aria-hidden="true" href="#1-check-worthiness-detection">#</a></h3>
<p>In social media posts or political speeches, not every sentence is worth checking. Most sentences are just chitchat or opinions; only a few contain verifiable specific factual claims that require further evidence retrieval. This poses the <strong>Check-Worthiness Detection</strong> task: identifying sentences worth checking from a text. This is a binary classification problem (Check-worthy vs. Non-check-worthy), characterized by extreme class imbalance‚Äîcheckable sentences are often less than 5% of the total.</p>
<p>We can use supervised learning to train a detection model. For example, fine-tuning a pre-trained BERT model for binary classification. To handle class imbalance, common practices include giving positive examples (checkable sentences) larger loss weights or oversampling positive examples. In the HuggingFace Trainer framework, this can be implemented by customizing the loss. For example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;bert-base-uncased&#34;</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForSequenceClassification<span style="color:#f92672">.</span>from_pretrained(model_name, num_labels<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Map dataset encoding</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">encode_examples</span>(examples):
</span></span><span style="display:flex;"><span>    outputs <span style="color:#f92672">=</span> tokenizer(examples[<span style="color:#e6db74">&#34;text&#34;</span>], truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;max_length&#34;</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>    outputs[<span style="color:#e6db74">&#34;labels&#34;</span>] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> lbl <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;Check-worthy&#34;</span> <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">for</span> lbl <span style="color:#f92672">in</span> examples[<span style="color:#e6db74">&#34;label&#34;</span>]]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> outputs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>encoded_dataset <span style="color:#f92672">=</span> raw_dataset<span style="color:#f92672">.</span>map(encode_examples, batched<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define Trainer with Weighted Loss</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">WeightedLossTrainer</span>(Trainer):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, class_weights, <span style="color:#f92672">*</span>args, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>(<span style="color:#f92672">*</span>args, <span style="color:#f92672">**</span>kwargs)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>class_weights <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(class_weights)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_loss</span>(self, model, inputs, return_outputs<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>        labels <span style="color:#f92672">=</span> inputs<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;labels&#34;</span>)
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> model(<span style="color:#f92672">**</span>inputs)
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>logits
</span></span><span style="display:flex;"><span>        loss_fct <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>CrossEntropyLoss(weight<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>class_weights)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fct(logits, labels)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> (loss, outputs) <span style="color:#66d9ef">if</span> return_outputs <span style="color:#66d9ef">else</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculate class weights, e.g., Non-check:Check ‚âà 1:20 </span>
</span></span><span style="display:flex;"><span>weights <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">20.0</span>]
</span></span><span style="display:flex;"><span>trainer <span style="color:#f92672">=</span> WeightedLossTrainer(class_weights<span style="color:#f92672">=</span>weights, model<span style="color:#f92672">=</span>model, args<span style="color:#f92672">=</span>TrainingArguments(<span style="color:#f92672">...</span>),
</span></span><span style="display:flex;"><span>                              train_dataset<span style="color:#f92672">=</span>encoded_dataset[<span style="color:#e6db74">&#34;train&#34;</span>], eval_dataset<span style="color:#f92672">=</span>encoded_dataset[<span style="color:#e6db74">&#34;val&#34;</span>])
</span></span><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>train()
</span></span></code></pre></div><p>In the code above, we manually set class weights to 1:20, meaning the loss for each check-worthy sample is amplified 20 times, guiding the model to pay more attention to the minority class. This weighted cross-entropy is an effective means to alleviate class imbalance.</p>
<p>The trained model can then filter out a list of sentences requiring checking from input text. Assuming we have located several claims worth checking, we proceed to fact-check each claim.</p>
<h3 id="2-evidence-retrieval">2. Evidence Retrieval<a hidden class="anchor" aria-hidden="true" href="#2-evidence-retrieval">#</a></h3>
<p>Having a specific claim, we need to find supporting or refuting evidence sentences in a massive unstructured text base (knowledge base). We combine BM25 sparse retrieval and semantic dense retrieval.</p>
<ul>
<li><strong>BM25 Retrieval:</strong> First, search the index using keywords from the claim. Code example is as shown in the BM25S section above. BM25 will return a set of candidate evidence sentences (e.g., Top 50). Since BM25 favors exact matching, this step ensures documents related to key entities are not missed.</li>
<li><strong>Vector Retrieval:</strong> In parallel, retrieve using semantic vectors of the claim. Code example is in the semantic retrieval section. We can take the group of candidate sentences with the highest vector similarity (e.g., Top 50). Vector retrieval supplements evidence missed by BM25 that is semantically relevant but phrased differently.</li>
<li><strong>Candidate Fusion:</strong> Merge the candidate sets from both methods (usually totaling dozens of sentences), remove duplicates (as methods may hit the same sentence), and simply sort. Sorting can be based on BM25 scores or vector scores, or by training a learning-to-rank model. For simplicity, sorting by BM25 score is acceptable.</li>
<li><strong>Evidence Filtering:</strong> To reduce the burden on the subsequent NLI model, we can add a filtering layer. For example, use a lightweight Cross-Encoder to re-score and re-rank candidate evidence, selecting the Top 5 to send to the next inference step. This improves accuracy but increases complexity. For demonstration, this can be omitted, using all candidates.</li>
</ul>
<h3 id="3-fact-checking-judgment-nli-inference">3. Fact-Checking Judgment (NLI Inference)<a hidden class="anchor" aria-hidden="true" href="#3-fact-checking-judgment-nli-inference">#</a></h3>
<p>For each claim, we now have several candidate evidence sentences. We let the NLI model judge each Claim-Evidence pair. As mentioned, using a trained NLI model can directly give &ldquo;Supports/Refutes/Irrelevant&rdquo; classification results.</p>
<p>Pseudo-code follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>labels_map <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;ENTAILMENT&#34;</span>: <span style="color:#e6db74">&#34;SUPPORTS&#34;</span>, <span style="color:#e6db74">&#34;CONTRADICTION&#34;</span>: <span style="color:#e6db74">&#34;REFUTES&#34;</span>, <span style="color:#e6db74">&#34;NEUTRAL&#34;</span>: <span style="color:#e6db74">&#34;NOT ENOUGH INFO&#34;</span>}
</span></span><span style="display:flex;"><span>final_decisions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> claim <span style="color:#f92672">in</span> claims_to_check:
</span></span><span style="display:flex;"><span>    decisions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> evidence <span style="color:#f92672">in</span> candidate_evidences[claim]:
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> nli_pipeline({<span style="color:#e6db74">&#39;text&#39;</span>: claim, <span style="color:#e6db74">&#39;text_pair&#39;</span>: evidence})
</span></span><span style="display:flex;"><span>        decisions<span style="color:#f92672">.</span>append(labels_map[result[<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#39;label&#39;</span>]])
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Result Aggregation</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;REFUTES&#34;</span> <span style="color:#f92672">in</span> decisions:
</span></span><span style="display:flex;"><span>        final <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;False&#34;</span>  <span style="color:#75715e"># Evidence refutes</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> <span style="color:#e6db74">&#34;SUPPORTS&#34;</span> <span style="color:#f92672">in</span> decisions:
</span></span><span style="display:flex;"><span>        final <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;True&#34;</span>   <span style="color:#75715e"># Supported and not refuted</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        final <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Not Enough Info&#34;</span>
</span></span><span style="display:flex;"><span>    final_decisions<span style="color:#f92672">.</span>append((claim, final))
</span></span></code></pre></div><p>This logic implements a simple evidence aggregation strategy. After getting the final label, we have completed the truth judgment of the claim. However, giving just a label is often insufficient. In rumor governance, we also want to explain <em>why</em> the model made such a judgment. To this end, we introduce the next step: Explanation Generation.</p>
<h3 id="4-explanation-generation">4. Explanation Generation<a hidden class="anchor" aria-hidden="true" href="#4-explanation-generation">#</a></h3>
<p>To improve the acceptability and effectiveness of detection results, we want the model to generate a natural language explanation stating which evidence it based its conclusion on and how it reasoned. For example, for a rumor labeled &ldquo;False,&rdquo; the explanation might be: &ldquo;Because Evidence X indicates the exact opposite, the claim is unfounded.&rdquo; Such explanations not only help the audience understand the truth but also counter the persuasiveness of the rumor.</p>
<p>Generating explanations can be viewed as an NLG (Natural Language Generation) task, which we can complete using large generative models (like GPT). In practice, we can design a Prompt to let the model generate an explanation based on the claim, label, and evidence. For example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>You are tasked with creating an explanation of a fact check.
</span></span><span style="display:flex;"><span>Claim: &lt;Claim&gt;
</span></span><span style="display:flex;"><span>Label: &lt;Model Determined Label (True/False/Not Enough Info)&gt;
</span></span><span style="display:flex;"><span>Statements: 
</span></span><span style="display:flex;"><span>- &lt;Evidence Sentence 1&gt;
</span></span><span style="display:flex;"><span>- &lt;Evidence Sentence 2&gt;
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Please provide a brief explanation as to why the label is correct based on the provided evidence.
</span></span></code></pre></div><p>Submitting this template filled with specific content to a model like GPT-4o mini will generate an explanatory statement. For instance, for a rumor claim &ldquo;Maze Runner is a sports competition&rdquo; where evidence shows Maze Runner is a movie produced by Ellen Goldsmith-Vein, it might generate:</p>
<blockquote>
<p>&ldquo;The &lsquo;Refutes&rsquo; label is correct because the evidence clearly states that <em>The Maze Runner</em> is a film produced by Ellen Goldsmith-Vein, whereas the claim asserts it is a sports competition.&rdquo;</p>
</blockquote>
<p>This explanation clearly articulates how the evidence contradicts the claim, supporting the &ldquo;Refutes&rdquo; conclusion.</p>
<p><strong>Explanation Evaluation:</strong> How do we evaluate the quality of generated explanations? Traditional NLG metrics like BLEU and ROUGE look mainly at lexical overlap and cannot measure the logical consistency (i.e., &ldquo;faithfulness&rdquo;) between explanation and evidence. For this, we introduce the <strong>G-Eval</strong> mechanism: letting a more powerful LLM act as a judge, scoring according to pre-defined standards. For example, defining a scale of 1-5, where 1 means the explanation completely contradicts the evidence, and 5 means the explanation accurately and completely reflects the relationship between evidence and claim.</p>
<p>We can construct an evaluation prompt template:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Evaluation Criteria:
</span></span><span style="display:flex;"><span>Faithfulness (1-5) - the factual alignment between the fact-checking explanation and the evidence.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Evidence Provided: &lt;Evidence Text&gt;
</span></span><span style="display:flex;"><span>Fact-Checking Explanation: &lt;Model Generated Explanation&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Evaluation: Please provide a score from 1 to 5 only.
</span></span></code></pre></div><p>Let GPT-4 evaluate the same explanation multiple times (e.g., n=20). Due to LLM output randomness, we can average the scores to reduce single-instance bias. This evaluation method leverages the understanding and reasoning capabilities of LLMs, known as LLM-as-a-judge or the basic idea of G-Eval. Research shows G-Eval can evaluate generated text with consistency approaching human levels.</p>
<p>It should be noted that explanation generation is the icing on the cake. In practical applications, if the goal is just automated judgment, the first three steps are sufficient. But providing explanations undoubtedly enhances the system&rsquo;s interpretability and persuasiveness, making it very suitable for assisting human moderation or public education scenarios.</p>
<p>So far, our automated detection workflow has covered every link from inputting raw text to outputting a judgment result with an explanation. Let&rsquo;s look at the utility of this flow through actual cases.</p>
<hr>
<h2 id="case-studies-vaccine-rumors-and-climate-change-narratives">Case Studies: Vaccine Rumors and Climate Change Narratives<a hidden class="anchor" aria-hidden="true" href="#case-studies-vaccine-rumors-and-climate-change-narratives">#</a></h2>
<p>To intuitively understand how the above model deals with &ldquo;false narratives cloaked in science,&rdquo; we demonstrate with two social hotspots.</p>
<h3 id="case-1-vaccine-rumor">Case 1: Vaccine Rumor<a hidden class="anchor" aria-hidden="true" href="#case-1-vaccine-rumor">#</a></h3>
<p><strong>Scenario:</strong> An online post claims: &ldquo;Research proves that the MMR (measles, mumps, rubella) vaccine causes autism in children.&rdquo; This is a classic anti-vaccine narrative that cites &ldquo;research proves&rdquo; to add scientific legitimacy. However, the claim originates from a 1998 paper in <em>The Lancet</em> that was later proven fraudulent and retracted.</p>
<ul>
<li><strong>Check-Worthiness:</strong> The model identifies this sentence as check-worthy because it involves medical causality, has significant impact, and is verifiable.</li>
<li><strong>Evidence Retrieval:</strong> BM25 might use keywords like &ldquo;MMR,&rdquo; &ldquo;autism,&rdquo; &ldquo;study&rdquo; to retrieve relevant news or Wikipedia pages; vector retrieval captures semantically related sentences like &ldquo;no link found,&rdquo; &ldquo;studies find no evidence.&rdquo; Actual evidence is easily found in authoritative medical sources; e.g., an AAP press release stating: &ldquo;Numerous studies across time and countries have found no credible link between vaccines and autism. The report originally claiming MMR causes autism was retracted for fraud, and its author lost his medical license.&rdquo;</li>
<li><strong>NLI Judgment:</strong> Given the evidence above, the model determines &ldquo;Contradiction&rdquo; because the evidence explicitly refutes the claim (evidence says no link, claim says causal). Mapped label is &ldquo;False.&rdquo;</li>
<li><strong>Result Aggregation:</strong> If multiple pieces of evidence all show no link found, they consistently point to False with no conflict. The model outputs the rumor as False.</li>
<li><strong>Explanation Generation:</strong> The model generates an explanation, e.g., &ldquo;Authoritative research has repeatedly confirmed no causal link between the MMR vaccine and autism; the paper originally claiming a link was retracted for data fraud. Therefore, the claim is unfounded.&rdquo; This explanation highlights key evidence information (&ldquo;no causal link,&rdquo; &ldquo;paper fraud retracted&rdquo;), powerfully refuting the scientific camouflage of the rumor.</li>
</ul>
<h3 id="case-2-climate-change-narrative">Case 2: Climate Change Narrative<a hidden class="anchor" aria-hidden="true" href="#case-2-climate-change-narrative">#</a></h3>
<p><strong>Scenario:</strong> A blog writes: &ldquo;Arctic sea ice area has been trending upwards in the past few years; global warming is a hoax.&rdquo; This argument questions global warming by selecting data from a specific time period; it looks like it cites scientific observations but is an example of misleading content. It doesn&rsquo;t fabricate facts entirely (Arctic sea ice did rebound in certain years) but ignores long-term trends through cherry-picking.</p>
<ul>
<li><strong>Check-Worthiness:</strong> This sentence is also check-worthy, involving scientific data and conclusions needing verification.</li>
<li><strong>Evidence Retrieval:</strong> Retrieval might return NASA or IPCC report summaries on long-term Arctic sea ice trends. For example: &ldquo;Satellite data shows the long-term trend of Arctic sea ice coverage is still declining; despite brief increases in some years, the overall trend supports continued global warming.&rdquo; The model might also retrieve scientific debunking articles regarding &ldquo;global warming hoaxes.&rdquo;</li>
<li><strong>NLI Judgment:</strong> The relationship is subtle. The evidence doesn&rsquo;t say the specific data point is &ldquo;wrong,&rdquo; but provides the full context (short-term fluctuation doesn&rsquo;t change long-term warming). The model might lean towards &ldquo;Contradiction&rdquo; (because the claim calls warming a hoax, while evidence emphasizes warming is real). Some evidence might be seen as &ldquo;Neutral&rdquo; if it only provides data without directly addressing the &ldquo;hoax&rdquo; claim.</li>
<li><strong>Result Aggregation:</strong> Synthesizing authoritative evidence, there should be sufficient reason to refute &ldquo;global warming is a hoax.&rdquo; Even if the model doesn&rsquo;t judge every piece of evidence as Contradiction, multiple scientific reports together form a strong counter-proof. Thus, the final output should be False.</li>
<li><strong>Explanation Generation:</strong> The explanation needs to balance scientific accuracy and accessibility, e.g., &ldquo;Although Arctic sea ice rebounded in some recent periods, the overall trend remains downward. Authoritative climate reports state global warming is happening and is not a hoax. Therefore, asserting &lsquo;global warming is a hoax&rsquo; is misleading through cherry-picking.&rdquo; Such an explanation reveals the claim&rsquo;s technique (confusing long-term trends with short-term growth) and gives the complete scientific conclusion.</li>
</ul>
<p>These cases show that our model pipeline can not only identify completely false statements but also handle narratives based on facts but with wrong conclusions. For the latter, the model relies on multiple evidence fragments to reveal issues from different angles. This is why multi-evidence retrieval and NLI reasoning are necessary‚Äîthey empower AI to judge complex framed/biased statements like an expert, rather than just handling simple True/False questions.</p>
<p>Of course, these cases also remind us of model limitations: if the claim is expressed implicitly (e.g., suggestive wrong causality), the model&rsquo;s NLI judgment might lack confidence; if evidence is insufficient or the knowledge base lacks relevant info (long-tail rumors), the model can only output &ldquo;Not Sure.&rdquo; The next section discusses these challenges and future directions.</p>
<hr>
<h2 id="frontiers-and-challenges">Frontiers and Challenges<a hidden class="anchor" aria-hidden="true" href="#frontiers-and-challenges">#</a></h2>
<p>Current retrieval-augmented detection frameworks show high accuracy and interpretability in experiments, but there are still many open questions and room for improvement. Some are technical frontiers, others are challenges in application deployment. In this section, we list several directions worth watching:</p>
<ul>
<li><strong>Integration of Retrieval-Augmented Generation (RAG) Architecture:</strong> RAG refers to seamlessly integrating retrieval modules into generative models, enabling the model to retrieve external knowledge while generating answers. In the future, we can more tightly combine fact-checking systems with Large Language Models, allowing the model to dynamically retrieve materials when generating conclusions or explanations, ensuring the factualness and completeness of the output. This approach has been proven effective in tasks like open-domain QA and is highly beneficial for explanation generation in rumor detection. RAG architectures can also handle multi-hop reasoning for longer documents by iteratively retrieving relevant information to deal with complex claims.</li>
<li><strong>Explanation Quality Evaluation (G-Eval) Mechanism:</strong> With the application of generative models, how to evaluate whether their output explanations are reliable becomes a new challenge. The G-Eval framework offers the idea of using LLMs for evaluation. Future research can further explore multi-dimensional evaluation standards, such as examining not just faithfulness (strictly based on evidence) but also readability, persuasiveness, etc. The evaluation process can be made more automated and robust, for instance, by combining multiple different LLM judges to offset single-model bias. for communication research pursuing academic rigor, using AI to evaluate AI-generated explanations requires caution, but this direction promises to drastically improve evaluation efficiency.</li>
<li><strong>Zero-Shot and Out-of-Domain Generalization:</strong> A difficulty of information disorder is its rapid change. New conspiracy theories and slang expressions appear constantly and cannot be exhaustively included in training sets. Therefore, zero-shot learning and cross-domain generalization capabilities are crucial. We have introduced some zero-shot reasoning capability via NLI, but future work can explore prompting large models for more complex reasoning, or using self-supervised pre-training to let models master common sense reasoning, thus being able to judge rumors never seen before. Of course, complete zero-shot is unrealistic, so <strong>Continual Learning</strong> is also a direction‚Äîletting models evolve as new data updates, while avoiding catastrophic forgetting and rumor data polluting the model.</li>
<li><strong>Multimodal Detection:</strong> The methods discussed currently focus mainly on text. However, real-world disinformation is often multimodal‚Äîfake videos, forged images, audio clips, etc. The rise of Deepfakes is an example, requiring integration with image forensics, voice recognition, and other technologies. Multimodal fusion detection systems are urgently needed; for example, when a rumor spreads via image and text, how to combine NLP models with Computer Vision models for a shared judgment. Some multimodal fact-checking datasets exist, but this field is still in its infancy and holds great challenges.</li>
<li><strong>Adversarial Attacks and Robustness:</strong> Generative AI is not only a tool for defenders but is also used by bad actors to produce more deceptive rumor texts. These adversarial texts might specifically target detector weaknesses, avoiding keywords or using flowery syntax to confuse models. Improving model robustness against such attacks is urgent. Feasible solutions include: introducing adversarial training (letting the model see perturbed fake text); and developing verifiable models, such as using logical rules to constrain output. Meanwhile, we must prevent the model itself from generating misinformation‚Äîin RAG architectures, if false information is retrieved, the model might be misled, so source credibility assessment is also vital.</li>
<li><strong>Knowledge Gaps and the Long-Tail Problem:</strong> When rumors involve very novel or obscure topics, direct evidence might not be found in the knowledge base. Retrieval methods fall into a &ldquo;knowledge vacuum.&rdquo; The model might label such claims generally as &ldquo;Not Enough Info,&rdquo; but in communication research, we sometimes need to predictively judge risk. To this end, knowledge sources can be expanded (e.g., accessing scientific literature, expert databases) or deduction mechanisms introduced: inferring conclusions based on common sense and related knowledge. However, letting AI infer truth based on incomplete knowledge carries risks and needs deep exploration.</li>
<li><strong>Evaluation Metrics and Misjudgment Costs:</strong> For detection systems, simply pursuing overall accuracy might mask problems. For instance, if rumors are only 1% of the sample, a model predicting &ldquo;non-rumor&rdquo; for everything has 99% accuracy but is meaningless. Therefore, more appropriate assessment focuses on Recall of the positive class and F1 score. In practice, the cost of missing a real rumor (False Negative) differs from misjudging true information as a rumor (False Positive); model thresholds and optimization goals should be adjusted according to the application scenario. Communication scholars are particularly concerned with reducing False Negatives (don&rsquo;t miss important rumors) while providing evidence for human review. This requires comprehensive performance evaluation combining quantitative metrics and qualitative analysis.</li>
</ul>
<p><strong>Advantages and Limitations:</strong> Current retrieval-augmented detection models have clear advantages over pure classifiers: interpretable, scalable, and domain-agnostic (via evidence retrieval). At the same time, they easily integrate into human workflows‚Äîfact-checkers can directly view model-retrieved evidence and judgments to assist decisions. However, limitations cannot be ignored: high system complexity, need for massive external knowledge support, strong dependence on knowledge base freshness (helpless against new fake news not yet indexed), and difficulty capturing implicit misleading (models excel at explicit truth judgment but sometimes struggle to qualify technical misleading like satire or exaggeration). Additionally, the quality of explanations output by models is currently unstable, with potential AI hallucination, requiring human verification or more reliable generation constraints.</p>
<p>In conclusion, disinformation detection is a continuously evolving field of offense and defense, requiring interdisciplinary perspectives and multi-faceted innovation. From communication theory to artificial intelligence technology, every step of development will help us better identify and respond to information disorder. We hope the theoretical-practical framework provided in this tutorial offers a beneficial reference for related research and look forward to more intelligent and reliable detection systems emerging in complex and changing environments to jointly safeguard a healthy information ecosystem.</p>
<pre tabindex="0"><code></code></pre>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://llspublic.github.io/tags/digital-humanities/">Digital Humanities</a></li>
      <li><a href="https://llspublic.github.io/tags/computational-social-science/">Computational Social Science</a></li>
      <li><a href="https://llspublic.github.io/tags/nlp/">NLP</a></li>
      <li><a href="https://llspublic.github.io/tags/fake-news-detection/">Fake News Detection</a></li>
      <li><a href="https://llspublic.github.io/tags/rag/">RAG</a></li>
      <li><a href="https://llspublic.github.io/tags/python/">Python</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://llspublic.github.io/">Lyuxi Liu</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
